<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>0401 · SPMLJ</title><script async src="https://www.googletagmanager.com/gtag/js?id=G-Q39LHCRBB6"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-Q39LHCRBB6', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../index.html">SPMLJ</a></span></div><form class="docs-search" action="../search.html"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../index.html">Index</a></li><li><span class="tocitem">Lessons</span><ul><li><input class="collapse-toggle" id="menuitem-2-1" type="checkbox"/><label class="tocitem" for="menuitem-2-1"><span class="docs-label">KOM - Kick off meeting</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../00_-_KOM_-_Kickoff_meeting/0000-KOMeeting.html">0000-KOMeeting</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-2" type="checkbox"/><label class="tocitem" for="menuitem-2-2"><span class="docs-label">JULIA1 - Basic Julia Programming</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../01_-_JULIA1_-_Basic_Julia_programming/0101-basic_syntax.html">0101-basic syntax</a></li><li><a class="tocitem" href="../01_-_JULIA1_-_Basic_Julia_programming/0102-types_and_objects.html">0102-types and objects</a></li><li><a class="tocitem" href="../01_-_JULIA1_-_Basic_Julia_programming/0103-predefined_types.html">0103-predefined types</a></li><li><a class="tocitem" href="../01_-_JULIA1_-_Basic_Julia_programming/0104-control_flow_and_functions.html">0104-control flow and functions</a></li><li><a class="tocitem" href="../01_-_JULIA1_-_Basic_Julia_programming/0105-custom_types.html">0105-custom types</a></li><li><a class="tocitem" href="../01_-_JULIA1_-_Basic_Julia_programming/0106-further_topics.html">0106-further topics</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-3" type="checkbox"/><label class="tocitem" for="menuitem-2-3"><span class="docs-label">JULIA2 - Scientific programming with Julia</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../02_-_JULIA2_-_Scientific_programming_with_Julia/0201-wrangling_data.html">0201-wrangling data</a></li><li><a class="tocitem" href="../02_-_JULIA2_-_Scientific_programming_with_Julia/0202-further_topics.html">0202-further topics</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-4" type="checkbox"/><label class="tocitem" for="menuitem-2-4"><span class="docs-label">ML1 - Introduction to Machine Learning</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../03_-_ML1_-_Introduction_to_Machine_Learning/0301-MachineLearningMainIdeas.html">0301-MachineLearningMainIdeas</a></li><li><a class="tocitem" href="../03_-_ML1_-_Introduction_to_Machine_Learning/0302-perceptron.html">0302-perceptron</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-5" type="checkbox" checked/><label class="tocitem" for="menuitem-2-5"><span class="docs-label">NN - Neural Networks</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href="0401.html">0401</a><ul class="internal"><li><a class="tocitem" href="#Motivations-and-types"><span>Motivations and types</span></a></li><li><a class="tocitem" href="#Feed-forward-neural-networks"><span>Feed-forward neural networks</span></a></li></ul></li><li><a class="tocitem" href="0402.html">0402</a></li></ul></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Lessons</a></li><li><a class="is-disabled">NN - Neural Networks</a></li><li class="is-active"><a href="0401.html">0401</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="0401.html">0401</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/sylvaticus/SPMLJ/blob/master/lessonsSources/04_-_NN_-_Neural_Networks/0401.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Neural-Networks-theory"><a class="docs-heading-anchor" href="#Neural-Networks-theory">0401 - Neural Networks - theory</a><a id="Neural-Networks-theory-1"></a><a class="docs-heading-anchor-permalink" href="#Neural-Networks-theory" title="Permalink"></a></h1><p>While powerful, neural networks are really composed of very simple units that are akin to linear classification or regression methods.</p><p>We&#39;ll first describe them, and we&#39;ll later learn how to train a neural network from the data. Concerning the practical implementation in Julia, we&#39;ll not implement a complete neural network but rather only certain parts, as we will mostly deal with using them and apply to different kind of datasets.</p><h2 id="Motivations-and-types"><a class="docs-heading-anchor" href="#Motivations-and-types">Motivations and types</a><a id="Motivations-and-types-1"></a><a class="docs-heading-anchor-permalink" href="#Motivations-and-types" title="Permalink"></a></h2><p>We already saw the Perceptron algorithm as a linear classifier, but we also noted how we can transform the original feature vector <span>$\mathbf{x}$</span> to feature representation of <span>$\phi(\mathbf{x}$</span> where the transformation could well be non-linear, so to still use linear classifiers (we saw for classification but exactly the same is true for making regressions). The &quot;problem&quot; is that this feature transformation is not learn from the data but it is applied a priori, before using the actual machine learning (linear) algorithm. With neural networks instead, the feature transformation is endogenous to the learning (training) step.</p><p>We will see three kinds of neural networks:</p><ul><li><strong>feed-forward neural networks</strong>, the simplest one where the inputs flow through a set of &quot;layers&quot; to reach an output. </li><li><strong>convolutional neural networks</strong>, where one of more of the layers is a &quot;convolutional&quot; layer. These are used mainly for image classification</li><li><strong>recurrend neurla netorks (RNN)</strong>, where the input arrive not only at the beginning but also at each layer. RNNs are used to learn sequences of data</li></ul><h2 id="Feed-forward-neural-networks"><a class="docs-heading-anchor" href="#Feed-forward-neural-networks">Feed-forward neural networks</a><a id="Feed-forward-neural-networks-1"></a><a class="docs-heading-anchor-permalink" href="#Feed-forward-neural-networks" title="Permalink"></a></h2><h3 id="Description"><a class="docs-heading-anchor" href="#Description">Description</a><a id="Description-1"></a><a class="docs-heading-anchor-permalink" href="#Description" title="Permalink"></a></h3><p>In <strong>deep forward neural networks</strong>, neural network units are arranged in <strong>layers</strong>, from the <em>input layer</em>, where each unit holds the input coordinate, through various <em>hidden layer</em> transformations, until the actual <em>output</em> of the model:</p><p><img src="https://raw.githubusercontent.com/sylvaticus/SPMLJ/main/lessonsSources/04_-_NN_-_Neural_Networks/imgs/feedforwardNNChart.png" alt="Neural network scheme"/></p><p>More in detail, considering a single <em>dense</em> neuron (in the sense that is connected with <em>all</em> the previous layer&#39;s neurons or with the input layer), we have the following figure:</p><p><img src="https://raw.githubusercontent.com/sylvaticus/SPMLJ/main/lessonsSources/04_-_NN_-_Neural_Networks/imgs/singleNeuron.png" alt="Single neuron"/></p><p>where:</p><ul><li><p class="math-container">\[x\]</p>is a due dimensional input, with <span>$x_1$</span> and <span>$x_2$</span> being the two dimensions of our input data (they could equivalently be the outputs of a previous 2 neurons layers)</li><li><p class="math-container">\[w\]</p>are the <em>weigth</em> that are applied to <span>$x$</span> plus a constant term (<span>$w_0$</span>). <strong>These are the parameter we will want to learn with our algorithm</strong>. <span>$f$</span> is a function (often non-linear) that is applied to <span>$w_0 + x_1w_1 + x_2w_2$</span> to define the output of the neuron</li></ul><p>The output of the neuron can be the output of our neural network or it can be the input of a further layer.</p><p>Let&#39;s specific a bit of terminology concerning Naural Networks:</p><ul><li>The individual computation units of a layer are known as <strong>nodes</strong> or <strong>neurons</strong>.</li><li><strong>Width_l</strong> (<em>of the layer</em>) is the number of units in that specific layer <span>$l$</span></li><li><strong>Depth</strong> (<em>of the architecture</em>) is number of layers of the overall transformation before arriving to the final output</li><li>The <strong>weights</strong> are denoted with <span>$w$</span> and are what we want the algorithm to learn.</li><li>Each node&#39;s <strong>aggregated input</strong> is given by <span>$z = \sum_{i=1}^d x_i w_i + w_0$</span> (or, in vector form, <span>$z = \mathbf{x} \cdot \mathbf{w} + w_0$</span>, with <span>$z \in \mathbb{R}, \mathbf{x} \in \mathbb{R}^d, \mathbf{w} \in \mathbb{R}^d$</span>) and <span>$d$</span> is the width of the previous layer (or the input layer)</li><li>The output of the neuron is the result of a non-linear transformation of the aggregated input called <strong>activation function</strong> <span>$f = f(z)$</span></li><li>A <strong>neural network unit</strong> is a primitive neural network that consists of only the “input layer&quot;, and an output layer with only one output.</li><li><strong>hidden layers</strong> are the layers that are not dealing directly with the input nor the output layers </li><li><strong>Deep neural networks</strong> are neural network with at least one hidden layer</li></ul><p>While the weights will be learned, the width of each layer, the number of layers and the activation funcions are all elements that can be tuned as hyperparameters of the model, altought there are some more or less formal &quot;rules&quot;:</p><ul><li>the input layer is equal to the dimensions of the input data</li><li>the output layer is equal to the dimensions of the output data. This is tipically a scalar in a regression, but it is equal to the number of categories in a multi-class classification, where each &quot;output dimension&quot; will be the probability associated to that given class</li><li>the number of hidden layers reflects our judgment on how many &quot;levels&quot; we should decompose our input to arrive to the concept expressed in the label <span>$y$</span> (we&#39;ll see this point dealing with image classification and convolutional networks). Often 1-2 hidden layers are enought for classical regression/classification. </li><li>the number of neurons should give some &quot;flexibility&quot; to the architecture without exploding too much the number of parameters. An heuristic is to use a number of neurons ~20% higher than the input dimension. This is often fine-tuned using cross-validation as it risks to lead to overfitting</li><li>the activation function of the layers except the last one is chosen between a bunch of activation functions, nowadays it is almost always used a simple <em>Rectified Linear Unit</em> function, aka <code>relu</code>, defined as <code>relu(x) = max(0,x)</code>. The relu function has the advantage to add non-linearity to the transformation while remaining fast to compute (including the derivative) and avoiding the problem of vanishing or exploding the gradient (we&#39;ll see this aspect when dealing with the actual algorithm to obtain the weigths) </li><li>the activation function of the last layer depends on the nature of the labels we want the network to compute: if these are positive scalars we can use also here the <code>relu</code>, if we are doing a binary classification we can use the <code>sigmoid</code> function defined as <code>sigmoid(x) = 1/(1+exp(-x))</code> whose output is in the range [0,1] and which we can interpret as the probability of the class that we encode as <code>1</code>. If we are doing a multi-class classification we can use the <code>softmax</code> function whose output is a PMF of probabilities for each class.</li></ul><p>Let&#39;s now make an example of a single layer, single neuron with a 2D input <code>x=[2,4]</code>, weights <code>w=[2,1]</code>, <code>w₀ = 2</code> and activation function <code>f(x)=sin(x)</code>.</p><p>In such case the output of our network is <code>sin(2+2*1+4*2)</code>, i.e. -0.54. Note that with many neurons and many layers this becomes essentially (computationally) a problem of matrix multiplications, but matrix multiplication is easily parallelisable by the underliying BLAS/LAPACK libraries or, even better, by using GPU or TPU hardware, and running neural networks (and computing their gradients) is at the core of the demand for GPU computation.  </p><p>Let&#39;s now assume that the true label that we know being associated with our <span>$x$</span> is <code>y=-0.6</code>.</p><p>Out (basic) network did pretty well, but still did an <em>error</em>: -0.6 is not -0.54. The last element of a neural network is indeed to define an error metric (again, a function) between the output computed by the neural network and the true label. Commonly used error functions are the squared l-2 norm (i.e. <span>$\epsilon = \mid \mid \hat y - y \mid\mid ^2$</span>) for regression tasks and cross-entropy (i.e. <span>$\epsilon = - \sum_d p_d  * log(\hat p_d)$</span>) for classification jobs.</p><p>Before moving to the next section, where we will study how to put everything together and learn how to train the neural network in order to reduce this error, let&#39;s first observe that neural networks are powerful tools that can work on many sort of data, but they require however it to be encoded in a numerical form, as the computation is strictly numerical. If I have a categorical variable for example, I&#39;ll need to encode it expanding it to a set of dimensions where each dimension represent a single class and I encode with a indicator function if my record is that particular class or not. This is the most simple form of encoding and takes the name of <em>one hot encoding</em>:</p><p><img src="https://raw.githubusercontent.com/sylvaticus/SPMLJ/main/lessonsSources/04_-_NN_-_Neural_Networks/imgs/onehotencoding.png" alt="One-hot encoding"/></p><p>Note in the figure that using all the three columns leads to linearly dependancy, and while, yes, we could save a bit of resources by using only two columns instead of three, this is not a fundamental problem like it would be in a statistical analysis. </p><h3 id="Training-of-a-feed-forward-neural-network"><a class="docs-heading-anchor" href="#Training-of-a-feed-forward-neural-network">Training of a feed-forward neural network</a><a id="Training-of-a-feed-forward-neural-network-1"></a><a class="docs-heading-anchor-permalink" href="#Training-of-a-feed-forward-neural-network" title="Permalink"></a></h3><p>We now need a way to <em>learn</em> the parameters from the data, and a common way is to try to reduce the contribution of the individual parameter to the error made by the network. We will need first to define the  find the link between the individual parameter and the error</p><p><img src="https://raw.githubusercontent.com/sylvaticus/SPMLJ/main/lessonsSources/04_-_NN_-_Neural_Networks/imgs/learningRateEffect.png" alt="Learning rate effect"/></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../03_-_ML1_-_Introduction_to_Machine_Learning/0302-perceptron.html">« 0302-perceptron</a><a class="docs-footer-nextpage" href="0402.html">0402 »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.13 on <span class="colophon-date" title="Friday 18 March 2022 17:19">Friday 18 March 2022</span>. Using Julia version 1.7.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
