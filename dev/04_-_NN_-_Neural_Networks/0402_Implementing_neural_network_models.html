<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>0402 Implementing neural network models · SPMLJ</title><script async src="https://www.googletagmanager.com/gtag/js?id=G-Q39LHCRBB6"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-Q39LHCRBB6', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../index.html">SPMLJ</a></span></div><form class="docs-search" action="../search.html"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../index.html">Index</a></li><li><span class="tocitem">Lessons</span><ul><li><input class="collapse-toggle" id="menuitem-2-1" type="checkbox" checked/><label class="tocitem" for="menuitem-2-1"><span class="docs-label">NN - Neural Networks</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="0401_NeuralNetworksTheory.html">0401 NeuralNetworksTheory</a></li><li class="is-active"><a class="tocitem" href="0402_Implementing_neural_network_models.html">0402 Implementing neural network models</a><ul class="internal"><li><a class="tocitem" href="#Some-stuff-to-set-up-the-environment.."><span>Some stuff to set-up the environment..</span></a></li><li><a class="tocitem" href="#Feed-forward-neural-networks"><span>Feed-forward neural networks</span></a></li><li><a class="tocitem" href="#Convolutional-neural-networks"><span>Convolutional neural networks</span></a></li><li class="toplevel"><a class="tocitem" href="###-Recursive-neural-networks"><span>## Recursive neural networks</span></a></li><li class="toplevel"><a class="tocitem" href="#Generating-simulated-data"><span>Generating simulated data</span></a></li></ul></li></ul></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Lessons</a></li><li><a class="is-disabled">NN - Neural Networks</a></li><li class="is-active"><a href="0402_Implementing_neural_network_models.html">0402 Implementing neural network models</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="0402_Implementing_neural_network_models.html">0402 Implementing neural network models</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/sylvaticus/SPMLJ/blob/main/lessonsSources/04_-_NN_-_Neural_Networks/0402_Implementing_neural_network_models.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><pre></pre><h1 id="Neural-network-implementations"><a class="docs-heading-anchor" href="#Neural-network-implementations">0402 - Neural network implementations</a><a id="Neural-network-implementations-1"></a><a class="docs-heading-anchor-permalink" href="#Neural-network-implementations" title="Permalink"></a></h1><h2 id="Some-stuff-to-set-up-the-environment.."><a class="docs-heading-anchor" href="#Some-stuff-to-set-up-the-environment..">Some stuff to set-up the environment..</a><a id="Some-stuff-to-set-up-the-environment..-1"></a><a class="docs-heading-anchor-permalink" href="#Some-stuff-to-set-up-the-environment.." title="Permalink"></a></h2><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; cd(@__DIR__)</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; using Pkg</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; Pkg.activate(&quot;.&quot;)</code><code class="nohighlight hljs ansi" style="display:block;">  Activating project at `~/work/SPMLJ/SPMLJ/buildedDoc/04_-_NN_-_Neural_Networks`</code></pre><p>If using a Julia version different than 1.7 please uncomment and run the following line (the guarantee of reproducibility will however be lost) Pkg.resolve()</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; Pkg.instantiate()</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; using Random</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; Random.seed!(123)</code><code class="nohighlight hljs ansi" style="display:block;">Random.TaskLocalRNG()</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ENV[&quot;DATADEPS_ALWAYS_ACCEPT&quot;] = &quot;true&quot;</code><code class="nohighlight hljs ansi" style="display:block;">&quot;true&quot;</code></pre><p>We will <em>not</em> run cross validation here to find the optimal hypermarameters. The process will not be different than those we saw in the lesson on the Perceptron. Instead we focus on creating neural network models, train them based on data and evaluationg their predictions. For feed-forward neural networks (both for classification and regression) we will use <a href="https://github.com/sylvaticus/BetaML.jl">BetaML</a>, while for Convolutional Neural Networks and Recursive Neural NEtworks we will use the <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a> package.</p><h2 id="Feed-forward-neural-networks"><a class="docs-heading-anchor" href="#Feed-forward-neural-networks">Feed-forward neural networks</a><a id="Feed-forward-neural-networks-1"></a><a class="docs-heading-anchor-permalink" href="#Feed-forward-neural-networks" title="Permalink"></a></h2><h3 id="Binary-classification"><a class="docs-heading-anchor" href="#Binary-classification">Binary classification</a><a id="Binary-classification-1"></a><a class="docs-heading-anchor-permalink" href="#Binary-classification" title="Permalink"></a></h3><p>Data loading...</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; using BetaML, DelimitedFiles</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; data  = readdlm(joinpath(dirname(pathof(BetaML)),&quot;..&quot;,&quot;test&quot;,&quot;data&quot;,&quot;binary2DData.csv&quot;),&#39;\t&#39;)</code><code class="nohighlight hljs ansi" style="display:block;">200×3 Matrix{Float64}:
 -1.0   1.76    0.4
 -1.0   0.979   2.24
 -1.0   1.87   -0.977
 -1.0   0.95   -0.151
 -1.0  -0.103   0.411
 -1.0   0.144   1.45
 -1.0   0.761   0.122
 -1.0   0.444   0.334
 -1.0   1.49   -0.205
 -1.0   0.313  -0.854
  ⋮
  1.0  -0.256   0.977
  1.0   2.04    0.343
  1.0   1.01    0.528
  1.0   3.65    2.16
  1.0   2.57    1.78
  1.0   1.65    0.384
  1.0   1.71    1.24
  1.0   2.86    3.14
  1.0   3.47    2.85</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; nR   = size(data,1)</code><code class="nohighlight hljs ansi" style="display:block;">200</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; idx  = shuffle(1:nR)</code><code class="nohighlight hljs ansi" style="display:block;">200-element Vector{Int64}:
 123
 131
  74
  23
  19
  78
  43
 130
  83
 186
   ⋮
 137
 175
 182
  37
  71
  89
 142
  82
 170</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; data = data[idx,:]</code><code class="nohighlight hljs ansi" style="display:block;">200×3 Matrix{Float64}:
  1.0   1.69     0.324
  1.0   0.811    1.49
 -1.0  -0.913    1.12
 -1.0  -0.5097  -0.4381
 -1.0   1.23     1.2
 -1.0  -0.0985  -0.6635
 -1.0   1.49     1.9
  1.0   0.417    2.61
 -1.0  -1.23     0.844
  1.0   2.28     1.01
  ⋮
  1.0   3.96     2.39
  1.0   2.58     2.35
  1.0   2.93     2.34
 -1.0   1.14    -1.23
 -1.0  -1.49     0.439
 -1.0  -0.8034  -0.6895
  1.0   1.31     3.54
 -1.0   0.949    0.0876
  1.0   1.32     3.66</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; X    = copy(data[:,[2,3]])</code><code class="nohighlight hljs ansi" style="display:block;">200×2 Matrix{Float64}:
  1.69     0.324
  0.811    1.49
 -0.913    1.12
 -0.5097  -0.4381
  1.23     1.2
 -0.0985  -0.6635
  1.49     1.9
  0.417    2.61
 -1.23     0.844
  2.28     1.01
  ⋮
  3.96     2.39
  2.58     2.35
  2.93     2.34
  1.14    -1.23
 -1.49     0.439
 -0.8034  -0.6895
  1.31     3.54
  0.949    0.0876
  1.32     3.66</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; y    = max.(0,convert(Array{Int64,1},copy(data[:,1]))) # Converting labels from {-1,1} to {0,1}</code><code class="nohighlight hljs ansi" style="display:block;">200-element Vector{Int64}:
 1
 1
 0
 0
 0
 0
 0
 1
 0
 1
 ⋮
 1
 1
 1
 0
 0
 0
 1
 0
 1</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ((xtrain,xtest),(ytrain,ytest)) = partition([X,y],[0.7,0.3])</code><code class="nohighlight hljs ansi" style="display:block;">2-element Vector{Vector}:
 AbstractMatrix{Float64}[[1.23 1.2; 3.49 -0.07; … ; 2.69 2.69; 0.979 2.24], [-0.8708 -0.5788; 0.127 0.402; … ; -0.312 0.0562; -0.674 0.0318]]
 AbstractVector{Int64}[[0, 1, 0, 0, 0, 0, 1, 1, 1, 1  …  1, 1, 1, 0, 0, 0, 0, 1, 1, 0], [0, 0, 0, 1, 0, 1, 1, 1, 1, 1  …  0, 1, 1, 0, 0, 0, 1, 1, 0, 0]]</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; println(&quot;mydebug 0&quot;)</code><code class="nohighlight hljs ansi" style="display:block;">mydebug 0</code></pre><h4 id="Using-defaults-hidding-complexity"><a class="docs-heading-anchor" href="#Using-defaults-hidding-complexity">Using defaults - hidding complexity</a><a id="Using-defaults-hidding-complexity-1"></a><a class="docs-heading-anchor-permalink" href="#Using-defaults-hidding-complexity" title="Permalink"></a></h4><p>Model definition...</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; l1   = DenseLayer(2,5,f=tanh)</code><code class="nohighlight hljs ansi" style="display:block;">BetaML.Nn.DenseLayer([0.7114333605012103 -0.46840598025417524; -0.5672928157707955 0.4615531187467369; … ; 0.3460984398145658 -0.042222920388582996; -0.3373448835886066 0.13190727737286456], [-0.3003496305664325, 0.4564154347727757, -0.680410701712616, -0.4411408342040226, -0.727094508745879], tanh, nothing)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; l2   = DenseLayer(5,5,f=relu)</code><code class="nohighlight hljs ansi" style="display:block;">BetaML.Nn.DenseLayer([-0.24966234202395343 -0.1450088367408291 … -0.10738350533488994 0.7247141142027148; -0.6656904389257094 -0.25622478582029584 … 0.08056492594196829 -0.26225834425970906; … ; -0.05944120984655077 0.5919235655676248 … -0.6555792177003029 -0.48387955484315165; -0.34028100059184424 0.3808788075736441 … 0.1397207349693791 0.4839512714899258], [-0.5050945187284572, -0.4236829371301429, 0.4386534893642128, -0.014288651256534779, -0.22467496450689328], BetaML.Utils.relu, nothing)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; l3   = DenseLayer(5,1,f=sigmoid)</code><code class="nohighlight hljs ansi" style="display:block;">BetaML.Nn.DenseLayer([-0.6941847710439693 0.6253114897153567 … 0.9597552104353553 -0.3356524103426095], [-0.31962023986613586], BetaML.Utils.sigmoid, nothing)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; mynn = buildNetwork([l1,l2,l3],squaredCost)</code><code class="nohighlight hljs ansi" style="display:block;">BetaML.Nn.NN(BetaML.Nn.AbstractLayer[BetaML.Nn.DenseLayer([0.7114333605012103 -0.46840598025417524; -0.5672928157707955 0.4615531187467369; … ; 0.3460984398145658 -0.042222920388582996; -0.3373448835886066 0.13190727737286456], [-0.3003496305664325, 0.4564154347727757, -0.680410701712616, -0.4411408342040226, -0.727094508745879], tanh, nothing), BetaML.Nn.DenseLayer([-0.24966234202395343 -0.1450088367408291 … -0.10738350533488994 0.7247141142027148; -0.6656904389257094 -0.25622478582029584 … 0.08056492594196829 -0.26225834425970906; … ; -0.05944120984655077 0.5919235655676248 … -0.6555792177003029 -0.48387955484315165; -0.34028100059184424 0.3808788075736441 … 0.1397207349693791 0.4839512714899258], [-0.5050945187284572, -0.4236829371301429, 0.4386534893642128, -0.014288651256534779, -0.22467496450689328], BetaML.Utils.relu, nothing), BetaML.Nn.DenseLayer([-0.6941847710439693 0.6253114897153567 … 0.9597552104353553 -0.3356524103426095], [-0.31962023986613586], BetaML.Utils.sigmoid, nothing)], BetaML.Utils.squaredCost, nothing, false, &quot;Neural Network&quot;)</code></pre><p>Training...</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; train!(mynn,xtrain,ytrain)</code><code class="nohighlight hljs ansi" style="display:block;">***
*** Training Neural Network for 100 epochs with algorithm BetaML.Nn.ADAM.
Training.. 	 avg ϵ on (Epoch 1 Batch 4): 	 0.15699130044905962
Training.. 	 avg ϵ on (Epoch 10 Batch 4): 	 0.1567364224941614
Training.. 	 avg ϵ on (Epoch 20 Batch 4): 	 0.16405010084739874
Training.. 	 avg ϵ on (Epoch 30 Batch 4): 	 0.12619916483702404
Training.. 	 avg ϵ on (Epoch 40 Batch 4): 	 0.11126964564711478
Training.. 	 avg ϵ on (Epoch 50 Batch 4): 	 0.107809851602766
Training.. 	 avg ϵ on (Epoch 60 Batch 4): 	 0.09092457830472603
Training.. 	 avg ϵ on (Epoch 70 Batch 4): 	 0.06796781644461652
Training.. 	 avg ϵ on (Epoch 80 Batch 4): 	 0.06750170957599748
Training.. 	 avg ϵ on (Epoch 90 Batch 4): 	 0.053652390131002786
Training.. 	 avg ϵ on (Epoch 100 Batch 4): 	 0.03537649303167354
Training of 100 epoch completed. Final epoch error: 0.04901051694560962.
(epochs = 100, ϵ_epochs = [0.16917361724292546, 0.1673212323433058, 0.16559724017832195, 0.16391033138766398, 0.1622650152221565, 0.1606237255175449, 0.15904696343918534, 0.1574994634681001, 0.1560231922458023, 0.15463191100381887  …  0.05271910822952187, 0.05223613716710994, 0.05176930524991748, 0.05132435927692999, 0.05089006848008419, 0.05047956943681352, 0.05010007704280328, 0.04973158120785569, 0.049363124241603495, 0.04901051694560962], θ_epochs = Any[])</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ŷtrain         = predict(mynn, xtrain) |&gt; makeColVector .|&gt; round .|&gt; Int</code><code class="nohighlight hljs ansi" style="display:block;">140-element Vector{Int64}:
 1
 1
 0
 0
 0
 0
 1
 1
 1
 1
 ⋮
 1
 1
 0
 0
 0
 0
 1
 1
 1</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ŷtest          = predict(mynn, xtest)  |&gt; makeColVector .|&gt; round .|&gt; Int</code><code class="nohighlight hljs ansi" style="display:block;">60-element Vector{Int64}:
 0
 0
 0
 1
 0
 1
 1
 1
 1
 0
 ⋮
 1
 1
 0
 0
 1
 1
 1
 0
 0</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; trainAccuracy  = accuracy(ŷtrain,ytrain)</code><code class="nohighlight hljs ansi" style="display:block;">0.9214285714285714</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; testAccuracy   = accuracy(ŷtest,ytest)</code><code class="nohighlight hljs ansi" style="display:block;">0.9333333333333333</code></pre><h4 id="Specifying-all-options"><a class="docs-heading-anchor" href="#Specifying-all-options">Specifying all options</a><a id="Specifying-all-options-1"></a><a class="docs-heading-anchor-permalink" href="#Specifying-all-options" title="Permalink"></a></h4><p>Model definition...</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; l1   = DenseLayer(2,5,f=tanh, df= dtanh,rng=copy(FIXEDRNG))</code><code class="nohighlight hljs ansi" style="display:block;">BetaML.Nn.DenseLayer([-0.5906259287724187 -0.030940667488043583; -0.24536094247801754 0.48878436852866713; … ; -0.8466984663657858 0.027858152254290336; -0.11481842953412624 -0.009180955255778445], [0.37280922460295074, 0.35842073435858446, 0.696721837875378, 0.8225343299333828, 0.44293565195483586], tanh, BetaML.Utils.dtanh)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; l2   = DenseLayer(5,5,f=relu,df=drelu,rng=copy(FIXEDRNG))</code><code class="nohighlight hljs ansi" style="display:block;">BetaML.Nn.DenseLayer([-0.49415310523844497 -0.025886819681528617 … 0.09224620317870791 -0.2298725180847525; -0.20528369264408397 0.40894634274263597 … 0.01065168104625569 0.04243100148415224; … ; -0.7083987613359595 0.023307802404264888 … -0.3619336136169306 -0.4825559053138102; -0.09606399030062296 -0.0076813382679077336 … -0.3824319138128976 0.21378287379211514], [-0.11178452751598777, -0.2115472973462721, 0.12029035453379433, -0.06939594013486328, -0.5177091839997635], BetaML.Utils.relu, BetaML.Utils.drelu)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; l3   = DenseLayer(5,1,f=sigmoid,df=dsigmoid,rng=copy(FIXEDRNG))</code><code class="nohighlight hljs ansi" style="display:block;">BetaML.Nn.DenseLayer([-0.6379489156883928 -0.2650201076194998 … -0.9145388683760447 -0.12401807820151456], [-0.033419740504278206], BetaML.Utils.sigmoid, BetaML.Utils.dsigmoid)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; mynn = buildNetwork([l1,l2,l3],squaredCost,dcf=dSquaredCost,name=&quot;A classification task&quot;) # or crossEntropy / dCrossEntropy</code><code class="nohighlight hljs ansi" style="display:block;">BetaML.Nn.NN(BetaML.Nn.AbstractLayer[BetaML.Nn.DenseLayer([-0.5906259287724187 -0.030940667488043583; -0.24536094247801754 0.48878436852866713; … ; -0.8466984663657858 0.027858152254290336; -0.11481842953412624 -0.009180955255778445], [0.37280922460295074, 0.35842073435858446, 0.696721837875378, 0.8225343299333828, 0.44293565195483586], tanh, BetaML.Utils.dtanh), BetaML.Nn.DenseLayer([-0.49415310523844497 -0.025886819681528617 … 0.09224620317870791 -0.2298725180847525; -0.20528369264408397 0.40894634274263597 … 0.01065168104625569 0.04243100148415224; … ; -0.7083987613359595 0.023307802404264888 … -0.3619336136169306 -0.4825559053138102; -0.09606399030062296 -0.0076813382679077336 … -0.3824319138128976 0.21378287379211514], [-0.11178452751598777, -0.2115472973462721, 0.12029035453379433, -0.06939594013486328, -0.5177091839997635], BetaML.Utils.relu, BetaML.Utils.drelu), BetaML.Nn.DenseLayer([-0.6379489156883928 -0.2650201076194998 … -0.9145388683760447 -0.12401807820151456], [-0.033419740504278206], BetaML.Utils.sigmoid, BetaML.Utils.dsigmoid)], BetaML.Utils.squaredCost, BetaML.Utils.dSquaredCost, false, &quot;A classification task&quot;)</code></pre><p>Training...</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; function myOwnTrainingInfo(nn,x,y;n,nBatches,epochs,verbosity,nEpoch,nBatch)
           if verbosity == NONE
               return false # doesn&#39;t stop the training
           end
           nMsgDict = Dict(LOW =&gt; 0, STD =&gt; 10,HIGH =&gt; 100, FULL =&gt; n)
           nMsgs = nMsgDict[verbosity]
           batchSize = size(x,1)
           if verbosity == FULL || ( nBatch == nBatches &amp;&amp; ( nEpoch == 1  || nEpoch % ceil(epochs/nMsgs) == 0))
       
              ϵ = loss(nn,x,y)
              println(&quot;Training.. \t avg ϵ on (Epoch $nEpoch Batch $nBatch): \t $(ϵ)&quot;)
           end
           return false
        end</code><code class="nohighlight hljs ansi" style="display:block;">myOwnTrainingInfo (generic function with 1 method)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; train!(mynn,xtrain,ytrain,epochs=300,batchSize=6,sequential=false,verbosity=STD,cb=myOwnTrainingInfo,optAlg=ADAM(η=t -&gt; 0.001, λ=1.0, β₁=0.9, β₂=0.999, ϵ=1e-8),rng=copy(FIXEDRNG))</code><code class="nohighlight hljs ansi" style="display:block;">***
*** Training A classification task for 300 epochs with algorithm BetaML.Nn.ADAM.
Training.. 	 avg ϵ on (Epoch 1 Batch 23): 	 0.1840511466243301
Training.. 	 avg ϵ on (Epoch 30 Batch 23): 	 0.09731559879877578
Training.. 	 avg ϵ on (Epoch 60 Batch 23): 	 0.02742948436189047
Training.. 	 avg ϵ on (Epoch 90 Batch 23): 	 0.011200621372772343
Training.. 	 avg ϵ on (Epoch 120 Batch 23): 	 0.024872170024018602
Training.. 	 avg ϵ on (Epoch 150 Batch 23): 	 0.0010080454466619426
Training the Neural Network... 52%|██████████▉          |  ETA: 0:00:01Training.. 	 avg ϵ on (Epoch 180 Batch 23): 	 0.007429493035091297
Training.. 	 avg ϵ on (Epoch 210 Batch 23): 	 0.005038426745621506
Training.. 	 avg ϵ on (Epoch 240 Batch 23): 	 0.027626025065464535
Training.. 	 avg ϵ on (Epoch 270 Batch 23): 	 0.06717605514987254
Training.. 	 avg ϵ on (Epoch 300 Batch 23): 	 0.0036453813292615382
Training the Neural Network...100%|█████████████████████| Time: 0:00:02
Training of 300 epoch completed. Final epoch error: 0.030688406769744627.
(epochs = 300, ϵ_epochs = [0.20942586889145268, 0.1899818931956108, 0.17228518378814875, 0.1568057717892211, 0.14442720165443992, 0.13658860446242982, 0.13029632237888944, 0.12529286702177192, 0.12156299468506765, 0.11868528241639893  …  0.030712966557551197, 0.03070109140839259, 0.030701931213792794, 0.03069717822681469, 0.030693948444971676, 0.030694475551419124, 0.030694382730006697, 0.030691489834036575, 0.030689035284208525, 0.030688406769744627], θ_epochs = Any[])</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ŷtrain         = predict(mynn, xtrain) |&gt; makeColVector .|&gt; round .|&gt; Int</code><code class="nohighlight hljs ansi" style="display:block;">140-element Vector{Int64}:
 1
 1
 0
 0
 0
 0
 1
 1
 1
 1
 ⋮
 1
 1
 0
 0
 0
 0
 1
 1
 1</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ŷtest          = predict(mynn, xtest)  |&gt; makeColVector .|&gt; round .|&gt; Int</code><code class="nohighlight hljs ansi" style="display:block;">60-element Vector{Int64}:
 0
 0
 0
 1
 0
 1
 1
 1
 1
 0
 ⋮
 1
 1
 0
 0
 1
 1
 1
 0
 0</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; trainAccuracy  = accuracy(ŷtrain,ytrain)</code><code class="nohighlight hljs ansi" style="display:block;">0.9214285714285714</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; testAccuracy   = accuracy(ŷtest,ytest)</code><code class="nohighlight hljs ansi" style="display:block;">0.95</code></pre><h3 id="Multinomial-classification"><a class="docs-heading-anchor" href="#Multinomial-classification">Multinomial classification</a><a id="Multinomial-classification-1"></a><a class="docs-heading-anchor-permalink" href="#Multinomial-classification" title="Permalink"></a></h3><p>We want to determine the plant specie given some bothanic measures of the flower</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; iris     = readdlm(joinpath(dirname(Base.find_package(&quot;BetaML&quot;)),&quot;..&quot;,&quot;test&quot;,&quot;data&quot;,&quot;iris.csv&quot;),&#39;,&#39;,skipstart=1)</code><code class="nohighlight hljs ansi" style="display:block;">150×5 Matrix{Any}:
 5.1  3.5  1.4  0.2  &quot;setosa&quot;
 4.9  3.0  1.4  0.2  &quot;setosa&quot;
 4.7  3.2  1.3  0.2  &quot;setosa&quot;
 4.6  3.1  1.5  0.2  &quot;setosa&quot;
 5.0  3.6  1.4  0.2  &quot;setosa&quot;
 5.4  3.9  1.7  0.4  &quot;setosa&quot;
 4.6  3.4  1.4  0.3  &quot;setosa&quot;
 5.0  3.4  1.5  0.2  &quot;setosa&quot;
 4.4  2.9  1.4  0.2  &quot;setosa&quot;
 4.9  3.1  1.5  0.1  &quot;setosa&quot;
 ⋮
 6.9  3.1  5.1  2.3  &quot;virginica&quot;
 5.8  2.7  5.1  1.9  &quot;virginica&quot;
 6.8  3.2  5.9  2.3  &quot;virginica&quot;
 6.7  3.3  5.7  2.5  &quot;virginica&quot;
 6.7  3.0  5.2  2.3  &quot;virginica&quot;
 6.3  2.5  5.0  1.9  &quot;virginica&quot;
 6.5  3.0  5.2  2.0  &quot;virginica&quot;
 6.2  3.4  5.4  2.3  &quot;virginica&quot;
 5.9  3.0  5.1  1.8  &quot;virginica&quot;</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; iris     = iris[shuffle(axes(iris, 1)), :] # Shuffle the records, as they aren&#39;t by default</code><code class="nohighlight hljs ansi" style="display:block;">150×5 Matrix{Any}:
 7.6  3.0  6.6  2.1  &quot;virginica&quot;
 6.5  3.0  5.2  2.0  &quot;virginica&quot;
 6.3  3.3  6.0  2.5  &quot;virginica&quot;
 6.2  3.4  5.4  2.3  &quot;virginica&quot;
 6.3  3.4  5.6  2.4  &quot;virginica&quot;
 5.5  3.5  1.3  0.2  &quot;setosa&quot;
 5.0  2.3  3.3  1.0  &quot;versicolor&quot;
 7.1  3.0  5.9  2.1  &quot;virginica&quot;
 6.3  2.3  4.4  1.3  &quot;versicolor&quot;
 5.2  3.5  1.5  0.2  &quot;setosa&quot;
 ⋮
 5.1  2.5  3.0  1.1  &quot;versicolor&quot;
 7.2  3.0  5.8  1.6  &quot;virginica&quot;
 4.8  3.1  1.6  0.2  &quot;setosa&quot;
 5.1  3.5  1.4  0.3  &quot;setosa&quot;
 4.9  3.6  1.4  0.1  &quot;setosa&quot;
 6.1  2.6  5.6  1.4  &quot;virginica&quot;
 6.3  2.5  4.9  1.5  &quot;versicolor&quot;
 5.0  3.6  1.4  0.2  &quot;setosa&quot;
 7.7  2.6  6.9  2.3  &quot;virginica&quot;</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; x        = convert(Array{Float64,2}, iris[:,1:4])</code><code class="nohighlight hljs ansi" style="display:block;">150×4 Matrix{Float64}:
 7.6  3.0  6.6  2.1
 6.5  3.0  5.2  2.0
 6.3  3.3  6.0  2.5
 6.2  3.4  5.4  2.3
 6.3  3.4  5.6  2.4
 5.5  3.5  1.3  0.2
 5.0  2.3  3.3  1.0
 7.1  3.0  5.9  2.1
 6.3  2.3  4.4  1.3
 5.2  3.5  1.5  0.2
 ⋮
 5.1  2.5  3.0  1.1
 7.2  3.0  5.8  1.6
 4.8  3.1  1.6  0.2
 5.1  3.5  1.4  0.3
 4.9  3.6  1.4  0.1
 6.1  2.6  5.6  1.4
 6.3  2.5  4.9  1.5
 5.0  3.6  1.4  0.2
 7.7  2.6  6.9  2.3</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; y        = map(x-&gt;Dict(&quot;setosa&quot; =&gt; 1, &quot;versicolor&quot; =&gt; 2, &quot;virginica&quot; =&gt;3)[x],iris[:, 5]) # Convert the target column to numbers</code><code class="nohighlight hljs ansi" style="display:block;">150-element Vector{Int64}:
 3
 3
 3
 3
 3
 1
 2
 3
 2
 1
 ⋮
 2
 3
 1
 1
 1
 3
 2
 1
 3</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ((xtrain,xtest),(ytrain,ytest)) = partition([x,y],[0.8,0.2],shuffle=false)</code><code class="nohighlight hljs ansi" style="display:block;">2-element Vector{Vector}:
 AbstractMatrix{Float64}[[7.6 3.0 6.6 2.1; 6.5 3.0 5.2 2.0; … ; 6.3 3.3 4.7 1.6; 6.3 2.7 4.9 1.8], [4.4 2.9 1.4 0.2; 5.5 2.3 4.0 1.3; … ; 5.0 3.6 1.4 0.2; 7.7 2.6 6.9 2.3]]
 AbstractVector{Int64}[[3, 3, 3, 3, 3, 1, 2, 3, 2, 1  …  3, 2, 2, 1, 3, 1, 3, 2, 2, 3], [1, 2, 2, 1, 1, 2, 2, 3, 2, 2  …  3, 2, 3, 1, 1, 1, 3, 2, 1, 3]]</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ytrain_oh = oneHotEncoder(ytrain) # Convert to One-hot representation (e.g. 2 =&gt; [0 1 0], 3 =&gt; [0 0 1])</code><code class="nohighlight hljs ansi" style="display:block;">120×3 Matrix{Int64}:
 0  0  1
 0  0  1
 0  0  1
 0  0  1
 0  0  1
 1  0  0
 0  1  0
 0  0  1
 0  1  0
 1  0  0
 ⋮
 0  1  0
 0  1  0
 1  0  0
 0  0  1
 1  0  0
 0  0  1
 0  1  0
 0  1  0
 0  0  1</code></pre><p>Define the Artificial Neural Network model</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; l1   = DenseLayer(4,10,f=relu) # Activation function is ReLU</code><code class="nohighlight hljs ansi" style="display:block;">BetaML.Nn.DenseLayer([0.08810718257267769 0.4438377967110334 -0.37264137236830386 -0.5675793667600653; 0.28330514434945786 -0.6492760216930226 -0.38373378010496734 -0.2068666764154719; … ; 0.49212947049159106 0.3525703845340231 -0.2738356656608552 0.04824168948786289; 0.3620453496524063 -0.16339999103332714 -0.5223113549978162 0.27679840195925676], [-0.30345566664660867, 0.6221600121053414, 0.2583008737739664, -0.16376191205074286, 0.26464602008093285, 0.05938240548430895, 0.1776504251523865, 0.03896925233156634, 0.45328794043691545, 0.6401364079583236], BetaML.Utils.relu, nothing)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; l2   = DenseLayer(10,3)        # Activation function is identity by default</code><code class="nohighlight hljs ansi" style="display:block;">BetaML.Nn.DenseLayer([-0.16526408831202344 0.09443683140104497 … 0.2817357432654801 -0.5052590524413781; 0.3925951602664908 -0.0028374365951118197 … 0.47488714495141426 -0.1107389715437852; 0.37642387099674224 -0.2156111929531354 … 0.019362140605474698 0.030582101562182662], [-0.1316500788791678, 0.40702832655345755, -0.4739179415699], identity, nothing)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; l3   = VectorFunctionLayer(3,f=softmax) # Add a (parameterless) layer whose activation function (softMax in this case) is defined to all its nodes at once</code><code class="nohighlight hljs ansi" style="display:block;">BetaML.Nn.VectorFunctionLayer{0}(fill(NaN), 3, 3, BetaML.Utils.softmax, nothing, nothing)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; mynn = buildNetwork([l1,l2,l3],crossEntropy,name=&quot;Multinomial logistic regression Model Sepal&quot;) # Build the NN and use the squared cost (aka MSE) as error function (crossEntropy could also be used)</code><code class="nohighlight hljs ansi" style="display:block;">BetaML.Nn.NN(BetaML.Nn.AbstractLayer[BetaML.Nn.DenseLayer([0.08810718257267769 0.4438377967110334 -0.37264137236830386 -0.5675793667600653; 0.28330514434945786 -0.6492760216930226 -0.38373378010496734 -0.2068666764154719; … ; 0.49212947049159106 0.3525703845340231 -0.2738356656608552 0.04824168948786289; 0.3620453496524063 -0.16339999103332714 -0.5223113549978162 0.27679840195925676], [-0.30345566664660867, 0.6221600121053414, 0.2583008737739664, -0.16376191205074286, 0.26464602008093285, 0.05938240548430895, 0.1776504251523865, 0.03896925233156634, 0.45328794043691545, 0.6401364079583236], BetaML.Utils.relu, nothing), BetaML.Nn.DenseLayer([-0.16526408831202344 0.09443683140104497 … 0.2817357432654801 -0.5052590524413781; 0.3925951602664908 -0.0028374365951118197 … 0.47488714495141426 -0.1107389715437852; 0.37642387099674224 -0.2156111929531354 … 0.019362140605474698 0.030582101562182662], [-0.1316500788791678, 0.40702832655345755, -0.4739179415699], identity, nothing), BetaML.Nn.VectorFunctionLayer{0}(fill(NaN), 3, 3, BetaML.Utils.softmax, nothing, nothing)], BetaML.Utils.crossEntropy, nothing, false, &quot;Multinomial logistic regression Model Sepal&quot;)</code></pre><p>Training it (default to ADAM)</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; res = train!(mynn,scale(xtrain),ytrain_oh,batchSize=6) # Use optAlg=SGD() to use Stochastic Gradient Descent instead</code><code class="nohighlight hljs ansi" style="display:block;">***
*** Training Multinomial logistic regression Model Sepal for 100 epochs with algorithm BetaML.Nn.ADAM.
Training.. 	 avg ϵ on (Epoch 1 Batch 20): 	 0.9948127457289552
Training.. 	 avg ϵ on (Epoch 10 Batch 20): 	 0.5837865124896999
Training.. 	 avg ϵ on (Epoch 20 Batch 20): 	 0.36694017936439677
Training.. 	 avg ϵ on (Epoch 30 Batch 20): 	 0.22011985671566248
Training.. 	 avg ϵ on (Epoch 40 Batch 20): 	 0.31818952808538375
Training.. 	 avg ϵ on (Epoch 50 Batch 20): 	 0.13230320748958713
Training.. 	 avg ϵ on (Epoch 60 Batch 20): 	 0.2772086520705127
Training.. 	 avg ϵ on (Epoch 70 Batch 20): 	 0.02489704393546822
Training.. 	 avg ϵ on (Epoch 80 Batch 20): 	 0.03742574620568454
Training.. 	 avg ϵ on (Epoch 90 Batch 20): 	 0.03262732414394961
Training.. 	 avg ϵ on (Epoch 100 Batch 20): 	 0.027611497248841266
Training of 100 epoch completed. Final epoch error: 0.08016519346344649.
(epochs = 100, ϵ_epochs = [1.2671601037241338, 1.158705164288321, 1.0633035006544664, 0.9714764470962363, 0.8981174025629668, 0.8353360876340743, 0.779735982869415, 0.7282557325133754, 0.6848860468161013, 0.6412222884724327  …  0.08936372126355155, 0.08815735892785255, 0.08705601536883403, 0.08594278536952424, 0.08489919240480992, 0.08390185721584734, 0.08290798132642287, 0.08195111840306994, 0.08106313154147712, 0.08016519346344649], θ_epochs = Any[])</code></pre><p>Test it</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; ŷtrain        = predict(mynn,scale(xtrain))   # Note the scaling function</code><code class="nohighlight hljs ansi" style="display:block;">120×3 Matrix{Float64}:
 2.32979e-8   0.00515854  0.994841
 7.2735e-6    0.0653419   0.934651
 4.09545e-7   0.00458407  0.995416
 1.76891e-5   0.023216    0.976766
 4.16751e-6   0.0110585   0.988937
 0.994953     0.00504644  1.28891e-7
 0.00912393   0.983288    0.00758834
 2.25892e-7   0.0134471   0.986553
 3.73313e-5   0.905279    0.094684
 0.9953       0.00469946  2.1606e-7
 ⋮
 0.00746492   0.817767    0.174768
 0.000369756  0.808816    0.190814
 0.995381     0.00461871  5.23794e-7
 5.2395e-7    0.00987338  0.990126
 0.991457     0.00854238  9.9192e-7
 5.90119e-5   0.105251    0.89469
 0.0352721    0.940225    0.0245033
 0.011004     0.780962    0.208034
 1.06058e-5   0.209703    0.790287</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ŷtest         = predict(mynn,scale(xtest))</code><code class="nohighlight hljs ansi" style="display:block;">30×3 Matrix{Float64}:
 0.995291     0.00470855   3.59367e-7
 0.000557691  0.948207     0.0512354
 0.0875071    0.896935     0.0155581
 0.996055     0.0039452    1.52129e-7
 0.994537     0.00546294   2.91913e-7
 0.00208604   0.98966      0.00825415
 0.000107087  0.355057     0.644836
 8.97297e-5   0.334001     0.665909
 0.00842967   0.963262     0.028308
 0.0137498    0.940738     0.0455124
 ⋮
 0.0527497    0.943359     0.00389135
 8.14023e-5   0.23899      0.760929
 0.995957     0.00404292   2.58076e-7
 0.994855     0.00514453   1.9656e-7
 0.9974       0.00260001   6.56563e-8
 8.41223e-5   0.419339     0.580577
 6.4054e-5    0.574292     0.425643
 0.996376     0.00362363   1.09204e-7
 1.51542e-10  0.000823812  0.999176</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; trainAccuracy = accuracy(ŷtrain,ytrain)</code><code class="nohighlight hljs ansi" style="display:block;">0.975</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; testAccuracy  = accuracy(ŷtest,ytest,tol=1,ignoreLabels=false)</code><code class="nohighlight hljs ansi" style="display:block;">0.9666666666666667</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; cm = ConfusionMatrix(ŷtest,ytest, labels=[&quot;setosa&quot;, &quot;versicolor&quot;, &quot;virginica&quot;])</code><code class="nohighlight hljs ansi" style="display:block;">BetaML.Utils.ConfusionMatrix{Int64}([1, 2, 3], [&quot;setosa&quot;, &quot;versicolor&quot;, &quot;virginica&quot;], 0.9666666666666667, 0.033333333333333326, [9, 12, 9], [9, 11, 10], [9 0 0; 0 11 1; 0 0 9], [1.0 0.0 0.0; 0.0 0.9166666666666666 0.08333333333333333; 0.0 0.0 1.0], [9, 11, 9], [21, 18, 20], [0, 0, 1], [0, 1, 0], [1.0, 1.0, 0.9], [1.0, 0.9166666666666666, 1.0], [1.0, 1.0, 0.9523809523809523], [1.0, 0.9565217391304348, 0.9473684210526315], (0.9666666666666667, 0.9700000000000001), (0.9722222222222222, 0.9666666666666667), (0.9841269841269842, 0.9857142857142857), (0.9679633867276888, 0.9668192219679634))</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; println(cm)</code><code class="nohighlight hljs ansi" style="display:block;">
-----------------------------------------------------------------

*** CONFUSION MATRIX ***

Scores actual (rows) vs predicted (columns):


Normalised scores actual (rows) vs predicted (columns):


 *** CONFUSION REPORT ***

- Accuracy:               0.9666666666666667
- Misclassification rate: 0.033333333333333326
- Number of classes:      3

  N Class      precision   recall  specificity  f1Score  actualCount  predictedCount
                             TPR       TNR                 support                  

  1 setosa         1.000    1.000        1.000    1.000            9               9
  2 versicolor     1.000    0.917        1.000    0.957           12              11
  3 virginica      0.900    1.000        0.952    0.947            9              10

- Simple   avg.    0.967    0.972        0.984    0.968
- Weigthed avg.    0.970    0.967        0.986    0.967

-----------------------------------------------------------------</code></pre><h3 id="Regression"><a class="docs-heading-anchor" href="#Regression">Regression</a><a id="Regression-1"></a><a class="docs-heading-anchor-permalink" href="#Regression" title="Permalink"></a></h3><p>Data Loading and processing..</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; using Pipe, HTTP, CSV, Plots, DataFrames</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; urlData = &quot;https://www4.stat.ncsu.edu/~boos/var.select/diabetes.tab.txt&quot;</code><code class="nohighlight hljs ansi" style="display:block;">&quot;https://www4.stat.ncsu.edu/~boos/var.select/diabetes.tab.txt&quot;</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; data = @pipe HTTP.get(urlData).body |&gt; CSV.File(_, delim=&#39;\t&#39;) |&gt; DataFrame</code><code class="nohighlight hljs ansi" style="display:block;">442×11 DataFrame
 Row │ AGE    SEX    BMI      BP       S1     S2       S3       S4       S5    ⋯
     │ Int64  Int64  Float64  Float64  Int64  Float64  Float64  Float64  Float ⋯
─────┼──────────────────────────────────────────────────────────────────────────
   1 │    59      2     32.1   101.0     157     93.2     38.0     4.0    4.85 ⋯
   2 │    48      1     21.6    87.0     183    103.2     70.0     3.0    3.89
   3 │    72      2     30.5    93.0     156     93.6     41.0     4.0    4.67
   4 │    24      1     25.3    84.0     198    131.4     40.0     5.0    4.89
   5 │    50      1     23.0   101.0     192    125.4     52.0     4.0    4.29 ⋯
   6 │    23      1     22.6    89.0     139     64.8     61.0     2.0    4.18
   7 │    36      2     22.0    90.0     160     99.6     50.0     3.0    3.95
   8 │    66      2     26.2   114.0     255    185.0     56.0     4.55   4.24
  ⋮  │   ⋮      ⋮       ⋮        ⋮       ⋮       ⋮        ⋮        ⋮        ⋮  ⋱
 436 │    45      1     24.2    83.0     177    118.4     45.0     4.0    4.21 ⋯
 437 │    33      1     19.5    80.0     171     85.4     75.0     2.0    3.97
 438 │    60      2     28.2   112.0     185    113.8     42.0     4.0    4.98
 439 │    47      2     24.9    75.0     225    166.0     42.0     5.0    4.44
 440 │    60      2     24.9    99.67    162    106.6     43.0     3.77   4.12 ⋯
 441 │    36      1     30.0    95.0     201    125.2     42.0     4.79   5.12
 442 │    36      1     19.6    71.0     250    133.2     97.0     3.0    4.59
                                                  3 columns and 427 rows omitted</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; sex_oh = oneHotEncoder(data.SEX)</code><code class="nohighlight hljs ansi" style="display:block;">442×2 Matrix{Int64}:
 0  1
 1  0
 0  1
 1  0
 1  0
 1  0
 0  1
 0  1
 0  1
 1  0
 ⋮
 1  0
 1  0
 1  0
 1  0
 0  1
 0  1
 0  1
 1  0
 1  0</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; X = hcat(data.AGE, Matrix(data[:,3:10]),sex_oh)</code><code class="nohighlight hljs ansi" style="display:block;">442×11 Matrix{Float64}:
 59.0  32.1  101.0   157.0   93.2  38.0  4.0   4.8598   87.0  0.0  1.0
 48.0  21.6   87.0   183.0  103.2  70.0  3.0   3.8918   69.0  1.0  0.0
 72.0  30.5   93.0   156.0   93.6  41.0  4.0   4.6728   85.0  0.0  1.0
 24.0  25.3   84.0   198.0  131.4  40.0  5.0   4.8903   89.0  1.0  0.0
 50.0  23.0  101.0   192.0  125.4  52.0  4.0   4.2905   80.0  1.0  0.0
 23.0  22.6   89.0   139.0   64.8  61.0  2.0   4.1897   68.0  1.0  0.0
 36.0  22.0   90.0   160.0   99.6  50.0  3.0   3.9512   82.0  0.0  1.0
 66.0  26.2  114.0   255.0  185.0  56.0  4.55  4.2485   92.0  0.0  1.0
 60.0  32.1   83.0   179.0  119.4  42.0  4.0   4.4773   94.0  0.0  1.0
 29.0  30.0   85.0   180.0   93.4  43.0  4.0   5.3845   88.0  1.0  0.0
  ⋮                                 ⋮                              ⋮
 41.0  20.8   86.0   223.0  128.2  83.0  3.0   4.0775   89.0  1.0  0.0
 53.0  26.5   97.0   193.0  122.4  58.0  3.0   4.1431   99.0  1.0  0.0
 45.0  24.2   83.0   177.0  118.4  45.0  4.0   4.2195   82.0  1.0  0.0
 33.0  19.5   80.0   171.0   85.4  75.0  2.0   3.9703   80.0  1.0  0.0
 60.0  28.2  112.0   185.0  113.8  42.0  4.0   4.9836   93.0  0.0  1.0
 47.0  24.9   75.0   225.0  166.0  42.0  5.0   4.4427  102.0  0.0  1.0
 60.0  24.9   99.67  162.0  106.6  43.0  3.77  4.1271   95.0  0.0  1.0
 36.0  30.0   95.0   201.0  125.2  42.0  4.79  5.1299   85.0  1.0  0.0
 36.0  19.6   71.0   250.0  133.2  97.0  3.0   4.5951   92.0  1.0  0.0</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; y = data.Y</code><code class="nohighlight hljs ansi" style="display:block;">442-element Vector{Int64}:
 151
  75
 141
 206
 135
  97
 138
  63
 110
 310
   ⋮
  72
  49
  64
  48
 178
 104
 132
 220
  57</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; (xtrain,xval),(ytrain,yval) = partition([X,y],[0.8,0.2])</code><code class="nohighlight hljs ansi" style="display:block;">2-element Vector{Vector}:
 AbstractMatrix{Float64}[[61.0 32.7 … 0.0 1.0; 48.0 23.9 … 1.0 0.0; … ; 41.0 24.9 … 0.0 1.0; 51.0 23.5 … 1.0 0.0], [34.0 35.5 … 0.0 1.0; 47.0 28.6 … 1.0 0.0; … ; 57.0 26.9 … 1.0 0.0; 71.0 26.1 … 0.0 1.0]]
 AbstractVector{Int64}[[259, 152, 179, 116, 283, 69, 77, 155, 198, 65  …  52, 51, 177, 127, 144, 185, 142, 310, 53, 154], [243, 121, 61, 131, 268, 235, 253, 138, 60, 202  …  96, 179, 131, 85, 288, 95, 217, 217, 249, 139]]</code></pre><p>Model definition...</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; l1   = DenseLayer(11,20,f=relu)</code><code class="nohighlight hljs ansi" style="display:block;">BetaML.Nn.DenseLayer([-0.2924033262554372 -0.2539644635579579 … 0.2708283712107636 -0.3397651148559973; 0.3200055913343534 0.34843279884887385 … 0.2850466882977612 0.40310296345588464; … ; -0.14567183246806553 -0.3460098530593056 … -0.10977686864464409 -0.11023709217691652; 0.3731613045812631 -0.10210748942685816 … -0.338215061941476 0.1408952609322956], [0.09592800790203365, 0.15860323464133336, 0.3551464313117509, -0.04620145965674505, -0.36129502082159637, 0.31020624847324435, 0.1962275378594674, -0.19652653926097655, 0.16684584295204136, 0.23639369054502485, 0.1755727972983427, -0.3673393833437488, -0.40059902215997695, 0.1933896650049401, 0.4044925415855833, -0.28878525234767244, -0.20173882418331515, -0.17491544700243872, -0.2198048834412415, -0.4046519243293346], BetaML.Utils.relu, nothing)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; l2   = DenseLayer(20,20,f=relu)</code><code class="nohighlight hljs ansi" style="display:block;">BetaML.Nn.DenseLayer([-0.2927494810685075 0.22219075184423648 … 0.2673123538870123 0.20410674617461927; -0.2320135040440592 0.3292687339809626 … 0.11745976371776506 0.2749281212619707; … ; 0.29099415187214067 0.12183960430470392 … -0.09269557835592285 -0.01060543642578815; -0.09588210686525739 -0.17734571306244018 … -0.04581198248266788 -0.17839653524534227], [-0.3280003996511005, 0.04546914264795188, -0.2556527937208482, 0.22780164042762602, -0.36702175649579394, -0.08101120595044425, -0.26900852855358964, -0.003716502775307995, -0.06992958602385585, 0.0866901916940902, 0.3087058654826345, 0.28944003386379064, -0.09506133807591433, 0.30825057340536494, -0.34752756073212243, 0.3175303340526932, 0.2302359016855347, -0.05159916226591843, 0.1427898320160052, 0.25687884208929207], BetaML.Utils.relu, nothing)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; l3   = DenseLayer(20,1,f=relu) # y is positive</code><code class="nohighlight hljs ansi" style="display:block;">BetaML.Nn.DenseLayer([-0.19336934131814693 -0.0012534394550661743 … -0.40957078962790305 -0.28122106464306357], [-0.04427898450034906], BetaML.Utils.relu, nothing)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; mynn = buildNetwork([l1,l2,l3],squaredCost)</code><code class="nohighlight hljs ansi" style="display:block;">BetaML.Nn.NN(BetaML.Nn.AbstractLayer[BetaML.Nn.DenseLayer([-0.2924033262554372 -0.2539644635579579 … 0.2708283712107636 -0.3397651148559973; 0.3200055913343534 0.34843279884887385 … 0.2850466882977612 0.40310296345588464; … ; -0.14567183246806553 -0.3460098530593056 … -0.10977686864464409 -0.11023709217691652; 0.3731613045812631 -0.10210748942685816 … -0.338215061941476 0.1408952609322956], [0.09592800790203365, 0.15860323464133336, 0.3551464313117509, -0.04620145965674505, -0.36129502082159637, 0.31020624847324435, 0.1962275378594674, -0.19652653926097655, 0.16684584295204136, 0.23639369054502485, 0.1755727972983427, -0.3673393833437488, -0.40059902215997695, 0.1933896650049401, 0.4044925415855833, -0.28878525234767244, -0.20173882418331515, -0.17491544700243872, -0.2198048834412415, -0.4046519243293346], BetaML.Utils.relu, nothing), BetaML.Nn.DenseLayer([-0.2927494810685075 0.22219075184423648 … 0.2673123538870123 0.20410674617461927; -0.2320135040440592 0.3292687339809626 … 0.11745976371776506 0.2749281212619707; … ; 0.29099415187214067 0.12183960430470392 … -0.09269557835592285 -0.01060543642578815; -0.09588210686525739 -0.17734571306244018 … -0.04581198248266788 -0.17839653524534227], [-0.3280003996511005, 0.04546914264795188, -0.2556527937208482, 0.22780164042762602, -0.36702175649579394, -0.08101120595044425, -0.26900852855358964, -0.003716502775307995, -0.06992958602385585, 0.0866901916940902, 0.3087058654826345, 0.28944003386379064, -0.09506133807591433, 0.30825057340536494, -0.34752756073212243, 0.3175303340526932, 0.2302359016855347, -0.05159916226591843, 0.1427898320160052, 0.25687884208929207], BetaML.Utils.relu, nothing), BetaML.Nn.DenseLayer([-0.19336934131814693 -0.0012534394550661743 … -0.40957078962790305 -0.28122106464306357], [-0.04427898450034906], BetaML.Utils.relu, nothing)], BetaML.Utils.squaredCost, nothing, false, &quot;Neural Network&quot;)</code></pre><p>Training...</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; trainingLogs = train!(mynn,scale(xtrain),ytrain,batchSize=6,epochs=600)</code><code class="nohighlight hljs ansi" style="display:block;">***
*** Training Neural Network for 600 epochs with algorithm BetaML.Nn.ADAM.
Training.. 	 avg ϵ on (Epoch 1 Batch 59): 	 18200.666666666668
Training the Neural Network...  6%|█▍                   |  ETA: 0:00:14Training.. 	 avg ϵ on (Epoch 60 Batch 59): 	 2311.447494343469
Training the Neural Network... 13%|██▊                  |  ETA: 0:00:13Training.. 	 avg ϵ on (Epoch 120 Batch 59): 	 1096.3721539775713
Training the Neural Network... 20%|████▎                |  ETA: 0:00:12Training the Neural Network... 27%|█████▋               |  ETA: 0:00:11Training.. 	 avg ϵ on (Epoch 180 Batch 59): 	 1183.9727203520767
Training the Neural Network... 34%|███████▏             |  ETA: 0:00:10Training.. 	 avg ϵ on (Epoch 240 Batch 59): 	 1669.988564564045
Training the Neural Network... 40%|████████▌            |  ETA: 0:00:09Training the Neural Network... 47%|█████████▉           |  ETA: 0:00:08Training.. 	 avg ϵ on (Epoch 300 Batch 59): 	 2206.9635313792533
Training the Neural Network... 54%|███████████▎         |  ETA: 0:00:07Training.. 	 avg ϵ on (Epoch 360 Batch 59): 	 3609.3644618634694
Training the Neural Network... 60%|████████████▋        |  ETA: 0:00:06Training the Neural Network... 67%|██████████████▏      |  ETA: 0:00:05Training.. 	 avg ϵ on (Epoch 420 Batch 59): 	 727.1238508027868
Training the Neural Network... 74%|███████████████▌     |  ETA: 0:00:04Training.. 	 avg ϵ on (Epoch 480 Batch 59): 	 1004.9777649170204
Training the Neural Network... 80%|████████████████▉    |  ETA: 0:00:03Training the Neural Network... 87%|██████████████████▎  |  ETA: 0:00:02Training.. 	 avg ϵ on (Epoch 540 Batch 59): 	 2508.140012788528
Training the Neural Network... 94%|███████████████████▋ |  ETA: 0:00:01Training.. 	 avg ϵ on (Epoch 600 Batch 59): 	 187.24782669718715
Training the Neural Network...100%|█████████████████████| Time: 0:00:15
Training of 600 epoch completed. Final epoch error: 859.4899241611331.
(epochs = 600, ϵ_epochs = [14208.966671695342, 14208.077745778835, 14138.68312357319, 13629.947479634317, 12450.855876272957, 10171.303498611165, 7196.525130476429, 4705.788206260294, 3369.2826728035957, 2762.9103991956863  …  867.210357410691, 867.602434922532, 865.0629967095606, 865.099877426571, 864.1631343819134, 862.7428138777036, 862.2436356784573, 861.2465082652734, 861.0490572001324, 859.4899241611331], θ_epochs = Any[])</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ŷtrain   = predict(mynn, scale(xtrain)) |&gt; makeColVector</code><code class="nohighlight hljs ansi" style="display:block;">354-element Vector{Float64}:
 240.31120588557175
 190.8988303554195
 107.79245408713788
  69.17992668465486
 289.89894654193563
 113.25449969898348
  78.68896482395154
 229.72104446669428
 160.4160000940321
 109.35573080553155
   ⋮
  75.55409623341583
 122.67332996806492
 146.20831599248353
 136.7843660733973
 167.08493322603584
 147.47219464099368
 341.5978150300695
  56.73030004138741
 126.00509240093193</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ŷval     = predict(mynn, scale(xval))  |&gt; makeColVector</code><code class="nohighlight hljs ansi" style="display:block;">88-element Vector{Float64}:
 273.1093656906662
 122.26535101498642
  93.34352251036084
 136.61685032408332
 223.2622753106145
 183.0946247402568
  27.29629085316508
  71.97966164716122
 100.95504890276
 109.12048536952426
   ⋮
 137.96381446479472
 147.6423484030756
  87.1518598867429
 178.85337383593952
 148.63901112215885
 259.34129569773046
 179.8201520945061
 192.89044573992857
 108.02964996556128</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; trainRME = meanRelError(ŷtrain,ytrain,normRec=false)</code><code class="nohighlight hljs ansi" style="display:block;">0.21010628849128826</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; testRME  = meanRelError(ŷval,yval,normRec=false)</code><code class="nohighlight hljs ansi" style="display:block;">0.2640386331207359</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; plot(trainingLogs.ϵ_epochs[10:end])</code><code class="nohighlight hljs ansi" style="display:block;">Plot{Plots.GRBackend() n=1}</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; scatter(yval,ŷval,xlabel=&quot;obs&quot;,ylabel=&quot;est&quot;,legend=nothing)</code><code class="nohighlight hljs ansi" style="display:block;">Plot{Plots.GRBackend() n=1}</code></pre><h2 id="Convolutional-neural-networks"><a class="docs-heading-anchor" href="#Convolutional-neural-networks">Convolutional neural networks</a><a id="Convolutional-neural-networks-1"></a><a class="docs-heading-anchor-permalink" href="#Convolutional-neural-networks" title="Permalink"></a></h2><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; println(&quot;Working with a convolutional neural network...&quot;)</code><code class="nohighlight hljs ansi" style="display:block;">Working with a convolutional neural network...</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; using Flux, MLDatasets, Statistics, Plots</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; x_train, y_train = MLDatasets.MNIST.traindata()</code><code class="nohighlight hljs ansi" style="display:block;">([0.0N0f8 0.0N0f8 … 0.0N0f8 0.0N0f8; 0.0N0f8 0.0N0f8 … 0.0N0f8 0.0N0f8; … ; 0.0N0f8 0.0N0f8 … 0.0N0f8 0.0N0f8; 0.0N0f8 0.0N0f8 … 0.0N0f8 0.0N0f8;;; 0.0N0f8 0.0N0f8 … 0.0N0f8 0.0N0f8; 0.0N0f8 0.0N0f8 … 0.0N0f8 0.0N0f8; … ; 0.0N0f8 0.0N0f8 … 0.0N0f8 0.0N0f8; 0.0N0f8 0.0N0f8 … 0.0N0f8 0.0N0f8;;; 0.0N0f8 0.0N0f8 … 0.0N0f8 0.0N0f8; 0.0N0f8 0.0N0f8 … 0.0N0f8 0.0N0f8; … ; 0.0N0f8 0.0N0f8 … 0.0N0f8 0.0N0f8; 0.0N0f8 0.0N0f8 … 0.0N0f8 0.0N0f8;;; … ;;; 0.0N0f8 0.0N0f8 … 0.0N0f8 0.0N0f8; 0.0N0f8 0.0N0f8 … 0.0N0f8 0.0N0f8; … ; 0.0N0f8 0.0N0f8 … 0.0N0f8 0.0N0f8; 0.0N0f8 0.0N0f8 … 0.0N0f8 0.0N0f8;;; 0.0N0f8 0.0N0f8 … 0.0N0f8 0.0N0f8; 0.0N0f8 0.0N0f8 … 0.0N0f8 0.0N0f8; … ; 0.0N0f8 0.0N0f8 … 0.0N0f8 0.0N0f8; 0.0N0f8 0.0N0f8 … 0.0N0f8 0.0N0f8;;; 0.0N0f8 0.0N0f8 … 0.0N0f8 0.0N0f8; 0.0N0f8 0.0N0f8 … 0.0N0f8 0.0N0f8; … ; 0.0N0f8 0.0N0f8 … 0.0N0f8 0.0N0f8; 0.0N0f8 0.0N0f8 … 0.0N0f8 0.0N0f8], [5, 0, 4, 1, 9, 2, 1, 3, 1, 4  …  9, 2, 9, 5, 1, 8, 3, 5, 6, 8])</code></pre><p>x<em>train          = permutedims(x</em>train,(2,1,3)) # For correct img axis x<em>train          = convert(Array{Float32,3},x</em>train) x<em>train          = reshape(x</em>train,(28,28,1,60000)) y<em>train          = Flux.onehotbatch(y</em>train, 0:9) train<em>data       = Flux.Data.DataLoader((x</em>train, y<em>train), batchsize=128) x</em>test, y<em>test   = MLDatasets.MNIST.testdata() x</em>test           = permutedims(x<em>test,(2,1,3)) # For correct img axis x</em>test           = convert(Array{Float32,3},x<em>test) x</em>test           = reshape(x<em>test,(28,28,1,10000)) y</em>test           = Flux.onehotbatch(y_test, 0:9)</p><p>myaccuracy(ŷ, y) = (mean(Flux.onecold(ŷ) .== Flux.onecold(y))) myloss(x, y)     = Flux.crossentropy(model(x), y)</p><p>model = Chain(     # 28x28 =&gt; 14x14     Conv((5, 5), 1=&gt;8, pad=2, stride=2, relu),     # 14x14 =&gt; 7x7     Conv((3, 3), 8=&gt;16, pad=1, stride=2, relu),     # 7x7 =&gt; 4x4     Conv((3, 3), 16=&gt;32, pad=1, stride=2, relu),     # 4x4 =&gt; 2x2     Conv((3, 3), 32=&gt;32, pad=1, stride=2, relu),     # Average pooling on each width x height feature map     GlobalMeanPool(),     Flux.flatten,     Dense(32, 10),     Flux.softmax )</p><p>opt = Flux.ADAM() ps  = Flux.params(model) number<em>epochs = 4 println(&quot;mydebug a&quot;) Flux.@epochs number</em>epochs Flux.train!(myloss, ps, train_data, opt) println(&quot;mydebug b&quot;)</p><p>ŷtrain =   model(x<em>train) println(&quot;mydebug c&quot;) ŷtest  =   model(x</em>test) println(&quot;mydebug d&quot;) myaccuracy(ŷtrain, y<em>train) println(&quot;mydebug e&quot;) myaccuracy(ŷtest, y</em>test) println(&quot;mydebug f&quot;) plot(Gray.(x_train[:,:,1,1])) println(&quot;mydebug g&quot;)</p><h1 id="-Recursive-neural-networks"><a class="docs-heading-anchor" href="###-Recursive-neural-networks">## Recursive neural networks</a><a id="-Recursive-neural-networks-1"></a><a class="docs-heading-anchor-permalink" href="###-Recursive-neural-networks" title="Permalink"></a></h1><h1 id="Generating-simulated-data"><a class="docs-heading-anchor" href="#Generating-simulated-data">Generating simulated data</a><a id="Generating-simulated-data-1"></a><a class="docs-heading-anchor-permalink" href="#Generating-simulated-data" title="Permalink"></a></h1><p>nSeeds    = 5 seqLength = 10 nTrains   = 1000 nTest     = 100 println(&quot;mydebug h&quot;) nTot = nTrains+nTest makeSeeds(nSeeds) = 2 .* (rand(nSeeds) .- 0.5) # [-1,+1] function makeSequence(seeds,seqLength)   seq = Vector{Float32}(undef,seqLength+nSeeds) # Flux Works with Float32 for performance reasons   [seq[i] = seeds[i] for i in 1:nSeeds]   for i in nSeeds:(seqLength+nSeeds)     seq[i] = seq[i-1] + seeds[1]<em>0.1</em>seq[i-1] +seeds[2]<em>seeds[3]</em>seq[i-1]<em>0.4+seeds[4]</em>seeds[5]*(seq[i-3]-seq[i-4])     #seq[i] = seq[i-1] + mean(seeds)   end   return seq   return seq[nSeeds+1:end] end</p><p>seq=makeSequence(makeSeeds(nSeeds),seqLength) plot(seq)</p><p>x0   = [makeSeeds(nSeeds) for i in 1:nTot] seqs = makeSequence.(x0,seqLength) seqs<em>vectors = [[[e] for e in seq] for seq in seqs] seqs</em>vectors[1][1] y    = seqs_vectors # y here is the value of the sequence itself m    = Chain(Dense(1,5,σ),LSTM(5, 5), Dense(5, 1)) #σ function myloss(x, y)     Flux.reset!(m)               # Reset the state (not the weigtht!)     #[m(x[i]) for i in 1:nSeeds]  # Ignores the output but updates the hidden states     sum(Flux.mse(m(xi), yi) for (xi, yi) in zip(x[1:end], y[1:end])) end</p><p>ps  = params(m) opt = ADAM() println(&quot;mydebug i&quot;) trainxy = zip(seqs<em>vectors,seqs</em>vectors) println(&quot;mydebug l&quot;) Actual training Flux.train!(myloss, ps, trainxy, opt)</p><p>function predictSequence(m,seeds,seqLength)     seq = Vector{Vector{Float32}}(undef,seqLength+length(seeds))     Flux.reset!(m) # Reset the state (not the weigtht!)     [seq[i] = m([convert(Float32, seeds[i])]) for i in 1:nSeeds]     [seq[i] = m(seq[i-1]) for i in nSeeds+1:nSeeds+seqLength]     [s[1] for s in seq] end</p><p>a = predictSequence(m,x0[1],seqLength)</p><p>i = 2 trueseq = makeSequence(x0[i],seqLength) estseq  = predictSequence(m,x0[i],seqLength)</p><p>plot(trueseq[nSeeds+1:end]) plot!(estseq[nSeeds+1:end])</p><p><a href="https://github.com/sylvaticus/SPMLJ/blob/main/lessonsSources/04_-_NN_-_Neural_Networks/0402_Implementing_neural_network_models.jl">View this file on Github</a>.</p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="0401_NeuralNetworksTheory.html">« 0401 NeuralNetworksTheory</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.13 on <span class="colophon-date" title="Thursday 24 March 2022 11:09">Thursday 24 March 2022</span>. Using Julia version 1.7.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
