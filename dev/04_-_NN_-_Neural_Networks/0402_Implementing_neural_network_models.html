<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>0402 Implementing neural network models · SPMLJ</title><script async src="https://www.googletagmanager.com/gtag/js?id=G-Q39LHCRBB6"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-Q39LHCRBB6', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../index.html">SPMLJ</a></span></div><form class="docs-search" action="../search.html"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../index.html">Index</a></li><li><span class="tocitem">Lessons</span><ul><li><input class="collapse-toggle" id="menuitem-2-1" type="checkbox" checked/><label class="tocitem" for="menuitem-2-1"><span class="docs-label">NN - Neural Networks</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="0401_NeuralNetworksTheory.html">0401 NeuralNetworksTheory</a></li><li class="is-active"><a class="tocitem" href="0402_Implementing_neural_network_models.html">0402 Implementing neural network models</a><ul class="internal"><li><a class="tocitem" href="#Some-stuff-to-set-up-the-environment.."><span>Some stuff to set-up the environment..</span></a></li><li><a class="tocitem" href="#Feed-forward-neural-networks"><span>Feed-forward neural networks</span></a></li><li class="toplevel"><a class="tocitem" href="####-Regression"><span>### Regression</span></a></li><li class="toplevel"><a class="tocitem" href="#Data-Loading-and-processing.."><span>Data Loading and processing..</span></a></li><li class="toplevel"><a class="tocitem" href="#Model-definition..."><span>Model definition...</span></a></li><li class="toplevel"><a class="tocitem" href="#Training..."><span>Training...</span></a></li><li class="toplevel"><a class="tocitem" href="###-Convolutional-neural-networks"><span>## Convolutional neural networks</span></a></li><li class="toplevel"><a class="tocitem" href="###-Recursive-neural-networks"><span>## Recursive neural networks</span></a></li><li class="toplevel"><a class="tocitem" href="#Generating-simulated-data"><span>Generating simulated data</span></a></li></ul></li></ul></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Lessons</a></li><li><a class="is-disabled">NN - Neural Networks</a></li><li class="is-active"><a href="0402_Implementing_neural_network_models.html">0402 Implementing neural network models</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="0402_Implementing_neural_network_models.html">0402 Implementing neural network models</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/sylvaticus/SPMLJ/blob/main/lessonsSources/04_-_NN_-_Neural_Networks/0402_Implementing_neural_network_models.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><pre></pre><h1 id="Neural-network-implementations"><a class="docs-heading-anchor" href="#Neural-network-implementations">0402 - Neural network implementations</a><a id="Neural-network-implementations-1"></a><a class="docs-heading-anchor-permalink" href="#Neural-network-implementations" title="Permalink"></a></h1><h2 id="Some-stuff-to-set-up-the-environment.."><a class="docs-heading-anchor" href="#Some-stuff-to-set-up-the-environment..">Some stuff to set-up the environment..</a><a id="Some-stuff-to-set-up-the-environment..-1"></a><a class="docs-heading-anchor-permalink" href="#Some-stuff-to-set-up-the-environment.." title="Permalink"></a></h2><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; cd(@__DIR__)</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; using Pkg</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; Pkg.activate(&quot;.&quot;)</code><code class="nohighlight hljs ansi" style="display:block;">  Activating project at `~/work/SPMLJ/SPMLJ/buildedDoc/04_-_NN_-_Neural_Networks`</code></pre><p>If using a Julia version different than 1.7 please uncomment and run the following line (the guarantee of reproducibility will however be lost) Pkg.resolve()</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; Pkg.instantiate()</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; using Random</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; Random.seed!(123)</code><code class="nohighlight hljs ansi" style="display:block;">Random.TaskLocalRNG()</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ENV[&quot;DATADEPS_ALWAYS_ACCEPT&quot;] = &quot;true&quot;</code><code class="nohighlight hljs ansi" style="display:block;">&quot;true&quot;</code></pre><p>We will <em>not</em> run cross validation here to find the optimal hypermarameters. The process will not be different than those we saw in the lesson on the Perceptron. Instead we focus on creating neural network models, train them based on data and evaluationg their predictions. For feed-forward neural networks (both for classification and regression) we will use <a href="https://github.com/sylvaticus/BetaML.jl">BetaML</a>, while for Convolutional Neural Networks and Recursive Neural NEtworks we will use the <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a> package.</p><h2 id="Feed-forward-neural-networks"><a class="docs-heading-anchor" href="#Feed-forward-neural-networks">Feed-forward neural networks</a><a id="Feed-forward-neural-networks-1"></a><a class="docs-heading-anchor-permalink" href="#Feed-forward-neural-networks" title="Permalink"></a></h2><h3 id="Binary-classification"><a class="docs-heading-anchor" href="#Binary-classification">Binary classification</a><a id="Binary-classification-1"></a><a class="docs-heading-anchor-permalink" href="#Binary-classification" title="Permalink"></a></h3><p>Data loading...</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; using BetaML, DelimitedFiles</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; data  = readdlm(joinpath(dirname(pathof(BetaML)),&quot;..&quot;,&quot;test&quot;,&quot;data&quot;,&quot;binary2DData.csv&quot;),&#39;\t&#39;)</code><code class="nohighlight hljs ansi" style="display:block;">200×3 Matrix{Float64}:
 -1.0   1.76    0.4
 -1.0   0.979   2.24
 -1.0   1.87   -0.977
 -1.0   0.95   -0.151
 -1.0  -0.103   0.411
 -1.0   0.144   1.45
 -1.0   0.761   0.122
 -1.0   0.444   0.334
 -1.0   1.49   -0.205
 -1.0   0.313  -0.854
  ⋮
  1.0  -0.256   0.977
  1.0   2.04    0.343
  1.0   1.01    0.528
  1.0   3.65    2.16
  1.0   2.57    1.78
  1.0   1.65    0.384
  1.0   1.71    1.24
  1.0   2.86    3.14
  1.0   3.47    2.85</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; nR   = size(data,1)</code><code class="nohighlight hljs ansi" style="display:block;">200</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; idx  = shuffle(1:nR)</code><code class="nohighlight hljs ansi" style="display:block;">200-element Vector{Int64}:
 123
 131
  74
  23
  19
  78
  43
 130
  83
 186
   ⋮
 137
 175
 182
  37
  71
  89
 142
  82
 170</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; data = data[idx,:]</code><code class="nohighlight hljs ansi" style="display:block;">200×3 Matrix{Float64}:
  1.0   1.69     0.324
  1.0   0.811    1.49
 -1.0  -0.913    1.12
 -1.0  -0.5097  -0.4381
 -1.0   1.23     1.2
 -1.0  -0.0985  -0.6635
 -1.0   1.49     1.9
  1.0   0.417    2.61
 -1.0  -1.23     0.844
  1.0   2.28     1.01
  ⋮
  1.0   3.96     2.39
  1.0   2.58     2.35
  1.0   2.93     2.34
 -1.0   1.14    -1.23
 -1.0  -1.49     0.439
 -1.0  -0.8034  -0.6895
  1.0   1.31     3.54
 -1.0   0.949    0.0876
  1.0   1.32     3.66</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; X    = copy(data[:,[2,3]])</code><code class="nohighlight hljs ansi" style="display:block;">200×2 Matrix{Float64}:
  1.69     0.324
  0.811    1.49
 -0.913    1.12
 -0.5097  -0.4381
  1.23     1.2
 -0.0985  -0.6635
  1.49     1.9
  0.417    2.61
 -1.23     0.844
  2.28     1.01
  ⋮
  3.96     2.39
  2.58     2.35
  2.93     2.34
  1.14    -1.23
 -1.49     0.439
 -0.8034  -0.6895
  1.31     3.54
  0.949    0.0876
  1.32     3.66</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; y    = max.(0,convert(Array{Int64,1},copy(data[:,1]))) # Converting labels from {-1,1} to {0,1}</code><code class="nohighlight hljs ansi" style="display:block;">200-element Vector{Int64}:
 1
 1
 0
 0
 0
 0
 0
 1
 0
 1
 ⋮
 1
 1
 1
 0
 0
 0
 1
 0
 1</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ((xtrain,xtest),(ytrain,ytest)) = partition([X,y],[0.7,0.3])</code><code class="nohighlight hljs ansi" style="display:block;">2-element Vector{Vector}:
 AbstractMatrix{Float64}[[1.23 1.2; 3.49 -0.07; … ; 2.69 2.69; 0.979 2.24], [-0.8708 -0.5788; 0.127 0.402; … ; -0.312 0.0562; -0.674 0.0318]]
 AbstractVector{Int64}[[0, 1, 0, 0, 0, 0, 1, 1, 1, 1  …  1, 1, 1, 0, 0, 0, 0, 1, 1, 0], [0, 0, 0, 1, 0, 1, 1, 1, 1, 1  …  0, 1, 1, 0, 0, 0, 1, 1, 0, 0]]</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; println(&quot;mydebug 0&quot;)</code><code class="nohighlight hljs ansi" style="display:block;">mydebug 0</code></pre><h4 id="Using-defaults-hidding-complexity"><a class="docs-heading-anchor" href="#Using-defaults-hidding-complexity">Using defaults - hidding complexity</a><a id="Using-defaults-hidding-complexity-1"></a><a class="docs-heading-anchor-permalink" href="#Using-defaults-hidding-complexity" title="Permalink"></a></h4><p>Model definition...</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; l1   = DenseLayer(2,5,f=tanh)</code><code class="nohighlight hljs ansi" style="display:block;">BetaML.Nn.DenseLayer([0.7114333605012103 -0.46840598025417524; -0.5672928157707955 0.4615531187467369; … ; 0.3460984398145658 -0.042222920388582996; -0.3373448835886066 0.13190727737286456], [-0.3003496305664325, 0.4564154347727757, -0.680410701712616, -0.4411408342040226, -0.727094508745879], tanh, nothing)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; l2   = DenseLayer(5,5,f=relu)</code><code class="nohighlight hljs ansi" style="display:block;">BetaML.Nn.DenseLayer([-0.24966234202395343 -0.1450088367408291 … -0.10738350533488994 0.7247141142027148; -0.6656904389257094 -0.25622478582029584 … 0.08056492594196829 -0.26225834425970906; … ; -0.05944120984655077 0.5919235655676248 … -0.6555792177003029 -0.48387955484315165; -0.34028100059184424 0.3808788075736441 … 0.1397207349693791 0.4839512714899258], [-0.5050945187284572, -0.4236829371301429, 0.4386534893642128, -0.014288651256534779, -0.22467496450689328], BetaML.Utils.relu, nothing)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; l3   = DenseLayer(5,1,f=sigmoid)</code><code class="nohighlight hljs ansi" style="display:block;">BetaML.Nn.DenseLayer([-0.6941847710439693 0.6253114897153567 … 0.9597552104353553 -0.3356524103426095], [-0.31962023986613586], BetaML.Utils.sigmoid, nothing)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; mynn = buildNetwork([l1,l2,l3],squaredCost)</code><code class="nohighlight hljs ansi" style="display:block;">BetaML.Nn.NN(BetaML.Nn.AbstractLayer[BetaML.Nn.DenseLayer([0.7114333605012103 -0.46840598025417524; -0.5672928157707955 0.4615531187467369; … ; 0.3460984398145658 -0.042222920388582996; -0.3373448835886066 0.13190727737286456], [-0.3003496305664325, 0.4564154347727757, -0.680410701712616, -0.4411408342040226, -0.727094508745879], tanh, nothing), BetaML.Nn.DenseLayer([-0.24966234202395343 -0.1450088367408291 … -0.10738350533488994 0.7247141142027148; -0.6656904389257094 -0.25622478582029584 … 0.08056492594196829 -0.26225834425970906; … ; -0.05944120984655077 0.5919235655676248 … -0.6555792177003029 -0.48387955484315165; -0.34028100059184424 0.3808788075736441 … 0.1397207349693791 0.4839512714899258], [-0.5050945187284572, -0.4236829371301429, 0.4386534893642128, -0.014288651256534779, -0.22467496450689328], BetaML.Utils.relu, nothing), BetaML.Nn.DenseLayer([-0.6941847710439693 0.6253114897153567 … 0.9597552104353553 -0.3356524103426095], [-0.31962023986613586], BetaML.Utils.sigmoid, nothing)], BetaML.Utils.squaredCost, nothing, false, &quot;Neural Network&quot;)</code></pre><p>Training...</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; train!(mynn,xtrain,ytrain)</code><code class="nohighlight hljs ansi" style="display:block;">***
*** Training Neural Network for 100 epochs with algorithm BetaML.Nn.ADAM.
Training.. 	 avg ϵ on (Epoch 1 Batch 4): 	 0.15699130044905962
Training.. 	 avg ϵ on (Epoch 10 Batch 4): 	 0.1567364224941614
Training.. 	 avg ϵ on (Epoch 20 Batch 4): 	 0.16405010084739874
Training.. 	 avg ϵ on (Epoch 30 Batch 4): 	 0.12619916483702404
Training.. 	 avg ϵ on (Epoch 40 Batch 4): 	 0.1112696456471148
Training.. 	 avg ϵ on (Epoch 50 Batch 4): 	 0.107809851602766
Training.. 	 avg ϵ on (Epoch 60 Batch 4): 	 0.09092457830472603
Training.. 	 avg ϵ on (Epoch 70 Batch 4): 	 0.06796781644461654
Training.. 	 avg ϵ on (Epoch 80 Batch 4): 	 0.06750170957599748
Training.. 	 avg ϵ on (Epoch 90 Batch 4): 	 0.05365239013100279
Training.. 	 avg ϵ on (Epoch 100 Batch 4): 	 0.03537649303167354
Training of 100 epoch completed. Final epoch error: 0.049010516945609615.
(epochs = 100, ϵ_epochs = [0.16917361724292546, 0.1673212323433058, 0.16559724017832195, 0.16391033138766398, 0.1622650152221565, 0.1606237255175449, 0.15904696343918534, 0.1574994634681001, 0.1560231922458023, 0.15463191100381887  …  0.05271910822952187, 0.05223613716710994, 0.051769305249917484, 0.05132435927692999, 0.05089006848008419, 0.05047956943681352, 0.05010007704280327, 0.04973158120785568, 0.0493631242416035, 0.049010516945609615], θ_epochs = Any[])</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ŷtrain         = predict(mynn, xtrain) |&gt; makeColVector .|&gt; round .|&gt; Int</code><code class="nohighlight hljs ansi" style="display:block;">140-element Vector{Int64}:
 1
 1
 0
 0
 0
 0
 1
 1
 1
 1
 ⋮
 1
 1
 0
 0
 0
 0
 1
 1
 1</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ŷtest          = predict(mynn, xtest)  |&gt; makeColVector .|&gt; round .|&gt; Int</code><code class="nohighlight hljs ansi" style="display:block;">60-element Vector{Int64}:
 0
 0
 0
 1
 0
 1
 1
 1
 1
 0
 ⋮
 1
 1
 0
 0
 1
 1
 1
 0
 0</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; trainAccuracy  = accuracy(ŷtrain,ytrain)</code><code class="nohighlight hljs ansi" style="display:block;">0.9214285714285714</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; testAccuracy   = accuracy(ŷtest,ytest)</code><code class="nohighlight hljs ansi" style="display:block;">0.9333333333333333</code></pre><h4 id="Specifying-all-options"><a class="docs-heading-anchor" href="#Specifying-all-options">Specifying all options</a><a id="Specifying-all-options-1"></a><a class="docs-heading-anchor-permalink" href="#Specifying-all-options" title="Permalink"></a></h4><p>Model definition...</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; l1   = DenseLayer(2,5,f=tanh, df= dtanh,rng=copy(FIXEDRNG))</code><code class="nohighlight hljs ansi" style="display:block;">BetaML.Nn.DenseLayer([-0.5906259287724187 -0.030940667488043583; -0.24536094247801754 0.48878436852866713; … ; -0.8466984663657858 0.027858152254290336; -0.11481842953412624 -0.009180955255778445], [0.37280922460295074, 0.35842073435858446, 0.696721837875378, 0.8225343299333828, 0.44293565195483586], tanh, BetaML.Utils.dtanh)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; l2   = DenseLayer(5,5,f=relu,df=drelu,rng=copy(FIXEDRNG))</code><code class="nohighlight hljs ansi" style="display:block;">BetaML.Nn.DenseLayer([-0.49415310523844497 -0.025886819681528617 … 0.09224620317870791 -0.2298725180847525; -0.20528369264408397 0.40894634274263597 … 0.01065168104625569 0.04243100148415224; … ; -0.7083987613359595 0.023307802404264888 … -0.3619336136169306 -0.4825559053138102; -0.09606399030062296 -0.0076813382679077336 … -0.3824319138128976 0.21378287379211514], [-0.11178452751598777, -0.2115472973462721, 0.12029035453379433, -0.06939594013486328, -0.5177091839997635], BetaML.Utils.relu, BetaML.Utils.drelu)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; l3   = DenseLayer(5,1,f=sigmoid,df=dsigmoid,rng=copy(FIXEDRNG))</code><code class="nohighlight hljs ansi" style="display:block;">BetaML.Nn.DenseLayer([-0.6379489156883928 -0.2650201076194998 … -0.9145388683760447 -0.12401807820151456], [-0.033419740504278206], BetaML.Utils.sigmoid, BetaML.Utils.dsigmoid)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; mynn = buildNetwork([l1,l2,l3],squaredCost,dcf=dSquaredCost,name=&quot;A classification task&quot;) # or crossEntropy / dCrossEntropy</code><code class="nohighlight hljs ansi" style="display:block;">BetaML.Nn.NN(BetaML.Nn.AbstractLayer[BetaML.Nn.DenseLayer([-0.5906259287724187 -0.030940667488043583; -0.24536094247801754 0.48878436852866713; … ; -0.8466984663657858 0.027858152254290336; -0.11481842953412624 -0.009180955255778445], [0.37280922460295074, 0.35842073435858446, 0.696721837875378, 0.8225343299333828, 0.44293565195483586], tanh, BetaML.Utils.dtanh), BetaML.Nn.DenseLayer([-0.49415310523844497 -0.025886819681528617 … 0.09224620317870791 -0.2298725180847525; -0.20528369264408397 0.40894634274263597 … 0.01065168104625569 0.04243100148415224; … ; -0.7083987613359595 0.023307802404264888 … -0.3619336136169306 -0.4825559053138102; -0.09606399030062296 -0.0076813382679077336 … -0.3824319138128976 0.21378287379211514], [-0.11178452751598777, -0.2115472973462721, 0.12029035453379433, -0.06939594013486328, -0.5177091839997635], BetaML.Utils.relu, BetaML.Utils.drelu), BetaML.Nn.DenseLayer([-0.6379489156883928 -0.2650201076194998 … -0.9145388683760447 -0.12401807820151456], [-0.033419740504278206], BetaML.Utils.sigmoid, BetaML.Utils.dsigmoid)], BetaML.Utils.squaredCost, BetaML.Utils.dSquaredCost, false, &quot;A classification task&quot;)</code></pre><p>Training...</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; function myOwnTrainingInfo(nn,x,y;n,nBatches,epochs,verbosity,nEpoch,nBatch)
           if verbosity == NONE
               return false # doesn&#39;t stop the training
           end
           nMsgDict = Dict(LOW =&gt; 0, STD =&gt; 10,HIGH =&gt; 100, FULL =&gt; n)
           nMsgs = nMsgDict[verbosity]
           batchSize = size(x,1)
           if verbosity == FULL || ( nBatch == nBatches &amp;&amp; ( nEpoch == 1  || nEpoch % ceil(epochs/nMsgs) == 0))
       
              ϵ = loss(nn,x,y)
              println(&quot;Training.. \t avg ϵ on (Epoch $nEpoch Batch $nBatch): \t $(ϵ)&quot;)
           end
           return false
        end</code><code class="nohighlight hljs ansi" style="display:block;">myOwnTrainingInfo (generic function with 1 method)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; train!(mynn,xtrain,ytrain,epochs=300,batchSize=6,sequential=false,verbosity=STD,cb=myOwnTrainingInfo,optAlg=ADAM(η=t -&gt; 0.001, λ=1.0, β₁=0.9, β₂=0.999, ϵ=1e-8),rng=copy(FIXEDRNG))</code><code class="nohighlight hljs ansi" style="display:block;">***
*** Training A classification task for 300 epochs with algorithm BetaML.Nn.ADAM.
Training.. 	 avg ϵ on (Epoch 1 Batch 23): 	 0.1840511466243301
Training.. 	 avg ϵ on (Epoch 30 Batch 23): 	 0.09731559879877578
Training.. 	 avg ϵ on (Epoch 60 Batch 23): 	 0.02742948436189047
Training.. 	 avg ϵ on (Epoch 90 Batch 23): 	 0.011200621372772343
Training.. 	 avg ϵ on (Epoch 120 Batch 23): 	 0.024872170024018602
Training.. 	 avg ϵ on (Epoch 150 Batch 23): 	 0.0010080454466619428
Training.. 	 avg ϵ on (Epoch 180 Batch 23): 	 0.007429493035091291
Training.. 	 avg ϵ on (Epoch 210 Batch 23): 	 0.0050384267456215015
Training the Neural Network... 73%|███████████████▍     |  ETA: 0:00:00Training.. 	 avg ϵ on (Epoch 240 Batch 23): 	 0.027626025065464535
Training.. 	 avg ϵ on (Epoch 270 Batch 23): 	 0.06717605514987254
Training.. 	 avg ϵ on (Epoch 300 Batch 23): 	 0.00364538132926154
Training the Neural Network...100%|█████████████████████| Time: 0:00:01
Training of 300 epoch completed. Final epoch error: 0.030688406769744627.
(epochs = 300, ϵ_epochs = [0.20942586889145268, 0.1899818931956108, 0.17228518378814875, 0.1568057717892211, 0.14442720165443992, 0.13658860446242982, 0.13029632237888944, 0.12529286702177192, 0.12156299468506765, 0.11868528241639893  …  0.030712966557551197, 0.030701091408392583, 0.0307019312137928, 0.03069717822681469, 0.030693948444971676, 0.030694475551419124, 0.030694382730006697, 0.030691489834036575, 0.030689035284208525, 0.030688406769744627], θ_epochs = Any[])</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ŷtrain         = predict(mynn, xtrain) |&gt; makeColVector .|&gt; round .|&gt; Int</code><code class="nohighlight hljs ansi" style="display:block;">140-element Vector{Int64}:
 1
 1
 0
 0
 0
 0
 1
 1
 1
 1
 ⋮
 1
 1
 0
 0
 0
 0
 1
 1
 1</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ŷtest          = predict(mynn, xtest)  |&gt; makeColVector .|&gt; round .|&gt; Int</code><code class="nohighlight hljs ansi" style="display:block;">60-element Vector{Int64}:
 0
 0
 0
 1
 0
 1
 1
 1
 1
 0
 ⋮
 1
 1
 0
 0
 1
 1
 1
 0
 0</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; trainAccuracy  = accuracy(ŷtrain,ytrain)</code><code class="nohighlight hljs ansi" style="display:block;">0.9214285714285714</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; testAccuracy   = accuracy(ŷtest,ytest)</code><code class="nohighlight hljs ansi" style="display:block;">0.95</code></pre><h3 id="Multinomial-classification"><a class="docs-heading-anchor" href="#Multinomial-classification">Multinomial classification</a><a id="Multinomial-classification-1"></a><a class="docs-heading-anchor-permalink" href="#Multinomial-classification" title="Permalink"></a></h3><p>We want to determine the plant specie given some bothanic measures of the flower</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; iris     = readdlm(joinpath(dirname(Base.find_package(&quot;BetaML&quot;)),&quot;..&quot;,&quot;test&quot;,&quot;data&quot;,&quot;iris.csv&quot;),&#39;,&#39;,skipstart=1)</code><code class="nohighlight hljs ansi" style="display:block;">150×5 Matrix{Any}:
 5.1  3.5  1.4  0.2  &quot;setosa&quot;
 4.9  3.0  1.4  0.2  &quot;setosa&quot;
 4.7  3.2  1.3  0.2  &quot;setosa&quot;
 4.6  3.1  1.5  0.2  &quot;setosa&quot;
 5.0  3.6  1.4  0.2  &quot;setosa&quot;
 5.4  3.9  1.7  0.4  &quot;setosa&quot;
 4.6  3.4  1.4  0.3  &quot;setosa&quot;
 5.0  3.4  1.5  0.2  &quot;setosa&quot;
 4.4  2.9  1.4  0.2  &quot;setosa&quot;
 4.9  3.1  1.5  0.1  &quot;setosa&quot;
 ⋮
 6.9  3.1  5.1  2.3  &quot;virginica&quot;
 5.8  2.7  5.1  1.9  &quot;virginica&quot;
 6.8  3.2  5.9  2.3  &quot;virginica&quot;
 6.7  3.3  5.7  2.5  &quot;virginica&quot;
 6.7  3.0  5.2  2.3  &quot;virginica&quot;
 6.3  2.5  5.0  1.9  &quot;virginica&quot;
 6.5  3.0  5.2  2.0  &quot;virginica&quot;
 6.2  3.4  5.4  2.3  &quot;virginica&quot;
 5.9  3.0  5.1  1.8  &quot;virginica&quot;</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; iris     = iris[shuffle(axes(iris, 1)), :] # Shuffle the records, as they aren&#39;t by default</code><code class="nohighlight hljs ansi" style="display:block;">150×5 Matrix{Any}:
 7.6  3.0  6.6  2.1  &quot;virginica&quot;
 6.5  3.0  5.2  2.0  &quot;virginica&quot;
 6.3  3.3  6.0  2.5  &quot;virginica&quot;
 6.2  3.4  5.4  2.3  &quot;virginica&quot;
 6.3  3.4  5.6  2.4  &quot;virginica&quot;
 5.5  3.5  1.3  0.2  &quot;setosa&quot;
 5.0  2.3  3.3  1.0  &quot;versicolor&quot;
 7.1  3.0  5.9  2.1  &quot;virginica&quot;
 6.3  2.3  4.4  1.3  &quot;versicolor&quot;
 5.2  3.5  1.5  0.2  &quot;setosa&quot;
 ⋮
 5.1  2.5  3.0  1.1  &quot;versicolor&quot;
 7.2  3.0  5.8  1.6  &quot;virginica&quot;
 4.8  3.1  1.6  0.2  &quot;setosa&quot;
 5.1  3.5  1.4  0.3  &quot;setosa&quot;
 4.9  3.6  1.4  0.1  &quot;setosa&quot;
 6.1  2.6  5.6  1.4  &quot;virginica&quot;
 6.3  2.5  4.9  1.5  &quot;versicolor&quot;
 5.0  3.6  1.4  0.2  &quot;setosa&quot;
 7.7  2.6  6.9  2.3  &quot;virginica&quot;</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; x        = convert(Array{Float64,2}, iris[:,1:4])</code><code class="nohighlight hljs ansi" style="display:block;">150×4 Matrix{Float64}:
 7.6  3.0  6.6  2.1
 6.5  3.0  5.2  2.0
 6.3  3.3  6.0  2.5
 6.2  3.4  5.4  2.3
 6.3  3.4  5.6  2.4
 5.5  3.5  1.3  0.2
 5.0  2.3  3.3  1.0
 7.1  3.0  5.9  2.1
 6.3  2.3  4.4  1.3
 5.2  3.5  1.5  0.2
 ⋮
 5.1  2.5  3.0  1.1
 7.2  3.0  5.8  1.6
 4.8  3.1  1.6  0.2
 5.1  3.5  1.4  0.3
 4.9  3.6  1.4  0.1
 6.1  2.6  5.6  1.4
 6.3  2.5  4.9  1.5
 5.0  3.6  1.4  0.2
 7.7  2.6  6.9  2.3</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; y        = map(x-&gt;Dict(&quot;setosa&quot; =&gt; 1, &quot;versicolor&quot; =&gt; 2, &quot;virginica&quot; =&gt;3)[x],iris[:, 5]) # Convert the target column to numbers</code><code class="nohighlight hljs ansi" style="display:block;">150-element Vector{Int64}:
 3
 3
 3
 3
 3
 1
 2
 3
 2
 1
 ⋮
 2
 3
 1
 1
 1
 3
 2
 1
 3</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ((xtrain,xtest),(ytrain,ytest)) = partition([x,y],[0.8,0.2],shuffle=false)</code><code class="nohighlight hljs ansi" style="display:block;">2-element Vector{Vector}:
 AbstractMatrix{Float64}[[7.6 3.0 6.6 2.1; 6.5 3.0 5.2 2.0; … ; 6.3 3.3 4.7 1.6; 6.3 2.7 4.9 1.8], [4.4 2.9 1.4 0.2; 5.5 2.3 4.0 1.3; … ; 5.0 3.6 1.4 0.2; 7.7 2.6 6.9 2.3]]
 AbstractVector{Int64}[[3, 3, 3, 3, 3, 1, 2, 3, 2, 1  …  3, 2, 2, 1, 3, 1, 3, 2, 2, 3], [1, 2, 2, 1, 1, 2, 2, 3, 2, 2  …  3, 2, 3, 1, 1, 1, 3, 2, 1, 3]]</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ytrain_oh = oneHotEncoder(ytrain) # Convert to One-hot representation (e.g. 2 =&gt; [0 1 0], 3 =&gt; [0 0 1])</code><code class="nohighlight hljs ansi" style="display:block;">120×3 Matrix{Int64}:
 0  0  1
 0  0  1
 0  0  1
 0  0  1
 0  0  1
 1  0  0
 0  1  0
 0  0  1
 0  1  0
 1  0  0
 ⋮
 0  1  0
 0  1  0
 1  0  0
 0  0  1
 1  0  0
 0  0  1
 0  1  0
 0  1  0
 0  0  1</code></pre><p>Define the Artificial Neural Network model</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; l1   = DenseLayer(4,10,f=relu) # Activation function is ReLU</code><code class="nohighlight hljs ansi" style="display:block;">BetaML.Nn.DenseLayer([0.08810718257267769 0.4438377967110334 -0.37264137236830386 -0.5675793667600653; 0.28330514434945786 -0.6492760216930226 -0.38373378010496734 -0.2068666764154719; … ; 0.49212947049159106 0.3525703845340231 -0.2738356656608552 0.04824168948786289; 0.3620453496524063 -0.16339999103332714 -0.5223113549978162 0.27679840195925676], [-0.30345566664660867, 0.6221600121053414, 0.2583008737739664, -0.16376191205074286, 0.26464602008093285, 0.05938240548430895, 0.1776504251523865, 0.03896925233156634, 0.45328794043691545, 0.6401364079583236], BetaML.Utils.relu, nothing)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; l2   = DenseLayer(10,3)        # Activation function is identity by default</code><code class="nohighlight hljs ansi" style="display:block;">BetaML.Nn.DenseLayer([-0.16526408831202344 0.09443683140104497 … 0.2817357432654801 -0.5052590524413781; 0.3925951602664908 -0.0028374365951118197 … 0.47488714495141426 -0.1107389715437852; 0.37642387099674224 -0.2156111929531354 … 0.019362140605474698 0.030582101562182662], [-0.1316500788791678, 0.40702832655345755, -0.4739179415699], identity, nothing)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; l3   = VectorFunctionLayer(3,f=softmax) # Add a (parameterless) layer whose activation function (softMax in this case) is defined to all its nodes at once</code><code class="nohighlight hljs ansi" style="display:block;">BetaML.Nn.VectorFunctionLayer{0}(fill(NaN), 3, 3, BetaML.Utils.softmax, nothing, nothing)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; mynn = buildNetwork([l1,l2,l3],crossEntropy,name=&quot;Multinomial logistic regression Model Sepal&quot;) # Build the NN and use the squared cost (aka MSE) as error function (crossEntropy could also be used)</code><code class="nohighlight hljs ansi" style="display:block;">BetaML.Nn.NN(BetaML.Nn.AbstractLayer[BetaML.Nn.DenseLayer([0.08810718257267769 0.4438377967110334 -0.37264137236830386 -0.5675793667600653; 0.28330514434945786 -0.6492760216930226 -0.38373378010496734 -0.2068666764154719; … ; 0.49212947049159106 0.3525703845340231 -0.2738356656608552 0.04824168948786289; 0.3620453496524063 -0.16339999103332714 -0.5223113549978162 0.27679840195925676], [-0.30345566664660867, 0.6221600121053414, 0.2583008737739664, -0.16376191205074286, 0.26464602008093285, 0.05938240548430895, 0.1776504251523865, 0.03896925233156634, 0.45328794043691545, 0.6401364079583236], BetaML.Utils.relu, nothing), BetaML.Nn.DenseLayer([-0.16526408831202344 0.09443683140104497 … 0.2817357432654801 -0.5052590524413781; 0.3925951602664908 -0.0028374365951118197 … 0.47488714495141426 -0.1107389715437852; 0.37642387099674224 -0.2156111929531354 … 0.019362140605474698 0.030582101562182662], [-0.1316500788791678, 0.40702832655345755, -0.4739179415699], identity, nothing), BetaML.Nn.VectorFunctionLayer{0}(fill(NaN), 3, 3, BetaML.Utils.softmax, nothing, nothing)], BetaML.Utils.crossEntropy, nothing, false, &quot;Multinomial logistic regression Model Sepal&quot;)</code></pre><p>Training it (default to ADAM)</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; res = train!(mynn,scale(xtrain),ytrain_oh,batchSize=6) # Use optAlg=SGD() to use Stochastic Gradient Descent instead</code><code class="nohighlight hljs ansi" style="display:block;">***
*** Training Multinomial logistic regression Model Sepal for 100 epochs with algorithm BetaML.Nn.ADAM.
Training.. 	 avg ϵ on (Epoch 1 Batch 20): 	 0.9948127457289552
Training.. 	 avg ϵ on (Epoch 10 Batch 20): 	 0.5837865124896999
Training.. 	 avg ϵ on (Epoch 20 Batch 20): 	 0.36694017936439677
Training.. 	 avg ϵ on (Epoch 30 Batch 20): 	 0.22011985671566248
Training.. 	 avg ϵ on (Epoch 40 Batch 20): 	 0.3181895280853837
Training.. 	 avg ϵ on (Epoch 50 Batch 20): 	 0.13230320748958713
Training.. 	 avg ϵ on (Epoch 60 Batch 20): 	 0.2772086520705127
Training.. 	 avg ϵ on (Epoch 70 Batch 20): 	 0.024897043935468256
Training.. 	 avg ϵ on (Epoch 80 Batch 20): 	 0.03742574620568454
Training.. 	 avg ϵ on (Epoch 90 Batch 20): 	 0.03262732414394961
Training.. 	 avg ϵ on (Epoch 100 Batch 20): 	 0.027611497248841266
Training of 100 epoch completed. Final epoch error: 0.0801651934634465.
(epochs = 100, ϵ_epochs = [1.2671601037241338, 1.158705164288321, 1.0633035006544664, 0.9714764470962363, 0.8981174025629668, 0.8353360876340743, 0.779735982869415, 0.7282557325133753, 0.6848860468161013, 0.6412222884724327  …  0.08936372126355156, 0.08815735892785259, 0.087056015368834, 0.08594278536952424, 0.08489919240480992, 0.08390185721584732, 0.08290798132642288, 0.08195111840306991, 0.08106313154147711, 0.0801651934634465], θ_epochs = Any[])</code></pre><p>Test it</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; ŷtrain        = predict(mynn,scale(xtrain))   # Note the scaling function</code><code class="nohighlight hljs ansi" style="display:block;">120×3 Matrix{Float64}:
 2.32979e-8   0.00515854  0.994841
 7.2735e-6    0.0653419   0.934651
 4.09545e-7   0.00458407  0.995416
 1.76891e-5   0.023216    0.976766
 4.16751e-6   0.0110585   0.988937
 0.994953     0.00504644  1.28891e-7
 0.00912393   0.983288    0.00758834
 2.25892e-7   0.0134471   0.986553
 3.73313e-5   0.905279    0.094684
 0.9953       0.00469946  2.1606e-7
 ⋮
 0.00746492   0.817767    0.174768
 0.000369756  0.808816    0.190814
 0.995381     0.00461871  5.23794e-7
 5.2395e-7    0.00987338  0.990126
 0.991457     0.00854238  9.9192e-7
 5.90119e-5   0.105251    0.89469
 0.0352721    0.940225    0.0245033
 0.011004     0.780962    0.208034
 1.06058e-5   0.209703    0.790287</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ŷtest         = predict(mynn,scale(xtest))</code><code class="nohighlight hljs ansi" style="display:block;">30×3 Matrix{Float64}:
 0.995291     0.00470855   3.59367e-7
 0.000557691  0.948207     0.0512354
 0.0875071    0.896935     0.0155581
 0.996055     0.0039452    1.52129e-7
 0.994537     0.00546294   2.91913e-7
 0.00208604   0.98966      0.00825415
 0.000107087  0.355057     0.644836
 8.97297e-5   0.334001     0.665909
 0.00842967   0.963262     0.028308
 0.0137498    0.940738     0.0455124
 ⋮
 0.0527497    0.943359     0.00389135
 8.14023e-5   0.23899      0.760929
 0.995957     0.00404292   2.58076e-7
 0.994855     0.00514453   1.9656e-7
 0.9974       0.00260001   6.56563e-8
 8.41223e-5   0.419339     0.580577
 6.4054e-5    0.574292     0.425643
 0.996376     0.00362363   1.09204e-7
 1.51542e-10  0.000823812  0.999176</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; trainAccuracy = accuracy(ŷtrain,ytrain)</code><code class="nohighlight hljs ansi" style="display:block;">0.975</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; testAccuracy  = accuracy(ŷtest,ytest,tol=1,ignoreLabels=false)</code><code class="nohighlight hljs ansi" style="display:block;">0.9666666666666667</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; cm = ConfusionMatrix(ŷtest,ytest, labels=[&quot;setosa&quot;, &quot;versicolor&quot;, &quot;virginica&quot;])</code><code class="nohighlight hljs ansi" style="display:block;">BetaML.Utils.ConfusionMatrix{Int64}([1, 2, 3], [&quot;setosa&quot;, &quot;versicolor&quot;, &quot;virginica&quot;], 0.9666666666666667, 0.033333333333333326, [9, 12, 9], [9, 11, 10], [9 0 0; 0 11 1; 0 0 9], [1.0 0.0 0.0; 0.0 0.9166666666666666 0.08333333333333333; 0.0 0.0 1.0], [9, 11, 9], [21, 18, 20], [0, 0, 1], [0, 1, 0], [1.0, 1.0, 0.9], [1.0, 0.9166666666666666, 1.0], [1.0, 1.0, 0.9523809523809523], [1.0, 0.9565217391304348, 0.9473684210526315], (0.9666666666666667, 0.9700000000000001), (0.9722222222222222, 0.9666666666666667), (0.9841269841269842, 0.9857142857142857), (0.9679633867276888, 0.9668192219679634))</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; println(cm)</code><code class="nohighlight hljs ansi" style="display:block;">
-----------------------------------------------------------------

*** CONFUSION MATRIX ***

Scores actual (rows) vs predicted (columns):


Normalised scores actual (rows) vs predicted (columns):


 *** CONFUSION REPORT ***

- Accuracy:               0.9666666666666667
- Misclassification rate: 0.033333333333333326
- Number of classes:      3

  N Class      precision   recall  specificity  f1Score  actualCount  predictedCount
                             TPR       TNR                 support                  

  1 setosa         1.000    1.000        1.000    1.000            9               9
  2 versicolor     1.000    0.917        1.000    0.957           12              11
  3 virginica      0.900    1.000        0.952    0.947            9              10

- Simple   avg.    0.967    0.972        0.984    0.968
- Weigthed avg.    0.970    0.967        0.986    0.967

-----------------------------------------------------------------</code></pre><h1 id="-Regression"><a class="docs-heading-anchor" href="####-Regression">### Regression</a><a id="-Regression-1"></a><a class="docs-heading-anchor-permalink" href="####-Regression" title="Permalink"></a></h1><h1 id="Data-Loading-and-processing.."><a class="docs-heading-anchor" href="#Data-Loading-and-processing..">Data Loading and processing..</a><a id="Data-Loading-and-processing..-1"></a><a class="docs-heading-anchor-permalink" href="#Data-Loading-and-processing.." title="Permalink"></a></h1><p>using Pipe, HTTP, CSV, Plots, DataFrames urlData = &quot;https://www4.stat.ncsu.edu/~boos/var.select/diabetes.tab.txt&quot; data = @pipe HTTP.get(urlData).body |&gt; CSV.File(<em>, delim=&#39;\t&#39;) |&gt; DataFrame sex</em>oh = oneHotEncoder(data.SEX) X = hcat(data.AGE, Matrix(data[:,3:10]),sex_oh) y = data.Y (xtrain,xval),(ytrain,yval) = partition([X,y],[0.8,0.2])</p><h1 id="Model-definition..."><a class="docs-heading-anchor" href="#Model-definition...">Model definition...</a><a id="Model-definition...-1"></a><a class="docs-heading-anchor-permalink" href="#Model-definition..." title="Permalink"></a></h1><p>l1   = DenseLayer(11,20,f=relu) l2   = DenseLayer(20,20,f=relu) l3   = DenseLayer(20,1,f=relu) # y is positive mynn = buildNetwork([l1,l2,l3],squaredCost)</p><h1 id="Training..."><a class="docs-heading-anchor" href="#Training...">Training...</a><a id="Training...-1"></a><a class="docs-heading-anchor-permalink" href="#Training..." title="Permalink"></a></h1><p>trainingLogs = train!(mynn,scale(xtrain),ytrain,batchSize=6,epochs=600)</p><p>ŷtrain   = predict(mynn, scale(xtrain)) |&gt; makeColVector ŷval     = predict(mynn, scale(xval))  |&gt; makeColVector trainRME = meanRelError(ŷtrain,ytrain,normRec=false) testRME  = meanRelError(ŷval,yval,normRec=false) plot(trainingLogs.ϵ_epochs[10:end]) scatter(yval,ŷval,xlabel=&quot;obs&quot;,ylabel=&quot;est&quot;,legend=nothing)</p><h1 id="-Convolutional-neural-networks"><a class="docs-heading-anchor" href="###-Convolutional-neural-networks">## Convolutional neural networks</a><a id="-Convolutional-neural-networks-1"></a><a class="docs-heading-anchor-permalink" href="###-Convolutional-neural-networks" title="Permalink"></a></h1><p>println(&quot;Working with a convolutional neural network...&quot;) using Flux, MLDatasets, Statistics, Plots</p><p>x<em>train, y</em>train = MLDatasets.MNIST.traindata() x<em>train          = permutedims(x</em>train,(2,1,3)) # For correct img axis x<em>train          = convert(Array{Float32,3},x</em>train) x<em>train          = reshape(x</em>train,(28,28,1,60000)) y<em>train          = Flux.onehotbatch(y</em>train, 0:9) train<em>data       = Flux.Data.DataLoader((x</em>train, y<em>train), batchsize=128) x</em>test, y<em>test   = MLDatasets.MNIST.testdata() x</em>test           = permutedims(x<em>test,(2,1,3)) # For correct img axis x</em>test           = convert(Array{Float32,3},x<em>test) x</em>test           = reshape(x<em>test,(28,28,1,10000)) y</em>test           = Flux.onehotbatch(y_test, 0:9)</p><p>myaccuracy(ŷ, y) = (mean(Flux.onecold(ŷ) .== Flux.onecold(y))) myloss(x, y)     = Flux.crossentropy(model(x), y)</p><p>model = Chain(     # 28x28 =&gt; 14x14     Conv((5, 5), 1=&gt;8, pad=2, stride=2, relu),     # 14x14 =&gt; 7x7     Conv((3, 3), 8=&gt;16, pad=1, stride=2, relu),     # 7x7 =&gt; 4x4     Conv((3, 3), 16=&gt;32, pad=1, stride=2, relu),     # 4x4 =&gt; 2x2     Conv((3, 3), 32=&gt;32, pad=1, stride=2, relu),     # Average pooling on each width x height feature map     GlobalMeanPool(),     Flux.flatten,     Dense(32, 10),     Flux.softmax )</p><p>opt = Flux.ADAM() ps  = Flux.params(model) number<em>epochs = 4 println(&quot;mydebug a&quot;) Flux.@epochs number</em>epochs Flux.train!(myloss, ps, train_data, opt) println(&quot;mydebug b&quot;)</p><p>ŷtrain =   model(x<em>train) println(&quot;mydebug c&quot;) ŷtest  =   model(x</em>test) println(&quot;mydebug d&quot;) myaccuracy(ŷtrain, y<em>train) println(&quot;mydebug e&quot;) myaccuracy(ŷtest, y</em>test) println(&quot;mydebug f&quot;) plot(Gray.(x_train[:,:,1,1])) println(&quot;mydebug g&quot;)</p><h1 id="-Recursive-neural-networks"><a class="docs-heading-anchor" href="###-Recursive-neural-networks">## Recursive neural networks</a><a id="-Recursive-neural-networks-1"></a><a class="docs-heading-anchor-permalink" href="###-Recursive-neural-networks" title="Permalink"></a></h1><h1 id="Generating-simulated-data"><a class="docs-heading-anchor" href="#Generating-simulated-data">Generating simulated data</a><a id="Generating-simulated-data-1"></a><a class="docs-heading-anchor-permalink" href="#Generating-simulated-data" title="Permalink"></a></h1><p>nSeeds    = 5 seqLength = 10 nTrains   = 1000 nTest     = 100 println(&quot;mydebug h&quot;) nTot = nTrains+nTest makeSeeds(nSeeds) = 2 .* (rand(nSeeds) .- 0.5) # [-1,+1] function makeSequence(seeds,seqLength)   seq = Vector{Float32}(undef,seqLength+nSeeds) # Flux Works with Float32 for performance reasons   [seq[i] = seeds[i] for i in 1:nSeeds]   for i in nSeeds:(seqLength+nSeeds)     seq[i] = seq[i-1] + seeds[1]<em>0.1</em>seq[i-1] +seeds[2]<em>seeds[3]</em>seq[i-1]<em>0.4+seeds[4]</em>seeds[5]*(seq[i-3]-seq[i-4])     #seq[i] = seq[i-1] + mean(seeds)   end   return seq   return seq[nSeeds+1:end] end</p><p>seq=makeSequence(makeSeeds(nSeeds),seqLength) plot(seq)</p><p>x0   = [makeSeeds(nSeeds) for i in 1:nTot] seqs = makeSequence.(x0,seqLength) seqs<em>vectors = [[[e] for e in seq] for seq in seqs] seqs</em>vectors[1][1] y    = seqs_vectors # y here is the value of the sequence itself m    = Chain(Dense(1,5,σ),LSTM(5, 5), Dense(5, 1)) #σ function myloss(x, y)     Flux.reset!(m)               # Reset the state (not the weigtht!)     #[m(x[i]) for i in 1:nSeeds]  # Ignores the output but updates the hidden states     sum(Flux.mse(m(xi), yi) for (xi, yi) in zip(x[1:end], y[1:end])) end</p><p>ps  = params(m) opt = ADAM() println(&quot;mydebug i&quot;) trainxy = zip(seqs<em>vectors,seqs</em>vectors) println(&quot;mydebug l&quot;) Actual training Flux.train!(myloss, ps, trainxy, opt)</p><p>function predictSequence(m,seeds,seqLength)     seq = Vector{Vector{Float32}}(undef,seqLength+length(seeds))     Flux.reset!(m) # Reset the state (not the weigtht!)     [seq[i] = m([convert(Float32, seeds[i])]) for i in 1:nSeeds]     [seq[i] = m(seq[i-1]) for i in nSeeds+1:nSeeds+seqLength]     [s[1] for s in seq] end</p><p>a = predictSequence(m,x0[1],seqLength)</p><p>i = 2 trueseq = makeSequence(x0[i],seqLength) estseq  = predictSequence(m,x0[i],seqLength)</p><p>plot(trueseq[nSeeds+1:end]) plot!(estseq[nSeeds+1:end])</p><p><a href="https://github.com/sylvaticus/SPMLJ/blob/main/lessonsSources/04_-_NN_-_Neural_Networks/0402_Implementing_neural_network_models.jl">View this file on Github</a>.</p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="0401_NeuralNetworksTheory.html">« 0401 NeuralNetworksTheory</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.13 on <span class="colophon-date" title="Thursday 24 March 2022 10:51">Thursday 24 March 2022</span>. Using Julia version 1.7.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
