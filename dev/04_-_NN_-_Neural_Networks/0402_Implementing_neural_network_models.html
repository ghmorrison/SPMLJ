<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>0402 Implementing neural network models · SPMLJ</title><script async src="https://www.googletagmanager.com/gtag/js?id=G-Q39LHCRBB6"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-Q39LHCRBB6', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../index.html">SPMLJ</a></span></div><form class="docs-search" action="../search.html"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../index.html">Index</a></li><li><span class="tocitem">Lessons</span><ul><li><input class="collapse-toggle" id="menuitem-2-1" type="checkbox" checked/><label class="tocitem" for="menuitem-2-1"><span class="docs-label">NN - Neural Networks</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="0401_NeuralNetworksTheory.html">0401 NeuralNetworksTheory</a></li><li class="is-active"><a class="tocitem" href="0402_Implementing_neural_network_models.html">0402 Implementing neural network models</a><ul class="internal"><li><a class="tocitem" href="#Some-stuff-to-set-up-the-environment.."><span>Some stuff to set-up the environment..</span></a></li><li><a class="tocitem" href="#Feed-forward-neural-networks"><span>Feed-forward neural networks</span></a></li><li class="toplevel"><a class="tocitem" href="#####-Using-defaults-hidding-complexity"><span>#### Using defaults - hidding complexity</span></a></li><li class="toplevel"><a class="tocitem" href="#Model-definition..."><span>Model definition...</span></a></li><li class="toplevel"><a class="tocitem" href="#Training..."><span>Training...</span></a></li><li class="toplevel"><a class="tocitem" href="#####-Specifying-all-options"><span>#### Specifying all options</span></a></li><li class="toplevel"><a class="tocitem" href="#Model-definition...-2"><span>Model definition...</span></a></li><li class="toplevel"><a class="tocitem" href="#Training...-2"><span>Training...</span></a></li><li class="toplevel"><a class="tocitem" href="####-Multinomial-classification"><span>### Multinomial classification</span></a></li><li class="toplevel"><a class="tocitem" href="#We-want-to-determine-the-plant-specie-given-some-bothanic-measures-of-the-flower"><span>We want to determine the plant specie given some bothanic measures of the flower</span></a></li><li class="toplevel"><a class="tocitem" href="#Define-the-Artificial-Neural-Network-model"><span>Define the Artificial Neural Network model</span></a></li><li class="toplevel"><a class="tocitem" href="#Training-it-(default-to-ADAM)"><span>Training it (default to ADAM)</span></a></li><li class="toplevel"><a class="tocitem" href="#Test-it"><span>Test it</span></a></li><li class="toplevel"><a class="tocitem" href="####-Regression"><span>### Regression</span></a></li><li class="toplevel"><a class="tocitem" href="#Data-Loading-and-processing.."><span>Data Loading and processing..</span></a></li><li class="toplevel"><a class="tocitem" href="#Model-definition...-3"><span>Model definition...</span></a></li><li class="toplevel"><a class="tocitem" href="#Training...-3"><span>Training...</span></a></li><li class="toplevel"><a class="tocitem" href="###-Convolutional-neural-networks"><span>## Convolutional neural networks</span></a></li><li class="toplevel"><a class="tocitem" href="###-Recursive-neural-networks"><span>## Recursive neural networks</span></a></li><li class="toplevel"><a class="tocitem" href="#Generating-simulated-data"><span>Generating simulated data</span></a></li></ul></li></ul></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Lessons</a></li><li><a class="is-disabled">NN - Neural Networks</a></li><li class="is-active"><a href="0402_Implementing_neural_network_models.html">0402 Implementing neural network models</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="0402_Implementing_neural_network_models.html">0402 Implementing neural network models</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/sylvaticus/SPMLJ/blob/main/lessonsSources/04_-_NN_-_Neural_Networks/0402_Implementing_neural_network_models.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><pre></pre><h1 id="Neural-network-implementations"><a class="docs-heading-anchor" href="#Neural-network-implementations">0402 - Neural network implementations</a><a id="Neural-network-implementations-1"></a><a class="docs-heading-anchor-permalink" href="#Neural-network-implementations" title="Permalink"></a></h1><h2 id="Some-stuff-to-set-up-the-environment.."><a class="docs-heading-anchor" href="#Some-stuff-to-set-up-the-environment..">Some stuff to set-up the environment..</a><a id="Some-stuff-to-set-up-the-environment..-1"></a><a class="docs-heading-anchor-permalink" href="#Some-stuff-to-set-up-the-environment.." title="Permalink"></a></h2><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; cd(@__DIR__)</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; using Pkg</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; Pkg.activate(&quot;.&quot;)</code><code class="nohighlight hljs ansi" style="display:block;">  Activating project at `~/work/SPMLJ/SPMLJ/buildedDoc/04_-_NN_-_Neural_Networks`</code></pre><p>If using a Julia version different than 1.7 please uncomment and run the following line (the guarantee of reproducibility will however be lost) Pkg.resolve()</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; Pkg.instantiate()</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; using Random</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; Random.seed!(123)</code><code class="nohighlight hljs ansi" style="display:block;">Random.TaskLocalRNG()</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ENV[&quot;DATADEPS_ALWAYS_ACCEPT&quot;] = &quot;true&quot;</code><code class="nohighlight hljs ansi" style="display:block;">&quot;true&quot;</code></pre><p>We will <em>not</em> run cross validation here to find the optimal hypermarameters. The process will not be different than those we saw in the lesson on the Perceptron. Instead we focus on creating neural network models, train them based on data and evaluationg their predictions. For feed-forward neural networks (both for classification and regression) we will use <a href="https://github.com/sylvaticus/BetaML.jl">BetaML</a>, while for Convolutional Neural Networks and Recursive Neural NEtworks we will use the <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a> package.</p><h2 id="Feed-forward-neural-networks"><a class="docs-heading-anchor" href="#Feed-forward-neural-networks">Feed-forward neural networks</a><a id="Feed-forward-neural-networks-1"></a><a class="docs-heading-anchor-permalink" href="#Feed-forward-neural-networks" title="Permalink"></a></h2><h3 id="Binary-classification"><a class="docs-heading-anchor" href="#Binary-classification">Binary classification</a><a id="Binary-classification-1"></a><a class="docs-heading-anchor-permalink" href="#Binary-classification" title="Permalink"></a></h3><p>Data loading...</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; using BetaML, DelimitedFiles</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; data  = readdlm(joinpath(dirname(pathof(BetaML)),&quot;..&quot;,&quot;test&quot;,&quot;data&quot;,&quot;binary2DData.csv&quot;),&#39;\t&#39;)</code><code class="nohighlight hljs ansi" style="display:block;">200×3 Matrix{Float64}:
 -1.0   1.76    0.4
 -1.0   0.979   2.24
 -1.0   1.87   -0.977
 -1.0   0.95   -0.151
 -1.0  -0.103   0.411
 -1.0   0.144   1.45
 -1.0   0.761   0.122
 -1.0   0.444   0.334
 -1.0   1.49   -0.205
 -1.0   0.313  -0.854
  ⋮
  1.0  -0.256   0.977
  1.0   2.04    0.343
  1.0   1.01    0.528
  1.0   3.65    2.16
  1.0   2.57    1.78
  1.0   1.65    0.384
  1.0   1.71    1.24
  1.0   2.86    3.14
  1.0   3.47    2.85</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; nR   = size(data,1)</code><code class="nohighlight hljs ansi" style="display:block;">200</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; idx  = shuffle(1:nR)</code><code class="nohighlight hljs ansi" style="display:block;">200-element Vector{Int64}:
 123
 131
  74
  23
  19
  78
  43
 130
  83
 186
   ⋮
 137
 175
 182
  37
  71
  89
 142
  82
 170</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; data = data[idx,:]</code><code class="nohighlight hljs ansi" style="display:block;">200×3 Matrix{Float64}:
  1.0   1.69     0.324
  1.0   0.811    1.49
 -1.0  -0.913    1.12
 -1.0  -0.5097  -0.4381
 -1.0   1.23     1.2
 -1.0  -0.0985  -0.6635
 -1.0   1.49     1.9
  1.0   0.417    2.61
 -1.0  -1.23     0.844
  1.0   2.28     1.01
  ⋮
  1.0   3.96     2.39
  1.0   2.58     2.35
  1.0   2.93     2.34
 -1.0   1.14    -1.23
 -1.0  -1.49     0.439
 -1.0  -0.8034  -0.6895
  1.0   1.31     3.54
 -1.0   0.949    0.0876
  1.0   1.32     3.66</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; X    = copy(data[:,[2,3]])</code><code class="nohighlight hljs ansi" style="display:block;">200×2 Matrix{Float64}:
  1.69     0.324
  0.811    1.49
 -0.913    1.12
 -0.5097  -0.4381
  1.23     1.2
 -0.0985  -0.6635
  1.49     1.9
  0.417    2.61
 -1.23     0.844
  2.28     1.01
  ⋮
  3.96     2.39
  2.58     2.35
  2.93     2.34
  1.14    -1.23
 -1.49     0.439
 -0.8034  -0.6895
  1.31     3.54
  0.949    0.0876
  1.32     3.66</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; y    = max.(0,convert(Array{Int64,1},copy(data[:,1]))) # Converting labels from {-1,1} to {0,1}</code><code class="nohighlight hljs ansi" style="display:block;">200-element Vector{Int64}:
 1
 1
 0
 0
 0
 0
 0
 1
 0
 1
 ⋮
 1
 1
 1
 0
 0
 0
 1
 0
 1</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ((xtrain,xtest),(ytrain,ytest)) = partition([X,y],[0.7,0.3])</code><code class="nohighlight hljs ansi" style="display:block;">2-element Vector{Vector}:
 AbstractMatrix{Float64}[[1.23 1.2; 3.49 -0.07; … ; 2.69 2.69; 0.979 2.24], [-0.8708 -0.5788; 0.127 0.402; … ; -0.312 0.0562; -0.674 0.0318]]
 AbstractVector{Int64}[[0, 1, 0, 0, 0, 0, 1, 1, 1, 1  …  1, 1, 1, 0, 0, 0, 0, 1, 1, 0], [0, 0, 0, 1, 0, 1, 1, 1, 1, 1  …  0, 1, 1, 0, 0, 0, 1, 1, 0, 0]]</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; println(&quot;mydebug 0&quot;)</code><code class="nohighlight hljs ansi" style="display:block;">mydebug 0</code></pre><h1 id="-Using-defaults-hidding-complexity"><a class="docs-heading-anchor" href="#####-Using-defaults-hidding-complexity">#### Using defaults - hidding complexity</a><a id="-Using-defaults-hidding-complexity-1"></a><a class="docs-heading-anchor-permalink" href="#####-Using-defaults-hidding-complexity" title="Permalink"></a></h1><h1 id="Model-definition..."><a class="docs-heading-anchor" href="#Model-definition...">Model definition...</a><a id="Model-definition...-1"></a><a class="docs-heading-anchor-permalink" href="#Model-definition..." title="Permalink"></a></h1><p>l1   = DenseLayer(2,5,f=tanh) l2   = DenseLayer(5,5,f=relu) l3   = DenseLayer(5,1,f=sigmoid) mynn = buildNetwork([l1,l2,l3],squaredCost)</p><h1 id="Training..."><a class="docs-heading-anchor" href="#Training...">Training...</a><a id="Training...-1"></a><a class="docs-heading-anchor-permalink" href="#Training..." title="Permalink"></a></h1><p>train!(mynn,xtrain,ytrain)</p><p>ŷtrain         = predict(mynn, xtrain) |&gt; makeColVector .|&gt; round .|&gt; Int ŷtest          = predict(mynn, xtest)  |&gt; makeColVector .|&gt; round .|&gt; Int trainAccuracy  = accuracy(ŷtrain,ytrain) testAccuracy   = accuracy(ŷtest,ytest)</p><h1 id="-Specifying-all-options"><a class="docs-heading-anchor" href="#####-Specifying-all-options">#### Specifying all options</a><a id="-Specifying-all-options-1"></a><a class="docs-heading-anchor-permalink" href="#####-Specifying-all-options" title="Permalink"></a></h1><h1 id="Model-definition...-2"><a class="docs-heading-anchor" href="#Model-definition...-2">Model definition...</a><a class="docs-heading-anchor-permalink" href="#Model-definition...-2" title="Permalink"></a></h1><p>l1   = DenseLayer(2,5,f=tanh, df= dtanh,rng=copy(FIXEDRNG)) l2   = DenseLayer(5,5,f=relu,df=drelu,rng=copy(FIXEDRNG)) l3   = DenseLayer(5,1,f=sigmoid,df=dsigmoid,rng=copy(FIXEDRNG)) mynn = buildNetwork([l1,l2,l3],squaredCost,dcf=dSquaredCost,name=&quot;A classification task&quot;) # or crossEntropy / dCrossEntropy</p><h1 id="Training...-2"><a class="docs-heading-anchor" href="#Training...-2">Training...</a><a class="docs-heading-anchor-permalink" href="#Training...-2" title="Permalink"></a></h1><p>function myOwnTrainingInfo(nn,x,y;n,nBatches,epochs,verbosity,nEpoch,nBatch)     if verbosity == NONE         return false # doesn&#39;t stop the training     end     nMsgDict = Dict(LOW =&gt; 0, STD =&gt; 10,HIGH =&gt; 100, FULL =&gt; n)     nMsgs = nMsgDict[verbosity]     batchSize = size(x,1)     if verbosity == FULL || ( nBatch == nBatches &amp;&amp; ( nEpoch == 1  || nEpoch % ceil(epochs/nMsgs) == 0))</p><pre><code class="nohighlight hljs">   ϵ = loss(nn,x,y)
   println(&quot;Training.. \t avg ϵ on (Epoch $nEpoch Batch $nBatch): \t $(ϵ)&quot;)
end
return false</code></pre><p>end train!(mynn,xtrain,ytrain,epochs=300,batchSize=6,sequential=false,verbosity=STD,cb=myOwnTrainingInfo,optAlg=ADAM(η=t -&gt; 0.001, λ=1.0, β₁=0.9, β₂=0.999, ϵ=1e-8),rng=copy(FIXEDRNG))</p><p>ŷtrain         = predict(mynn, xtrain) |&gt; makeColVector .|&gt; round .|&gt; Int ŷtest          = predict(mynn, xtest)  |&gt; makeColVector .|&gt; round .|&gt; Int trainAccuracy  = accuracy(ŷtrain,ytrain) testAccuracy   = accuracy(ŷtest,ytest)</p><h1 id="-Multinomial-classification"><a class="docs-heading-anchor" href="####-Multinomial-classification">### Multinomial classification</a><a id="-Multinomial-classification-1"></a><a class="docs-heading-anchor-permalink" href="####-Multinomial-classification" title="Permalink"></a></h1><h1 id="We-want-to-determine-the-plant-specie-given-some-bothanic-measures-of-the-flower"><a class="docs-heading-anchor" href="#We-want-to-determine-the-plant-specie-given-some-bothanic-measures-of-the-flower">We want to determine the plant specie given some bothanic measures of the flower</a><a id="We-want-to-determine-the-plant-specie-given-some-bothanic-measures-of-the-flower-1"></a><a class="docs-heading-anchor-permalink" href="#We-want-to-determine-the-plant-specie-given-some-bothanic-measures-of-the-flower" title="Permalink"></a></h1><p>iris     = readdlm(joinpath(dirname(Base.find_package(&quot;BetaML&quot;)),&quot;..&quot;,&quot;test&quot;,&quot;data&quot;,&quot;iris.csv&quot;),&#39;,&#39;,skipstart=1) iris     = iris[shuffle(axes(iris, 1)), :] # Shuffle the records, as they aren&#39;t by default x        = convert(Array{Float64,2}, iris[:,1:4]) y        = map(x-&gt;Dict(&quot;setosa&quot; =&gt; 1, &quot;versicolor&quot; =&gt; 2, &quot;virginica&quot; =&gt;3)[x],iris[:, 5]) # Convert the target column to numbers</p><p>((xtrain,xtest),(ytrain,ytest)) = partition([x,y],[0.8,0.2],shuffle=false)</p><p>ytrain_oh = oneHotEncoder(ytrain) # Convert to One-hot representation (e.g. 2 =&gt; [0 1 0], 3 =&gt; [0 0 1])</p><h1 id="Define-the-Artificial-Neural-Network-model"><a class="docs-heading-anchor" href="#Define-the-Artificial-Neural-Network-model">Define the Artificial Neural Network model</a><a id="Define-the-Artificial-Neural-Network-model-1"></a><a class="docs-heading-anchor-permalink" href="#Define-the-Artificial-Neural-Network-model" title="Permalink"></a></h1><p>l1   = DenseLayer(4,10,f=relu) # Activation function is ReLU l2   = DenseLayer(10,3)        # Activation function is identity by default l3   = VectorFunctionLayer(3,f=softmax) # Add a (parameterless) layer whose activation function (softMax in this case) is defined to all its nodes at once mynn = buildNetwork([l1,l2,l3],crossEntropy,name=&quot;Multinomial logistic regression Model Sepal&quot;) # Build the NN and use the squared cost (aka MSE) as error function (crossEntropy could also be used)</p><h1 id="Training-it-(default-to-ADAM)"><a class="docs-heading-anchor" href="#Training-it-(default-to-ADAM)">Training it (default to ADAM)</a><a id="Training-it-(default-to-ADAM)-1"></a><a class="docs-heading-anchor-permalink" href="#Training-it-(default-to-ADAM)" title="Permalink"></a></h1><p>res = train!(mynn,scale(xtrain),ytrain_oh,batchSize=6) # Use optAlg=SGD() to use Stochastic Gradient Descent instead</p><h1 id="Test-it"><a class="docs-heading-anchor" href="#Test-it">Test it</a><a id="Test-it-1"></a><a class="docs-heading-anchor-permalink" href="#Test-it" title="Permalink"></a></h1><p>ŷtrain        = predict(mynn,scale(xtrain))   # Note the scaling function ŷtest         = predict(mynn,scale(xtest)) trainAccuracy = accuracy(ŷtrain,ytrain) testAccuracy  = accuracy(ŷtest,ytest,tol=1,ignoreLabels=false)</p><p>cm = ConfusionMatrix(ŷtest,ytest, labels=[&quot;setosa&quot;, &quot;versicolor&quot;, &quot;virginica&quot;]) println(cm)</p><h1 id="-Regression"><a class="docs-heading-anchor" href="####-Regression">### Regression</a><a id="-Regression-1"></a><a class="docs-heading-anchor-permalink" href="####-Regression" title="Permalink"></a></h1><h1 id="Data-Loading-and-processing.."><a class="docs-heading-anchor" href="#Data-Loading-and-processing..">Data Loading and processing..</a><a id="Data-Loading-and-processing..-1"></a><a class="docs-heading-anchor-permalink" href="#Data-Loading-and-processing.." title="Permalink"></a></h1><p>using Pipe, HTTP, CSV, Plots, DataFrames urlData = &quot;https://www4.stat.ncsu.edu/~boos/var.select/diabetes.tab.txt&quot; data = @pipe HTTP.get(urlData).body |&gt; CSV.File(<em>, delim=&#39;\t&#39;) |&gt; DataFrame sex</em>oh = oneHotEncoder(data.SEX) X = hcat(data.AGE, Matrix(data[:,3:10]),sex_oh) y = data.Y (xtrain,xval),(ytrain,yval) = partition([X,y],[0.8,0.2])</p><h1 id="Model-definition...-3"><a class="docs-heading-anchor" href="#Model-definition...-3">Model definition...</a><a class="docs-heading-anchor-permalink" href="#Model-definition...-3" title="Permalink"></a></h1><p>l1   = DenseLayer(11,20,f=relu) l2   = DenseLayer(20,20,f=relu) l3   = DenseLayer(20,1,f=relu) # y is positive mynn = buildNetwork([l1,l2,l3],squaredCost)</p><h1 id="Training...-3"><a class="docs-heading-anchor" href="#Training...-3">Training...</a><a class="docs-heading-anchor-permalink" href="#Training...-3" title="Permalink"></a></h1><p>trainingLogs = train!(mynn,scale(xtrain),ytrain,batchSize=6,epochs=600)</p><p>ŷtrain   = predict(mynn, scale(xtrain)) |&gt; makeColVector ŷval     = predict(mynn, scale(xval))  |&gt; makeColVector trainRME = meanRelError(ŷtrain,ytrain,normRec=false) testRME  = meanRelError(ŷval,yval,normRec=false) plot(trainingLogs.ϵ_epochs[10:end]) scatter(yval,ŷval,xlabel=&quot;obs&quot;,ylabel=&quot;est&quot;,legend=nothing)</p><h1 id="-Convolutional-neural-networks"><a class="docs-heading-anchor" href="###-Convolutional-neural-networks">## Convolutional neural networks</a><a id="-Convolutional-neural-networks-1"></a><a class="docs-heading-anchor-permalink" href="###-Convolutional-neural-networks" title="Permalink"></a></h1><p>println(&quot;Working with a convolutional neural network...&quot;) using Flux, MLDatasets, Statistics, Plots</p><p>x<em>train, y</em>train = MLDatasets.MNIST.traindata() x<em>train          = permutedims(x</em>train,(2,1,3)) # For correct img axis x<em>train          = convert(Array{Float32,3},x</em>train) x<em>train          = reshape(x</em>train,(28,28,1,60000)) y<em>train          = Flux.onehotbatch(y</em>train, 0:9) train<em>data       = Flux.Data.DataLoader((x</em>train, y<em>train), batchsize=128) x</em>test, y<em>test   = MLDatasets.MNIST.testdata() x</em>test           = permutedims(x<em>test,(2,1,3)) # For correct img axis x</em>test           = convert(Array{Float32,3},x<em>test) x</em>test           = reshape(x<em>test,(28,28,1,10000)) y</em>test           = Flux.onehotbatch(y_test, 0:9)</p><p>myaccuracy(ŷ, y) = (mean(Flux.onecold(ŷ) .== Flux.onecold(y))) myloss(x, y)     = Flux.crossentropy(model(x), y)</p><p>model = Chain(     # 28x28 =&gt; 14x14     Conv((5, 5), 1=&gt;8, pad=2, stride=2, relu),     # 14x14 =&gt; 7x7     Conv((3, 3), 8=&gt;16, pad=1, stride=2, relu),     # 7x7 =&gt; 4x4     Conv((3, 3), 16=&gt;32, pad=1, stride=2, relu),     # 4x4 =&gt; 2x2     Conv((3, 3), 32=&gt;32, pad=1, stride=2, relu),     # Average pooling on each width x height feature map     GlobalMeanPool(),     Flux.flatten,     Dense(32, 10),     Flux.softmax )</p><p>opt = Flux.ADAM() ps  = Flux.params(model) number<em>epochs = 4 println(&quot;mydebug a&quot;) Flux.@epochs number</em>epochs Flux.train!(myloss, ps, train_data, opt) println(&quot;mydebug b&quot;)</p><p>ŷtrain =   model(x<em>train) println(&quot;mydebug c&quot;) ŷtest  =   model(x</em>test) println(&quot;mydebug d&quot;) myaccuracy(ŷtrain, y<em>train) println(&quot;mydebug e&quot;) myaccuracy(ŷtest, y</em>test) println(&quot;mydebug f&quot;) plot(Gray.(x_train[:,:,1,1])) println(&quot;mydebug g&quot;)</p><h1 id="-Recursive-neural-networks"><a class="docs-heading-anchor" href="###-Recursive-neural-networks">## Recursive neural networks</a><a id="-Recursive-neural-networks-1"></a><a class="docs-heading-anchor-permalink" href="###-Recursive-neural-networks" title="Permalink"></a></h1><h1 id="Generating-simulated-data"><a class="docs-heading-anchor" href="#Generating-simulated-data">Generating simulated data</a><a id="Generating-simulated-data-1"></a><a class="docs-heading-anchor-permalink" href="#Generating-simulated-data" title="Permalink"></a></h1><p>nSeeds    = 5 seqLength = 10 nTrains   = 1000 nTest     = 100 println(&quot;mydebug h&quot;) nTot = nTrains+nTest makeSeeds(nSeeds) = 2 .* (rand(nSeeds) .- 0.5) # [-1,+1] function makeSequence(seeds,seqLength)   seq = Vector{Float32}(undef,seqLength+nSeeds) # Flux Works with Float32 for performance reasons   [seq[i] = seeds[i] for i in 1:nSeeds]   for i in nSeeds:(seqLength+nSeeds)     seq[i] = seq[i-1] + seeds[1]<em>0.1</em>seq[i-1] +seeds[2]<em>seeds[3]</em>seq[i-1]<em>0.4+seeds[4]</em>seeds[5]*(seq[i-3]-seq[i-4])     #seq[i] = seq[i-1] + mean(seeds)   end   return seq   return seq[nSeeds+1:end] end</p><p>seq=makeSequence(makeSeeds(nSeeds),seqLength) plot(seq)</p><p>x0   = [makeSeeds(nSeeds) for i in 1:nTot] seqs = makeSequence.(x0,seqLength) seqs<em>vectors = [[[e] for e in seq] for seq in seqs] seqs</em>vectors[1][1] y    = seqs_vectors # y here is the value of the sequence itself m    = Chain(Dense(1,5,σ),LSTM(5, 5), Dense(5, 1)) #σ function myloss(x, y)     Flux.reset!(m)               # Reset the state (not the weigtht!)     #[m(x[i]) for i in 1:nSeeds]  # Ignores the output but updates the hidden states     sum(Flux.mse(m(xi), yi) for (xi, yi) in zip(x[1:end], y[1:end])) end</p><p>ps  = params(m) opt = ADAM() println(&quot;mydebug i&quot;) trainxy = zip(seqs<em>vectors,seqs</em>vectors) println(&quot;mydebug l&quot;) Actual training Flux.train!(myloss, ps, trainxy, opt)</p><p>function predictSequence(m,seeds,seqLength)     seq = Vector{Vector{Float32}}(undef,seqLength+length(seeds))     Flux.reset!(m) # Reset the state (not the weigtht!)     [seq[i] = m([convert(Float32, seeds[i])]) for i in 1:nSeeds]     [seq[i] = m(seq[i-1]) for i in nSeeds+1:nSeeds+seqLength]     [s[1] for s in seq] end</p><p>a = predictSequence(m,x0[1],seqLength)</p><p>i = 2 trueseq = makeSequence(x0[i],seqLength) estseq  = predictSequence(m,x0[i],seqLength)</p><p>plot(trueseq[nSeeds+1:end]) plot!(estseq[nSeeds+1:end])</p><p><a href="https://github.com/sylvaticus/SPMLJ/blob/main/lessonsSources/04_-_NN_-_Neural_Networks/0402_Implementing_neural_network_models.jl">View this file on Github</a>.</p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="0401_NeuralNetworksTheory.html">« 0401 NeuralNetworksTheory</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.13 on <span class="colophon-date" title="Thursday 24 March 2022 10:36">Thursday 24 March 2022</span>. Using Julia version 1.7.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
