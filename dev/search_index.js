var documenterSearchIndex = {"docs":
[{"location":"00_-_INTRO_-_Introduction_julia_ml/0003_-_Introduction_to_Julia.html","page":"0003 - Introduction to Julia","title":"0003 - Introduction to Julia","text":"TODO. Please refer to the videos.","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_Modules_packages_and_environments.html","page":"0006 - Modules packages and environments","title":"0006 - Modules packages and environments","text":"TODO. Please refer to the videos.","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html#Quiz-01.1-on-Modules,-packages-and-environments","page":"0006 - q01 - QUIZ 0.1","title":"Quiz 01.1 on Modules, packages and environments","text":"","category":"section"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"cd(@__DIR__)    \nusing Pkg      \nPkg.activate(\".\")  \n## Pkg.resolve()   \n## Pkg.instantiate()\nusing Random\nRandom.seed!(123)\nusing QuizQuestions","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html#Q1:-What-is-printed-?","page":"0006 - q01 - QUIZ 0.1","title":"Q1: What is printed ?","text":"","category":"section"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"Given a file foo.jl with the following code:","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"println(\"A - I am in foo.jl\")\n\nmodule Foo\n\nprintln(\"B - I am in module Foo in foo.jl\")\nexport x\n\nconst x=1\nconst y=2\nz() = println(\"C - I am in a function of module Foo\")\nend\n\nmodule Foo2\n\nprintln(\"D - I am in module Foo2 in foo.jl\")\nexport a\n\nconst a=1\nconst b=2\nc() = println(\"E - I am in a function of module Foo2\")\nend","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"Which print statements will appear after running the command include(\"foo.jl\") ?","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"\nchoices = [ # hide\n  \"All statements from `A` to `E`\", # hide\n  \"No statements will be printed (e.g. due to an error)\", # hide\n  \"Statement `A` only\", # hide\n  \"Statements `A`, `B` and `D` only\", # hide\n  \"Statements `A`, `C` and `E` only\", # hide\n  \"Statements `B` and `D` only\", # hide\n  ]  # hide\nanswers = [4]  # hide\nmultiq(choices, answers;)  # hide\n","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"<details><summary>RESOLUTION</summary>","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"Including the file would result in evaluating the code in the file and hence in the statements A, B and D to be printed.  Statements C and E are within function definition and would occur only when functions z or c would have been called.}","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"The correct answer is:","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"\"No statements will be printed (e.g. due to an error)\"","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"</details>","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html#Q2:-Inclusion-of-a-module","page":"0006 - q01 - QUIZ 0.1","title":"Q2: Inclusion of a module","text":"","category":"section"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"Given a file foo.jl with the following code:","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"module Foo\n\nexport x\n\nconst x=1\nconst y=2\nz() = println(\"Hello world!\")\nend","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"and the following sequence of commands:","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"include(\"foo.jl\")         # Command 1\nx                         # Command 2\nFoo.x                     # Command 3\nusing Foo                 # Command 4\nusing .Foo                # Command 5\nx                         # Command 6\nFoo.z()                   # Command 7","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"Which statements are correct ?","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"\nchoices = [ # hide\n  \"Command 1 is wrong at it should have been `include foo` (without the .jl file extension)\", # hide\n  \"Command 2 returns the value `1`\", # hide\n  \"Command 3 returns the value `1`\", # hide\n  \"Command 4 returns an `ArgumentError: Package Foo not found in current path:`\", # hide\n  \"Command 5 returns an `ArgumentError: Package Foo not found in current path:`\", # hide\n  \"Command 6 returns the value  `1`\", # hide\n  \"Command 7 returns an `UndefVarError: z not defined`\", # hide\n  ]  # hide\nanswers = [3,4,6]  # hide\nmultiq(choices, answers;keep_order=true)  # hide\n","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"<details><summary>RESOLUTION</summary>","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"The include(\"foo.jl\") statement evaluates the included content, but it doesn't yet bring it into scope. You can't yet refer directly to the objects of the Foo module, you need to use the qualified name as in command 3. Foo is a module, not a package, so command 4 will complain that it doesn't find the \"package\" Foo. After the module has been bring to scope we can refer to x directly as in command 6. Command 7, as we are using the qualified name, is indipenden than whether z was exported by Foo or not, and hence it works, and would have been worked even without the using .Foo of command 5.","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"The correct answers are:","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"\"Command 3 returns the value 1\"\n\"Command 4 returns an ArgumentError: Package Foo not found in current path:\"\n\"Command 6 returns the value  1\"","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"</details>","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html#Q3:-Submodules","page":"0006 - q01 - QUIZ 0.1","title":"Q3: Submodules","text":"","category":"section"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"Given a file Foo.jl with the following code:","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"module Foo\nexport x, plusOne\nx = 1\nplusOne(x) = x + 1\nmodule Foo2\n  export plusTwo\n  plusTwo(x) = plusOne(x)+1\nend\nend","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"After including the file we try to run the command Foo.Foo2.plusTwo(10). Which of the following statements is correct ?","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"\nchoices = [ # hide\n  \"The result is 12\", # hide\n  \"The result is 3\", # hide\n  \"The result is an error that we can avoid if we run instead the command `Main.Foo.Foo2.plusTwo(10)`\", # hide\n  \"The result is an error that we can avoid if we type `using Foo` before that command\", # hide\n  \"The result is an error that we can avoid if we type `using .Foo` before that command\", # hide\n  \"The result is an error that we can avoid if the function `plusTwo` in module `Foo2` is defined as `plusTwo(x) = Foo.plusOne(x)+1`\", # hide\n  \"The result is an error that we can avoid if the function `plusTwo` in module `Foo2` is defined as `plusTwo(x) = Main.Foo.plusOne(x)+1`\", # hide\n  \"The result is an error that we can avoid if in module `Foo2` the function `plusTwo` is preceded by the statement `using Foo`\", # hide\n  \"The result is an error that we can avoid if in module `Foo2` the function `plusTwo` is preceded by the statement `using .Foo`\", # hide\n  \" The result is an error that we can avoid if in module `Foo2` the function `plusTwo` is preceded by the statement `using ..Foo`\", # hide\n  ]  # hide\nanswers = [7,10]  # hide\nmultiq(choices, answers;)  # hide\n","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"<details><summary>RESOLUTION</summary>","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"The given command results in a UndefVarError: plusOne not defined. Indeed even if Foo2 is a submodule of Foo, it doesn't inherit the scope of parent modules. So its code can't find the 'plusOne' function.  When in the REPL we run the command we are in the Main module. Adding using .Foo doesn't change anything, as the problem is in the scope of the Foo2 module, not in those of the REPL (Main - and , of course, typing using Foo looks-up for the package Foo, not the module Foo, and would end in a Package Foo not found error. So what can we do? One solution is using in the plusTwo function the full path of the plusOne function:  plusTwo(x) = Main.Foo.plusOne(x)+1. While this works, it may be a less portable solution, as it then requires module Foo to be a child of Main. Perhaps a better solution is to use a relative path and use the statement using ..Foo in module Foo2 before the definition of plusTwo (trying to use a relative path directly in the function definition as in plusTwo(x) = ..Foo.plusOne(x)+1 would result in a parsing error)","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"The correct answers are:","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"The result is an error that we can avoid if the function plusTwo in module Foo2 is defined as plusTwo(x) = Main.Foo.plusOne(x)+1\nThe result is an error that we can avoid if in module Foo2 the function plusTwo is preceded by the statement using ..Foo","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"</details>","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html#Q4:-Submodules2","page":"0006 - q01 - QUIZ 0.1","title":"Q4: Submodules2","text":"","category":"section"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"Given a module Foo with the following code:","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"module Foo\nexport x\nx = 1\nmodule Foo2\n  export plusTwo\n  plusTwo(x) = x+2\nend\nmodule Foo3\n  export plusThree\n  [XXXX]\n  plusThree(x) = plusTwo(x)+1\n  end                  \nend","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"Which of the following statements are correct ?","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"\nchoices = [ # hide\n  \"`[XXXX]` should be `using Main.Foo.Foo2` for the function `plusThree` to work\", # hide\n  \"`[XXXX]` should be `using Foo2` for the function `plusThree` to work\", # hide\n  \"`[XXXX]` should be `using .Foo2` for the function `plusThree` to work\", # hide\n  \"`[XXXX]` should be `using ..Foo2` for the function `plusThree` to work\", # hide\n  \"`[XXXX]` should be `import Main.Foo.Foo2` for the function `plusThree` to work\", # hide\n  \"`[XXXX]` should be `import Foo2` for the function `plusThree` to work\", # hide\n  \"`[XXXX]` should be `import .Foo2` for the function `plusThree` to work\", # hide\n  \"`[XXXX]` should be `import ..Foo2` for the function `plusThree` to work\", # hide\n  ]  # hide\nanswers = [1,4]  # hide\nmultiq(choices, answers;)  # hide\n","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"<details><summary>RESOLUTION</summary>","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"The function plusTwo needs to access a function on a sibling module. So the module Foo2 must be retrieved by going up to one level with the two dots and then naming the module, i.e. using ..Foo2 or using the full module path using Main.Foo.Foo2. import statemens alone will not work as the plusThree function call the plusTwo function using the unqualified name, without prefixing the module, so the plusThree function name need to be exported.","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"The correct answers are:","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"\"[XXXX] should be using Main.Foo.Foo2 for the function plusThree to work\"\n\"[XXXX] should be using ..Foo2 for the function plusThree to work\"","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"</details>","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html#Q5:-Reproducibility","page":"0006 - q01 - QUIZ 0.1","title":"Q5: Reproducibility","text":"","category":"section"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"Which elements do you have to provide to others to guarantee reproducibility of your results obtained with a Julia project?","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"\nchoices = [ # hide\n  \"The input data of your analysis\", # hide\n  \"The full source code of the scripts you have used\", # hide\n  \"The content of the Julia user folder on the machine your code ran to produce the results (e.g. `/home/[username]/.julia` in Linux)\", # hide\n  \"The file `Manifest.toml` of the environment where your code ran to produce the results\", # hide\n  \"The file `Project.toml` of the environment where your code ran to produce the results\", # hide\n  ]  # hide\nanswers = [1,2,4]  # hide\nmultiq(choices, answers;)  # hide\n","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"<details><summary>RESOLUTION</summary>","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"To provide replicable results, assuming a deterministic algorithm or one where the random seed generator has been fixed, we need to provide the input data, the source code and the 'Manifest.toml' file that describe the exact version of all packages. The Project.toml file instead, when present, is used to describe in which conditions our scripts could be used (i.e. the list and eventually range of dependent packages), but not a unique environment state. The information of the Manifest.toml (and, for Julia versions before 1.7, the Julia version itself, as this info was not encoded in the Manifest.toml file) is enougth, we don't need to provide the whole content of the user Julia folder.","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"The correct answers are:","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"\"The input data of your analysis\"\n\"The full source code of the scripts you have used\"\n\"The file Manifest.toml of the environment where your code ran to produce the results\"","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0006_-_q01_-_QUIZ_0.1.html","page":"0006 - q01 - QUIZ 0.1","title":"0006 - q01 - QUIZ 0.1","text":"</details>","category":"page"},{"location":"01_-_JULIA1_-_Basic_Julia_programming/0101_-_q01_-_QUIZ_1.1.html#Quiz-on-Basic-Syntax","page":"Quiz on Basic Syntax","title":"Quiz on Basic Syntax","text":"","category":"section"},{"location":"01_-_JULIA1_-_Basic_Julia_programming/0101_-_q01_-_QUIZ_1.1.html","page":"Quiz on Basic Syntax","title":"Quiz on Basic Syntax","text":"cd(@__DIR__)    \nusing Pkg      \nPkg.activate(\".\")  \n## Pkg.resolve()   \n## Pkg.instantiate()\nusing Random\nRandom.seed!(123)\nusing QuizQuestions","category":"page"},{"location":"01_-_JULIA1_-_Basic_Julia_programming/0101_-_q01_-_QUIZ_1.1.html","page":"Quiz on Basic Syntax","title":"Quiz on Basic Syntax","text":"","category":"page"},{"location":"01_-_JULIA1_-_Basic_Julia_programming/0101_-_q01_-_QUIZ_1.1.html#Q1:-What-is-it-stored-in-a-project-file-?","page":"Quiz on Basic Syntax","title":"Q1: What is it stored in a project file ?","text":"","category":"section"},{"location":"01_-_JULIA1_-_Basic_Julia_programming/0101_-_q01_-_QUIZ_1.1.html","page":"Quiz on Basic Syntax","title":"Quiz on Basic Syntax","text":"What information can be stored on a Julia Project.toml file ?","category":"page"},{"location":"01_-_JULIA1_-_Basic_Julia_programming/0101_-_q01_-_QUIZ_1.1.html","page":"Quiz on Basic Syntax","title":"Quiz on Basic Syntax","text":"\nchoices = [ # hide\n    \"The name of the packages directly used in the project (julia scripts)\", # hide\n    \"The ID of the packages directly used in the project (julia scripts)\", # hide\n    \"The minimum and maximum version of the packages directly used in the project (julia scripts) that are compatible with the project\", # hide\n    \"The exact version of the packages directly emploied in the project (julia scripts)\", # hide\n    \"The name of all the dependencies libraries of the project (julia scripts)\", # hide\n    \"The ID of all the dependencies libraries of the project (julia scripts)\", # hide\n    \"The minimum and maximum version of all the dependencies libraries used in the project (julia scripts) that are compatible with the project\", # hide\n    \"The exact version of all the dependencies libraries emploied in the project (julia scripts)\", # hide\n    \"None of the (other) sentences is correct\" ]  # hide\nanswers = [1,2,3]  # hide\nmultiq(choices, answers;)  # hide\n","category":"page"},{"location":"01_-_JULIA1_-_Basic_Julia_programming/0101_-_q01_-_QUIZ_1.1.html","page":"Quiz on Basic Syntax","title":"Quiz on Basic Syntax","text":"<details><summary>RESOLUTION</summary>","category":"page"},{"location":"01_-_JULIA1_-_Basic_Julia_programming/0101_-_q01_-_QUIZ_1.1.html","page":"Quiz on Basic Syntax","title":"Quiz on Basic Syntax","text":"The Project.toml file task is to indicate which is the set of packages that works with the given project, but not the concrete istance of the environment that is used in a project, that is the exact version of all directly and indirectly used packages. This is indeed the task of the Manifest.toml file.","category":"page"},{"location":"01_-_JULIA1_-_Basic_Julia_programming/0101_-_q01_-_QUIZ_1.1.html","page":"Quiz on Basic Syntax","title":"Quiz on Basic Syntax","text":"The correct answers are:","category":"page"},{"location":"01_-_JULIA1_-_Basic_Julia_programming/0101_-_q01_-_QUIZ_1.1.html","page":"Quiz on Basic Syntax","title":"Quiz on Basic Syntax","text":"\"The name of the packages directly used in the project (julia scripts)\"\n\"The ID of the packages directly used in the project (julia scripts)\"\n\"The minimum and maximum version of the packages directly used in the project (julia scripts) that are compatible with the project\"","category":"page"},{"location":"01_-_JULIA1_-_Basic_Julia_programming/0101_-_q01_-_QUIZ_1.1.html","page":"Quiz on Basic Syntax","title":"Quiz on Basic Syntax","text":"</details>","category":"page"},{"location":"01_-_JULIA1_-_Basic_Julia_programming/0101_-_q01_-_QUIZ_1.1.html","page":"Quiz on Basic Syntax","title":"Quiz on Basic Syntax","text":"","category":"page"},{"location":"01_-_JULIA1_-_Basic_Julia_programming/0101_-_q01_-_QUIZ_1.1.html#Q2:-Syntax-for-comments","page":"Quiz on Basic Syntax","title":"Q2: Syntax for comments","text":"","category":"section"},{"location":"01_-_JULIA1_-_Basic_Julia_programming/0101_-_q01_-_QUIZ_1.1.html","page":"Quiz on Basic Syntax","title":"Quiz on Basic Syntax","text":"Given the following sequence of commands (one for each line) run in an interactive session:","category":"page"},{"location":"01_-_JULIA1_-_Basic_Julia_programming/0101_-_q01_-_QUIZ_1.1.html","page":"Quiz on Basic Syntax","title":"Quiz on Basic Syntax","text":"# a = 1\na = 2 # hello\na = # hello # 3\n#= a = 4\n#= a = 5 =#\na = 6\n=#","category":"page"},{"location":"01_-_JULIA1_-_Basic_Julia_programming/0101_-_q01_-_QUIZ_1.1.html","page":"Quiz on Basic Syntax","title":"Quiz on Basic Syntax","text":"Which statements are correct ?","category":"page"},{"location":"01_-_JULIA1_-_Basic_Julia_programming/0101_-_q01_-_QUIZ_1.1.html","page":"Quiz on Basic Syntax","title":"Quiz on Basic Syntax","text":"choices = [ # hide\n    \"`a` is now `1`\", # hide\n    \"`a` is now `2`\", # hide\n    \"`a` is now `3`\", # hide\n    \"`a` is now `4`\", # hide\n    \"`a` is now `5`\", # hide\n    \"`a` is now `6`\", # hide\n    \"None of the (other) sentences is correct\", # hide\n    \"At least one of that commands raises a run-time error\", # hide\n    \"None of that commands raises a run-time error\"]  # hide\nanswers = [2,8]  # hide\nmultiq(choices, answers;)  # hide","category":"page"},{"location":"01_-_JULIA1_-_Basic_Julia_programming/0101_-_q01_-_QUIZ_1.1.html","page":"Quiz on Basic Syntax","title":"Quiz on Basic Syntax","text":"<details><summary>RESOLUTION</summary>","category":"page"},{"location":"01_-_JULIA1_-_Basic_Julia_programming/0101_-_q01_-_QUIZ_1.1.html","page":"Quiz on Basic Syntax","title":"Quiz on Basic Syntax","text":"The first command is a comment. On the second one, a is assigned the value 2. The third one raises a syntax error as the equal operator expects a right and a left hand side, while here the right hand side is all commented out. Finally lines 4  to the end is a big nested comment. It results that after that commands have been run, a remains assigned to 2. The correct answers are:","category":"page"},{"location":"01_-_JULIA1_-_Basic_Julia_programming/0101_-_q01_-_QUIZ_1.1.html","page":"Quiz on Basic Syntax","title":"Quiz on Basic Syntax","text":"\"a is now 2\"\n\"At least one of that commands raises a run-time error\"","category":"page"},{"location":"01_-_JULIA1_-_Basic_Julia_programming/0101_-_q01_-_QUIZ_1.1.html","page":"Quiz on Basic Syntax","title":"Quiz on Basic Syntax","text":"</details>","category":"page"},{"location":"01_-_JULIA1_-_Basic_Julia_programming/0101_-_q01_-_QUIZ_1.1.html","page":"Quiz on Basic Syntax","title":"Quiz on Basic Syntax","text":"","category":"page"},{"location":"01_-_JULIA1_-_Basic_Julia_programming/0101_-_q01_-_QUIZ_1.1.html#Q3:-Various-syntax-rules","page":"Quiz on Basic Syntax","title":"Q3: Various syntax rules","text":"","category":"section"},{"location":"01_-_JULIA1_-_Basic_Julia_programming/0101_-_q01_-_QUIZ_1.1.html","page":"Quiz on Basic Syntax","title":"Quiz on Basic Syntax","text":"Given a file \"Foo.jl\" with the following code:","category":"page"},{"location":"01_-_JULIA1_-_Basic_Julia_programming/0101_-_q01_-_QUIZ_1.1.html","page":"Quiz on Basic Syntax","title":"Quiz on Basic Syntax","text":"function foo(x)\nprintln(x²)\nend\na = [2,3]\nfoo(a)\nfoo.(a)\nfoo(a[1])","category":"page"},{"location":"01_-_JULIA1_-_Basic_Julia_programming/0101_-_q01_-_QUIZ_1.1.html","page":"Quiz on Basic Syntax","title":"Quiz on Basic Syntax","text":"Which of the following statements are correct ?","category":"page"},{"location":"01_-_JULIA1_-_Basic_Julia_programming/0101_-_q01_-_QUIZ_1.1.html","page":"Quiz on Basic Syntax","title":"Quiz on Basic Syntax","text":"choices = [ # hide\n    \"The output of `foo(a)` is `[4,9]`\", # hide\n    \"The output of `foo.(a)` is `[4,9]`\", # hide\n    \"The output of `foo(a[1])` is `4`\", # hide\n    \"The output of `foo(a[1])` is `9`\", # hide\n    \"Defining the function produces a run-time error because the body of the function is not idented\", # hide\n    \"Calling the function produces a run-time error because the body of the function is not idented\", # hide\n    \"Calling the function produces a run-time error because Unicode characters (`²`) are not allowed in Julia\", # hide\n    \"Calling the function produces a run-time error because `x²` is not defined\", # hide\n    \"None of the (other) sentences is correct\", # hide\n]  # hide\nanswers = [8]  # hide\nmultiq(choices, answers;)  # hide","category":"page"},{"location":"01_-_JULIA1_-_Basic_Julia_programming/0101_-_q01_-_QUIZ_1.1.html","page":"Quiz on Basic Syntax","title":"Quiz on Basic Syntax","text":"<details><summary>RESOLUTION</summary>","category":"page"},{"location":"01_-_JULIA1_-_Basic_Julia_programming/0101_-_q01_-_QUIZ_1.1.html","page":"Quiz on Basic Syntax","title":"Quiz on Basic Syntax","text":"First, Unicode characters are allowed (with very rare exceptions) and identation doesn't matter in Julia. We would then be tempted to say hence that the broadcasted call foo.(a) produces [4,9] as output and foo(a[1]) produces 4. However the rising to the power is not obtained by using the Unicode ² character, but using the exponential operator, i.e. x^2. x² is just an other idetifier name that has not been defined, so the function in all cases returns an error that x² is not defined. The correct answer is:","category":"page"},{"location":"01_-_JULIA1_-_Basic_Julia_programming/0101_-_q01_-_QUIZ_1.1.html","page":"Quiz on Basic Syntax","title":"Quiz on Basic Syntax","text":"\"Calling the function produces a run-time error because x² is not defined\"","category":"page"},{"location":"01_-_JULIA1_-_Basic_Julia_programming/0101_-_q01_-_QUIZ_1.1.html","page":"Quiz on Basic Syntax","title":"Quiz on Basic Syntax","text":"</details>","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0005_-_How_to_install_Julia_and_git.html","page":"0005 - How to install Julia and git","title":"0005 - How to install Julia and git","text":"TODO. Please refer to the videos.","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html#Neural-Networks-architectures","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"","category":"section"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"While powerful, neural networks are actually composed of very simple units that are akin to linear classification or regression methods. Don't be afraid by their name and the metaphor with the human brain. Neural networks are really simple transformations of the data that flow from the input, through various layers, ending with an output.   That's their beauty ! Complex capabilities emerge from very simple units when we put them together. For example the capability to recognise not \"just\" objects in an image but also abstract concepts such as people's emotions or situations and environments.","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"We'll first describe them, and we'll later learn how to train a neural network from the data. Concerning the practical implementation in Julia, we'll not implement a complete neural network but rather only certain parts, as we will mostly deal with using them and apply them to different kinds of datasets.","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html#Motivations-and-types","page":"0401 - Neural Networks architectures","title":"Motivations and types","text":"","category":"section"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"When we studied the Perceptron algorithm, we noted how we can transform the original feature vector mathbfx to a feature representation  phi(mathbfx) that includes non-linear transformations, to still use linear classifiers for non-linearly separable datasets (we studied classification tasks but the same is true for regression ones). The \"problem\" is that this feature transformation is not learned from the data but is applied a priori, before using the actual machine learning (linear) algorithm. With neural networks instead, the feature transformation is endogenous to the learning (training) step.","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"We will see three kinds of neural networks:","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"Feed-forward Neural Networks, the simplest one where the inputs flow through a set of \"layers\" to reach an output. \nConvolutional Neural Networks (CNN), where one or more of the layers is a \"convolutional\" layer. These are used mainly for image classification\nRecurrent Neural Networks (RNN), where the input arrives not only at the beginning but also at each layer. RNNs are used to learn sequences of data","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html#Feed-forward-neural-networks","page":"0401 - Neural Networks architectures","title":"Feed-forward neural networks","text":"","category":"section"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html#Description","page":"0401 - Neural Networks architectures","title":"Description","text":"","category":"section"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"In deep forward neural networks, neural network units are arranged in layers, from the input layer, where each unit holds the input coordinate, through various hidden layer transformations, until the actual output of the model:","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"(Image: Neural network scheme)","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"More in detail, considering a single dense neuron (in the sense that is connected with all the previous layer's neurons or with the input layer), we have the following figure:","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"(Image: Single neuron)","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"where:","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"x\nis a two-dimensional input, with x_1 and x_2 being the two dimensions of our input data (they could equivalently be the outputs of a previous 2 neurons layers)\nw\nare the weigths that are applied to x plus a constant term (w_0). These are the parameter we will want to learn with our algorithm. f is a function (often non-linear) that is applied to w_0 + x_1w_1 + x_2w_2 to define the output of the neuron","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"The output of the neuron can be the output of our neural network or it can be the input of a further layer.","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"In Julia we can implement a layer of neurons and its predictions very easily (although implementing the learning of the weights is a bit more complex):","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"using LinearAlgebra\nmutable struct DenseLayer\n    wb::Array{Float64,1} # weights with reference to the bias (will be learned from data)\n    wi::Array{Float64,2} # weigths with reference to the input (will be learned from data)\n    f::Function          # the activation function of each neuron (chosen from the modeller)\nend\n\nfunction forward(m,x) # The predictions  - or \"forward\" to the next layer\n      return m.f.(m.wb .+ m.wi * x)\nend\n\n(nI,nO) = 3,2 # Number of nodes in input and in outputs of this layer\n\nlayer = DenseLayer(rand(nO),rand(nO,nI),tanh)\nx = zeros(nI)\ny = forward(layer,x)","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"Let's specific a bit of terminology concerning Neural Networks:","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"The individual computation units of a layer are known as nodes or neurons.\nWidth_l (of the layer) is the number of units in that specific layer l\nDepth (of the architecture) is number of layers of the overall transformation before arriving to the final output\nThe weights are denoted with w and are what we want the algorithm to learn.\nEach node's aggregated input is given by z = sum_i=1^d x_i w_i + w_0 (or, in vector form, z = mathbfx cdot mathbfw + w_0, with z in mathbbR mathbfx in mathbbR^d mathbfw in mathbbR^d) and d is the width of the previous layer (or the input layer)\nThe output of the neuron is the result of a non-linear transformation of the aggregated input called activation function f = f(z)\nA neural network unit is a primitive neural network that consists of only the “input layer\", and an output layer with only one output.\nhidden layers are the layers that are not dealing directly with the input nor the output layers \nDeep neural networks are neural network with at least one hidden layer","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"While the weights will be learned, the width of each layer, the number of layers and the activation functions are all elements that can be tuned as hyperparameters of the model, although there are some more or less formal \"rules\":","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"the input layer is equal to the dimensions of the input data;\nthe output layer is equal to the dimensions of the output data. This is typically a scalar in a regression task, but it is equal to the number of categories in a multi-class classification, where each \"output dimension\" will be the probability associated with that given class;\nthe number of hidden layers reflects our judgment on how many \"levels\" we should decompose our input to arrive at the concept expressed in the label y (we'll see this point dealing with image classification and convolutional networks). Often 1-2 hidden layers are enough for classical regression/classification;\nthe number of neurons should give some \"flexibility\" to the architecture without exploding too much the number of parameters. A rule of thumb is to use about 20% more neurons than the input dimension. This is often fine-tuned using cross-validation as it risks leading to overfitting;\nthe activation function of the layers, with the exclusion of the last one, is chosen between a bunch of activation functions, nowadays almost always the Rectified Linear Unit function, aka relu, defined as relu(x) = max(0,x). The relu function has the advantage to add non-linearity to the transformation while remaining fast to compute (including the derivative) and limiting the problem of vanishing or exploding the gradient (we'll see this aspect when dealing with the actual algorithm to obtain the weights). Another common choice is tanh(), the hyperbolic tangent function, that maps with an \"S\" shape the real line to the interval [-1,1].\nthe activation function of the last layer depends on the nature of the labels we want the network to compute: if these are positive scalars we can use also here the relu, if we are doing a binary classification we can use the sigmoid function defined as sigmoid(x) = 1/(1+exp(-x)) whose output is in the range [0,1] and which we can interpret as the probability of the class that we encode as 1. If we are doing a multi-class classification we can use the softmax function whose output is a PMF of probabilities for each class. It is defined as softmax(xk) = frace^x_ksum_j=1^K e^x_j where x is the input vector, K its length and k the specific position of the class for which we want to retrieve its \"probability\".","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"Let's now make an example of a single layer, single neuron with a 2D input x=[2,4], weights w=[2,1], w₀ = 2 and activation function f(x)=sin(x).","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"In such a case, the output of our network is sin(2+2*2+4*1), i.e. -0.54. Note that with many neurons and many layers this becomes essentially (computationally) a problem of matrix multiplications, but matrix multiplication is easily parallelisable by the underlying BLAS/LAPACK libraries or, even better, by using GPU or TPU hardware, and running neural networks (and computing their gradients) is at the core of the demand for GPU computation.  ","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"Let's now assume that the true label that we know to be associated with our x is y=-0.6.","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"Out (basic) network did pretty well, but still did an error: -0.6 is not -0.54. The last element of a neural network is indeed to define an error metric (the loss function) between the output computed by the neural network and the true label. Commonly used loss functions are the squared l-2 norm (i.e. epsilon = mid mid hat y - y midmid ^2) for regression tasks and cross-entropy (i.e. epsilon = - sum_d p_d  * log(hat p_d)) for classification jobs.","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"Before moving to the next section, where we will study how to put everything together and learn how to train the neural network in order to reduce this error, let's first observe that neural networks are powerful tools that can work on many sorts of data, but they require however the input to be encoded in a numerical form, as the computation is strictly numerical. If I have a categorical variable, for example, I'll need to encode it expanding it to a set of dimensions where each dimension represent a single class and I encode with an indicator function if my record is that particular class or not. This is the most simple form of encoding and takes the name of one hot encoding:","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"(Image: One-hot encoding)","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"Note in the figure that using all the three columns leads to linearly dependency, and while, yes, we could save some resources by using only two columns instead of three, this is not a fundamental problem like it would be in statistical analysis. ","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html#Training-of-a-feed-forward-neural-network","page":"0401 - Neural Networks architectures","title":"Training of a feed-forward neural network","text":"","category":"section"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html#Gradient-and-learning-rate","page":"0401 - Neural Networks architectures","title":"Gradient and learning rate","text":"","category":"section"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"We now need a way to learn the parameters from the data, and a common way is to try to reduce the contribution of the individual parameter to the error made by the network. We need first to find the link between the individual parameter and the output of the loss function, that is how the error change when we change the parameter. But this is nothing else than the derivative of the loss function with respect to the parameter. In our simple one-neuron example above we have the parameters directly appearing in the loss function. Considering the squared error as lost we have epsilon = (y - sin(w_0 + w_1 x_1 + w_2 x_2))^2. If we are interested in the w_1 parameter we can compute the derivate of the error with respect to it using the chain rule as fracpartialepsilonpartial w_1 = 2*(y - sin(w_0 + w_1 x_1 + w_2 x_2)) * - cos(w_0 + w_1 x_1 + w_2 x_2) * x_1.","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"Numerically, we have: fracpartialepsilonpartial w_1 = 2(-06-sin(2+4+4)) * -cos(2+4+4) * 2 = -0188 If I increase w_1 of 0.01, I should have my error moving of -001*0188 = -00018. Indeed, if I compute the original error I have epsilon^t=0 = 000313, but after having moved w_1 to 2.01, the output of the neural network chain would now be hat y^t=1 = 0561 and its error lowered to epsilon^t=1 =  000154. The difference is 000159, slighly lower in absolute terms than what we computed with the derivate, 00018. The reason, of course, is that the derivative is a concept at the margin, when the step tends to zero.","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"We should note a few things:","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"the derivate depends on the level of w_1. \"zero\" is almost always a bad starting point (as the derivatives of previous layers will be zero). Various initialisation strategies are used, but all involve sampling randomly the initial parameters under a certain range\nthe derivate depends also on the data on which we are currently operating, x and y. If we consider different data we will obtain different derivates\nwhile extending our simple example to even a few more layers would seem to make the above exercise extremely complex, it remains just an application of the chain rule, and we can compute the derivatives efficiently by making firstly a forward passage, computing (and storing) the values of the chain at each layer, and then making a backward passage by computing (and storing) the derivatives with the chain rule backwards from the last to the first layer\nthe fact that the computation of the derivates for a layer includes the multiplication of the derivates for all the other layers means that if these are very small (big) the overall derivate may vanish (explode). This is a serious problem with neural networks and one of the main reasons why simple activation functions such as the relu are preferred.","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"The derivate(s) of the error with respect to the various parameters is called the gradient.","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"If the gradient with respect to a parameter is negative, like in our example, it means that if we slightly increase the parameter we will obtain a lower error. On the opposite, if it is positive, if we slightly reduce the parameter we should find a lower error.","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"A gradient-descent based algorithm can hence be used to look iteratively for the minimum error by moving against the gradient with a certain step. The most basic algorithm is then w_i^t = w_i^t-1 - fracpartialepsilon^t-1partial w_1^t-1 * lambda where lambda is the step that you are willing to make against the gradient, also known as learning rate. Note in the example above that if instead of moving the parameter w_1 of 001 we would have increased it of 01, we would have increased the error to 000997 instead of reducing it. This highlights the problem to use a good learning rate (see next Figure): a too-small learning rate would make the learning slow and with the risk to get trapped in a local minimum instead of a global one. Conversely, a too-large learning rate would risk causing the algorithm to diverge.","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"(Image: Learning rate effect)","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"So, the learning rate is also a hyper-parameter to calibrate, although some modern gradient descent variations, like the ADAptive Moment estimation (ADAM) optimisation algorithm, tend to self_tune themselves and we rarely need to calibrate the default values.","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html#Batches-and-Stochastic-gradient-descent","page":"0401 - Neural Networks architectures","title":"Batches and Stochastic gradient descent","text":"","category":"section"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"We already note that the computation of the gradient depends on the data levels. We can then move between two extremes: on one extreme we compute the gradient as the average of those computed on all data points and we apply the optimisation algorithm to this average. On the other extreme, we sample randomly record by record and, at each record, we move the parameter. The compromise is to partition the data in a set of batches, compute the average gradient of each batch and at each time update the parameter with the optimisation algorithm. The \"one record at the time\" is the slowest approach, but also is very sensitive to the presence of outliers. The \"take the average of all the data\" approach is faster in running a certain epoch, but it takes longer to converge (i.e. it requires more epochs, the number of times we pass through the whole training data). It also requires more memory, as we need to store the gradients with respect to all records. So, the \"batch\" approach is a good compromise, and we normally set the batch number to a multiplier of the number of threads in the machine performing the training, as this step is often parallelised, and it represents a further parameter that we can cross-validate. When we sample the records (individually or in batch) before running the optimisation algorithm we speak of stochastic gradient descent.","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html#Convolutional-neural-networks","page":"0401 - Neural Networks architectures","title":"Convolutional neural networks","text":"","category":"section"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html#Motivations","page":"0401 - Neural Networks architectures","title":"Motivations","text":"","category":"section"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"Despite typically classified separately, convolutional neural networks are essentially feed-forward neural networks with the only specification that one or more layers are convolutional. These layers are very good in recognising patterns within data with many dimensions, like spatial data or images, where each pixel can be thought a dimension of a single record. In both cases, we could use \"normal\" feed-forward neural networks, but convolutional layers offer two big advantages:","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"They require much fewer parameters.","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"Convolutional neurons are made of small filters (or kernels) (typically 3x3 or 5x5) where the same weight convolves across the image. Conversely, if we would like to process with a dense layer a mid-size resolution image of 1000 times 1000 pixels, each layer would need a weight matrix connecting all these 10^6 pixels in input with 10^6 pixel in output, i.e. 10^12 weights","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"They can extend globally what they learn \"locally\"","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"If we train a feed-forward network to recognise cars, and it happens that our training photos have the cars all in the bottom half, then the network would not recognise a car in the top half, as these would activate different neurons. Instead, convolutional layers can learn independently on where the individual features apply.","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html#Description-2","page":"0401 - Neural Networks architectures","title":"Description","text":"","category":"section"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"In these networks, the layer l is obtained by operating over the image at layer l-1 a small filter (or kernel) that is slid across the image with a step of 1 (typically) or more pixels at the time. The step is called stride, while the whole process of sliding the filter throughout the whole image can be mathematically seen as a convolution.","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"So, while we slide the filter, at each location of the filter, the output is composed of the dot product between the values of the filter and the corresponding location in the image (both vectorised), where the values of the filters are the weights that we want to learn, and they remain constant across the sliding. If our filter is a 10 times 10 matrix, we have only 100 weights to learn by layer (plus one for the offset). Exactly as for feedforward neural networks, then the dot product is passed through an activation function, here typically the ReLU function (max(0x)) rather than tanh:","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"(Image: Convolutional filter)","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html#Example","page":"0401 - Neural Networks architectures","title":"Example","text":"","category":"section"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"For example, given an image x = beginbmatrix 1  1  2  1  1 \n3  1  4  1  1 \n1  3  1  2  2 \n1  2  1  1  1 \n1  1  2  1  1 \nendbmatrix and filter weights w = beginbmatrix  1  -2  0 \n 1   0  1 \n-1   1  0 \nendbmatrix, then the output of the filter z would be beginbmatrix  8  -3  6 \n 4   -3  5 \n-3   5  -2 \nendbmatrix. For example, the element of this matrix z_23 = 5 is the result of the sum of the scalar multiplication between x^prime = beginbmatrix  4   1  1 \n 1   2  2 \n 1   1  1 \nendbmatrix and w.","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"Finally, the output of the layer would be (using ReLU) beginbmatrix  8  0  6 \n 4   0  5 \n 0   5  0 \nendbmatrix.","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"We can run the following snippet to make the above computations:","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"ReLU(x) = max(0,x)\nx = [1 1 2 1 1;\n        3 1 4 1 1;\n        1 3 1 2 2;\n        1 2 1 1 1;\n        1 1 2 1 1]\nw = [ 1 -2  0;\n        1  0  1;\n        -1  1  0]\n(xr,xc) = size(x)\n(wr,wc) = size(w)\nz = [sum(x[r:r+wr-1,c:c+wc-1] .* w) for c in 1:xc-wc+1 for r in 1:xr-wr+1] # Julia is column major\nu = ReLU.(z)\nfinal = reshape(u, xr-wr+1, xc-wc+1)","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"You can notice that by applying the filter we obtain a dimensionality reduction. This reduction depends on both the dimension of the filter and the stride (sliding step). In order to avoid this, padding of one or more rows/columns can be applied to the image to preserve in the output the same dimension of the input (in the above example padding of one row and one column on both sides would suffice). Typically the padded cells are given a value of zero so not to contribute anything when they are included in the dot product computed by the filter.","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"To determine the spatial size in the output of a filter (O), given the input size (I), the filter size (F), the stride (S) and the eventual padding (P) we can use the following simple formula:","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"O_d = 1 + (I_d+2*P_d-F_d)S","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"From it, we can also find the padding needed to obtain a certain output size as: P_d = ((O_d-1)S_d-I_d+F_d)2","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"Where the d index accounts for the (extremely unusual) case where one of the parameters is not a square matrix so that for example an image has different vertical and horizontal resolutions.","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"Because the weights of the filters are the same, it doesn't really matter where the object is learned, in which part of the image. With convolutional layers, we have translational invariance as the same filter is passed over the entire image. Therefore, it will detect the patterns regardless of their location.","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"Still, it is often convenient to operate some data augmentation to the training set, that is to add slightly modified images (rotated, mirrored..) in order to improve this translational invariance.","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html#Considering-multiple-filters-per-layer","page":"0401 - Neural Networks architectures","title":"Considering multiple filters per layer","text":"","category":"section"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"Typically, one single layer is formed by applying multiple filters, not just one. This is because we want to learn different kinds of features. For example in an image one filter will specialize to catch vertical lines, the other obliques ones, and maybe another filter different colours.","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"(Image: Set of different convolutional filters outputs)","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"Convolutional filters outputs on the first layer (filters are of size 11x11x3 and are applied across input images of size 224x224x3). Source: Krizhevsky et Oth. (2012), \"ImageNet Classification with Deep Convolutional Neural Networks\"","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"So in each layer we map the output of the previous layer (or the original image in case of the first layer) into multiple feature maps where each feature map is generated by a little weight matrix, the filter, that defines the little classifier that's run through the original image to get the associated feature map. Each of these feature maps defines a channel for information and we can represent it as a third dimension to form a \"volume\", where the depth is given by the different filters:","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"(Image: Convolutional layer)","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"In the image above the input layer has size (4,4,4) and the output layer has size (3,3,2), i.e. 2 \"independent\" filters of size (2,2).","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"For square layers, each filter has F^2 times D_l-1 + 1 parameters where D_l-1 is the dimensions (\"depth\") of the previous layer, so the total number of parameters per layer is (F^2 times D_l-1 + 1) * D_l.","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"For computational reasons, the number of filters per layer D_l is normally a power of 2. ","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"This representation allows remaining consistent with the input that can as well be represented as a volume. For images, the depth is usually given by 3 layers representing the values in terms of RGB colours.","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html#Pool-layers","page":"0401 - Neural Networks architectures","title":"Pool layers","text":"","category":"section"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"A further way to improve translational invariance, but also have some dimensionality reduction, is called pooling and is implemented by adding a layer with a filter whose output is the max of the corresponding area in the input (or, more rarely, the average). Note that this layer would have no weights to learn! With pooling we start separating what is in the image from where it is in the image, that is, pooling does a fine-scale, local translational invariance, while convolution does more a large-scale one.","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"Keeping the output of the above example as input, a pooling layer with a 2 times 2 filter and a stride of 1 would result in beginbmatrix  8  6 \n 5  5 \nendbmatrix.","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html#Convolutional-networks-conclusions","page":"0401 - Neural Networks architectures","title":"Convolutional networks conclusions","text":"","category":"section"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"We can then combine these convolutions, looking for features, and pooling, compressing the image a little bit, forgetting the information of where things are, but maintaining what is there.","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"In a typical CNN, these convolutional and pooling layers are repeated several times, where the initial few layers typically would capture the simpler and smaller features, whereas the later layers would use information from these low-level features to identify more complex and sophisticated features, like characterisations of a scene. The learned weights would hence specialise across the layers in a sequence like  edges -> simple parts-> parts -> objects -> scenes.","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"These layers are finally followed by some \"normal\", \"fully connected\" layers (like in \"normal\" feed-forward neural networks) and a final softmax layer indicating the probability that each image represents one of the possible categories (there could be thousands of them).","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"The best network implementations are tested in so-called \"competitions\", like the yearly ImageNet context.","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"Note that we can train these networks exactly like for feedforward NN, defining a loss function and finding the weights that minimise the loss function. In particular, we can apply the stochastic gradient descendent algorithm (with a few tricks based on getting pairs of image and the corresponding label), where the gradient with respect to the various parameters (weights) is obtained by backpropagation.","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html#Recurrent-Neural-Networks-(RNNs)","page":"0401 - Neural Networks architectures","title":"Recurrent Neural Networks (RNNs)","text":"","category":"section"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html#Motivations-2","page":"0401 - Neural Networks architectures","title":"Motivations","text":"","category":"section"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"Recurrent neural networks are used to learn sequences of data. A \"sequence\" is characterised by the fact that each element may depend not only on the features in place at time t, but also from lagged features or lagged values of the sequence (we use here the time dimension just for simplicity. Of course, a sequence can be defined on any dimension). And here comes the problem: we could always consider lagged features or sequence values as further dimensions at time t and use a \"standard\" feed-forward network. For example we could consider values at time t-1, those at time t-2 and those at time t-3. But, again, we would be doing \"manual\" feature engineering, similar to the way we can introduce non-linear feature transformation and use linear classifiers. But we want this to be learned by the algorithm. We want the model to learn how much of the history retain to predict the next element of the sequence, and which elements \"deserve\" to be kept in memory (to be used for predictions) even if far away in the sequence steps.","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html#Description-3","page":"0401 - Neural Networks architectures","title":"Description","text":"","category":"section"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"There are a few differences with feed-forward neural networks:","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"the input doesn't arrive only at the beginning of the chain, but at each layer (each input being an element of the sequence)\neach RNN layer processes, using learnable parameters, the input corresponding to its layer, together the input coming from the previous layers (called the state)\nthese weights are shared for the various RNN layers across the sequence","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"Note that you can interpret a recurrent network equivalently like being formed by different layers on each element of the sequence (but with shared weights) or like a single, evolving, layer that calls itself recursively. Note also the similarities with convolutional networks: there we have a filter than convolves along the image, keeping the weigths constant across the convolution, here we have a recurrent network that also \"filter\" the whole sequence and learn some shared weigths.  ","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"To implement a recurrent neural network we can adapt our code above to include the state: ","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"mutable struct RNNLayer\n    wb::Array{Float64,1} # weights with reference to the bias\n    wi::Array{Float64,2} # weigths with reference to the input\n    ws::Array{Float64,2} # weigths with reference to the state\n    f::Function\nend\n\n(nI,nO)  = 3,2\nrelu(x)  = max(0,x)\nrnnLayer = RNNLayer(rand(nO),rand(nO,nI),rand(nO,nO),relu)\nfunction forward(m,x,s)\n    return m.f.(m.wb .+ m.wi * x .+ m.ws * s)\nend\nx,s = zeros(nI),zeros(nO)\ns = forward(rnnLayer,x,s)\ns = forward(rnnLayer,x,s)  # The state change even if x remains constant","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"The code above is the simplest implementation of a Recurrent Neural Network (or at least of its forward passage). In practice, the state is often memorised as part of the layer structure so its usage in most neural network libraries is similar to a \"normal\" feed-forward layer forward(layer,x).","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html#Usage:-sequence-to-one","page":"0401 - Neural Networks architectures","title":"Usage: sequence-to-one","text":"","category":"section"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"RNNs can be used to characterise a sequence, like in sentiment analysis to predict the overall attitude (positive or negative) of a text or the language in which the text is written. In these cases, the RNN task is to encode the sequence in a vector format (the final state) and this is fed to a further part of the chain whose task is to decode according to the task required. Note that the parameters for both tasks are learned jointly. The scheme is as follow:","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"(Image: Sequence-to-one scheme)","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"Training in this scenario implies starting the model from an initial state (normally a zero-vector) and some random weights,  and  \"feeding\" the model with one item at a time until the sequence ends. At this time the final state is decoded to an overall output that is compared to the \"true\" y.  From here the backward passage is made in a similar way that in feed-forward networks so that the \"contribution\" of each weight to the errors can be assessed and the weights adjusted","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"warning: Warning\nWhile weights are progressively adjusted across the training samples, the state of the network should be reset at each new sequence sample.","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html#Usage:-sequence-to-sequence","page":"0401 - Neural Networks architectures","title":"Usage: sequence-to-sequence","text":"","category":"section"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"Another scenario is when we want the RNN to replicate some sequence pattern, like in next word, next note or next price predictions. In this case, we are interested in all the elements of the sequence and not only in the final state of the sequence.  The decoding part happens hence at each step of the sequence and the resulting hat y_i is compared with the true y_i, with the resulting loss used to train the weights:","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"(Image: Sequence-to-sequence scheme)","category":"page"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html#Gated-networks","page":"0401 - Neural Networks architectures","title":"Gated networks","text":"","category":"section"},{"location":"04_-_NN_-_Neural_Networks/0401_-_Neural_network_architectures.html","page":"0401 - Neural Networks architectures","title":"0401 - Neural Networks architectures","text":"While theoretically RNN can \"learn\" the importance of features across indeterminately long sequence steps, in practice the fact of continuing multiplicating the status across the varius elements of the sequence makes the problem of vanishing gradient even stronger for them. New contributions have hence been proposed with a \"gating\" system that \"learns\" what to store in memory (in the sequence state) and what to \"forget\". At the time of writing the most used approach is the Long short-term memory (LSTM). While internally more complex due to the presence of the gates and of several different states (hidden and visible in LSTM), LSTM networks are operationally used exactly in the same ways as the RNN networks described above.","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0001_-_Course_presentation.html#Course-Presentation","page":"0001 - Course presentation","title":"Course Presentation","text":"","category":"section"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0001_-_Course_presentation.html#Objectives","page":"0001 - Course presentation","title":"Objectives","text":"","category":"section"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0001_-_Course_presentation.html","page":"0001 - Course presentation","title":"0001 - Course presentation","text":"This course aims to provide a gentle introduction to Julia, a modern, expressive and efficient programming language, and introduce the main concepts around machine learning, together with some algorithms and practical implementations of machine learning workflows.","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0001_-_Course_presentation.html","page":"0001 - Course presentation","title":"0001 - Course presentation","text":"In particular the objectives are : ","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0001_-_Course_presentation.html","page":"0001 - Course presentation","title":"0001 - Course presentation","text":"Supply students with an operational knowledge of a modern and efficient general-purpose language that can be employed for the implementation of their research daily activities;\nPresent several specific but commonly used tools in multiple scientific areas (data transformation, optimisation,...)\nIntroducing students to modern tools for scientific collaboration and software quality, such as version control systems and best practices to obtain replicable results.\nIntroduce machine learning approaches: scopes, terminology, typologies, workflow organisation\nIntroduce some specific machine learning algorithms for classification and regression","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0001_-_Course_presentation.html#How-to-attend-the-course","page":"0001 - Course presentation","title":"How to attend the course","text":"","category":"section"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0001_-_Course_presentation.html","page":"0001 - Course presentation","title":"0001 - Course presentation","text":"The course is multi-channel, with these pages complemented with youtube videos (see Program ), quizzes and exercises. Thanks to projects as Literate.jl and Documenter.jl the source of most of these pages are runnable valid Julia files.","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0001_-_Course_presentation.html","page":"0001 - Course presentation","title":"0001 - Course presentation","text":"To fully master the subject, nothing is better than cloning the repository on GitHub and run these pages by yourself! To execute yourself the code discussed in the videos run the code in the lessonsSource folder. Other resources used in the course (in particular the examples of the introduciton and the exercises) are located under the lessonsMaterial folder.","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0001_-_Course_presentation.html","page":"0001 - Course presentation","title":"0001 - Course presentation","text":"While the content of this course may also be used in specific academic courses with a fixed calendar, you are free to complete the course at your pace. You don't even need to log-in. This however means that there is no way to track your progress or the outcomes of the quizes.","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0001_-_Course_presentation.html","page":"0001 - Course presentation","title":"0001 - Course presentation","text":"Note that there will be a bit of incongruence in the terminology used for \"units\". Sometimes I call them \"lessons\", especially at the beginning of the course, then it ended up that the actual content was much bigger than a standard \"lesson\", so I started to use the word \"unit\". Any-how, the course is organised in units/lessons (e.g. JULIA1). Each unit/lesson has its own Julia environment and it is organised in several segments. These are the individual pages on this site and correspond to a single file in the github repository. Then each segment is divised in multiple videos, that I call parts.","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0001_-_Course_presentation.html#Course-authors","page":"0001 - Course presentation","title":"Course authors","text":"","category":"section"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0001_-_Course_presentation.html","page":"0001 - Course presentation","title":"0001 - Course presentation","text":" <img src=\"assets/imgs/photo_antonello_lobianco.png\" alt=\"Antonello Lobianco\" width=\"150\"> ","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0001_-_Course_presentation.html","page":"0001 - Course presentation","title":"0001 - Course presentation","text":"Antonello Lobianco","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0001_-_Course_presentation.html","page":"0001 - Course presentation","title":"0001 - Course presentation","text":"OrcID - Google Schoolar - GitHub - Personal site","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0001_-_Course_presentation.html","page":"0001 - Course presentation","title":"0001 - Course presentation","text":"I am a Forest and Natural Resources Economist @ AgroParisTech, a French \"Grand ecole\" (sort of polytecnic university) in the life science domain and affiliated to BETA, the Bureau d'Économie Théorique et Appliquée, which brings together most of the economists located in the Grand-Est region of France. My main interest is in exploring the interplay between the biophysical layers in the forest sector (climate change, forest dynamics) and the socio-economic ones (individual forest owners behaviours, timber markets \"behaviours\") in order to better understand the space of actions that society has to maximise social benefits in a sustenible way. For that I design, develop and use bio-economic simulation models of the forest sector.","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0001_-_Course_presentation.html","page":"0001 - Course presentation","title":"0001 - Course presentation","text":"While this course is in English, English is not my native language... please, be understanding !","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0001_-_Course_presentation.html#How-to-contribute-to-the-course","page":"0001 - Course presentation","title":"How to contribute to the course","text":"","category":"section"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0001_-_Course_presentation.html","page":"0001 - Course presentation","title":"0001 - Course presentation","text":"Contributions are really welcome! From simple, minor editing and bug-fixes (by making new comments or, better, pull requests) to introducing new arguments along the course domain.","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0001_-_Course_presentation.html","page":"0001 - Course presentation","title":"0001 - Course presentation","text":"The only 3 conditions are that:","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0001_-_Course_presentation.html","page":"0001 - Course presentation","title":"0001 - Course presentation","text":"the prerequisites for the students are kept at the lowest possible level. If one concept can be fully expressed using a simple, trivial \"2+2\" method, this should be always preferred to using more advanced mathematical concepts;\na practical implementation of the concepts in a working Julia file is proposed to the students and the documentation builds on top of that file (this site is built using Literate.jl that transforms the .jl file to .md, and then Documenter.jl that tranforms the .md file to html);\nsome sort of interactive tools are proposed to the students, like the quizzes or  practical exercises.","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0001_-_Course_presentation.html","page":"0001 - Course presentation","title":"0001 - Course presentation","text":"While the production of videos for your content is welcome, this is not strictly necessary.","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0001_-_Course_presentation.html","page":"0001 - Course presentation","title":"0001 - Course presentation","text":"<script src=\"https://utteranc.es/client.js\"\n        repo=\"sylvaticus/SPMLJ\"\n        issue-term=\"url\"\n        label=\"💬 website_comment\"\n        theme=\"github-dark\"\n        crossorigin=\"anonymous\"\n        async>\n</script>","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0004_-_Introduction_to_ML.html","page":"0004 - Introduction to ML","title":"0004 - Introduction to ML","text":"TODO. Please refer to the videos.","category":"page"},{"location":"index.html#Introduction-to-Scientific-Programming-and-Machine-Learning-with-Julia","page":"Index","title":"Introduction to Scientific Programming and Machine Learning with Julia","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"Except for the introductions, this page and the conversion of sevaral quizes in the online format, the course is complete.","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"Please use the menu to access a particular section.","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"GitHub repository","category":"page"},{"location":"index.html#Acknowledgements","page":"Index","title":"Acknowledgements","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"The development of this course at the Bureau d'Economie Théorique et Appliquée (BETA, Nancy) was supported by the French National Research Agency through the Laboratory of Excellence ARBRE, a part of the “Investissements d'Avenir” Program (ANR 11 – LABX-0002-01).","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"Also, cite the course MITX and the QuizQuestions.jl package","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"(Image: logos)","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html#Machine-Learning-main-concepts","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"","category":"section"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"We start our journey on Machine Learning from the perceptron algorithm. This is a linear classifier that historically has been a bit a pioneer in ML algorithms. We start from it due to its simplicity, and it will be the only algorithm that we will study in detail.","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html#A-linear-classifier-for-the-binary-classification-problem","page":"0301 - Machine Learning -  main concepts","title":"A linear classifier for the binary classification problem","text":"","category":"section"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"So, which is our problem? We are in the realm of supervised machine learning as we defined in the kick-off lesson, where the objective is to make some predictions over a component of the dataset (an unknown characteristic, or some future event...) that we call \"label\" based on other known characteristics of the data (that we call \"features\"), and we can \"learn\" this relation by \"supervising\" the algorithm, that is providing the algorithm with data from which we know both the features and the associated labels (across this course I'll often use \"X\" as a shortcut for the former, and \"Y\" for the latter). More specifically we are in the realm of classification, where the labels are given a finite set of possible values, and we start from binary labels, where the values can only be -1 or +1 (it is easy then to use this binary classifier as the base for a multiclass classifier, e.g. using a so-called one-vs-all strategy). The features are numerical, and possibly highly multi-dimensional.","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"The Perceptron algorithm is a method to find the parameters of a linear classifier that minimise classification errors. These classifiers will be nothing else than a hyperplane (the generalisation of the concept of \"plane\" on multiple dimensions) that divides our space into two parts, with the hyperplane itself forming the boundaries between the two parts, and we will use this hyperplane to discriminate all points on one side of it v.s. all points on the other side. For example, the following figure shows a hyperplane in 2D (i.e. a line) used to classify some points:","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"(Image: A linear classifier in 2D)","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"All points in the direction of the arrow are \"positive\", all points in the opposite direction are negative. From the figure we can deduce a few things in that example:","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"the current classifier (as drawn) is NOT classifying all points correctly, ie it makes \"errors\"\nthe points are however linearly separable\nit would be enough to \"rotate\" a bit the classifier clockward to get the classifier making no more errors","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"The perceptron algorithm is indeed an online algorithm (that is, that is updated as data is processed..) to \"rotate\" the classifier until it minimises the classification errors. For simplicity, we will work with classifiers passing through the origin, but we don't lose in generality as we can always think of the constant term as another dimension where the feature data is all filled with ones.","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"Our first step is to \"decide\" how to define the classifier, and how to check if it classifies a record correctly or not. Concerning the former, we define the boundary given by the parameters theta as the set of points x for which x cdot theta = 0 (the inner product is equal to zero). It results that all points x for which $ x \\cdot \\theta > 0$ are classified positively, and similarly all points x for which $ x \\cdot \\theta < 0$ are classified negatively. For a given record, we can now check if this classifier matches the label of the record (-1 or +1). In analytical terms, we have a classification error of the (linear) classifier theta for the record n when  y^n * (theta cdot x^n)) leq 0. The overall error rate of the classifier will then be the sum of the errors for each record divided by the number of records N:","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"epsilon_n(theta) = sum_n=1^N frac mathbb1  y^n * (theta cdot x^n ) leq 0  n","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"where the one at the numerator is an indicator function. And note that we consider a point exactly on the boundary still as a classification error.","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html#Supervised-learning","page":"0301 - Machine Learning -  main concepts","title":"Supervised learning","text":"","category":"section"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"Before we can turn to the problem of actually finding a linear classifier that agrees with the data to the extent possible, that is looking at the specific perceptron algorithm, we need to develop a method to generally deal with \"learning algorithms\" in the context of supervised learning, where - like here - we are given a set of both the features (the \"X\" matrix) and the associated labels (the \"Y\" vector - or sometimes matrix).","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"Let's then define as the algorithm's parameters those parameters that are learned by an algorithm by processing the (X,y) data that we provide to it to learn its relations. Often an algorithm has also so-called hyperparameters. These are additional parameters that remain constant during the learning (also called training) step, while still affecting the performances of the algorithm. For example, the number of neurons in a neural network layer, or the number of individual decision trees in the random forest algorithm. Or, for many algorithms (including perceptron) how long should the learning step continue (this can take different forms depending on the algorithm).","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"Hyperparameters play a fundamental role in the trade-off between specialisation and generalisation. The objective of the training is not indeed to learn a relation between the given X and the given Y, but rather to learn from the provided data the general relationship between X and Y for all the populations from which X and Y have been sampled. And hyperparameters should be \"set\" to the levels that maximise this objective, not the minimisation of errors in the data used to train the algorithm. If we use too many neurons, if we train too much,.. we would learn the specific relation between X and Y in the training data. However, this reflects the specific data provided and not the general population. In statistical terminology, we would overfit our model, or in other words, generate too much variance in the trained parameter of our model, that would depend too much on the specific training data (i.e. different training data would lead to very different learned parameters). On the opposite, if our model is too simple or receives too little training, we will have too much bias and not learn the relationship sufficiently. Techniques that allow an algorithm to better generalise, at the expense of better performances over the training data, are called \"regularisation\".","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"How can we choose the hyperparameters that minimise the bias-variance trade-off ? We put no assumptions on the data except that they all come from the same population. And the idea is to use the data itself to \"evaluate\" the generality of our model. We randomly split our dataset into three subsets:","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"The training set is the one used to actually \"train\" the algorithm to learn the relation between the given X and the given y, provided a certain set of hyperparameters, that is to find the parameters than minimise the error made by the algorithm\nthe validation set is used to evaluate the results of our trained algorithm on data that has not been used by the algorithm to train the parameters, that is to find the hyperparameters that allow for the best generalisation\nfinally the test set is used to judge the overall performances of the algorithm when it is used with the \"best\" hyperparameter (we can't use the validation set for this, as the hyperparameters are \"fitted\" based on it).","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"(Image: Train, validation and test set)","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"In practice, we have various ways to look for the \"best hyperparameters\".. grid search over the hyperparameters space, random search, gradient-based methods... In all cases, we run the algorithm under the training set, and we evaluate it under the validation set until we find the \"best\" hyperparameter set. At this point, with the \"best\" hyperparameters we train one last time the algorithm using the training set and we evaluate it under the test set.","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html#K-folds-cross-validation","page":"0301 - Machine Learning -  main concepts","title":"K-folds cross validation","text":"","category":"section"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"Note that training and validation sets don't need to be the same sample across the whole process. Indeed a common technique is the so-called K-folds cross-validation:","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"(Image: 5-folds CrossValidation)","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"Here we first randomly divide our whole dataset in a train/validation set and in the test set. For each possible hyperparameter set, we randomly partition the train/validation test in K sets. We use K-1 of them for training and the remaining one for computing the out-of-sample score of the model. We do that (keeping the same hyperparameters and the same partition) for all the different K subsets and we average the performances of the model with that given hyperparameters. We then select the \"best\" hyperparameters and we run the final training on the train/validation set and evaluation on the test set.","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html#The-perceptron-algorithm","page":"0301 - Machine Learning -  main concepts","title":"The perceptron algorithm","text":"","category":"section"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"We can now start our analysis of the Perceptron algorithm.","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"We start with the parameters of the hyperplane all zeros theta = 0\nWe check if, with this parameter, the classifier makes an error\nIf so, we progressively update the classifier using as the update function  theta^n = theta^n - 1 + y^n * x^n.","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"As we start with theta^0 = beginbmatrix00endbmatrix, the first attempt will always lead to an error and to a first \"update\" that will be theta^1 = beginbmatrix00endbmatrix + y^1 * beginbmatrixx^1_d1x^1_d2endbmatrix.","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"Let's make an exaple in 2 dimensions, with two points x^1 = beginbmatrix24endbmatrix and x^2 = beginbmatrix-61endbmatrix, both with negative labels.","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"After being confronted with the first point, the classifier theta^0 undergos its first update to become theta^1 = beginbmatrix00endbmatrix + -1 * beginbmatrix24endbmatrix = beginbmatrix-2-4endbmatrix. Let's continue with the second point, x^(2) = beginbmatrix-61endbmatrix. Does theta^1 make an error in classifying x^2 ? We have:  y^(2) * theta^1 cdot x^(2) = -1 * beginbmatrix-2-4endbmatrix beginbmatrix-61endbmatrix = -8, so yes, we have another classification error. We hence run a second update to obtain  theta^2 = beginbmatrix-2-4endbmatrix + -1 * beginbmatrix-61endbmatrix = beginbmatrix4-5endbmatrix. I let you see geometrically that this classifier correctly classify the two points:","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"(Image: Perceptron example over 2 points)","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"More in general, we run the perceptron algorithm over the whole training set, starting from the 0 parameter vector and then going over all the training examples. And if the n-th example is a mistake, then we perform that update that we just discussed.","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"So we moved the parameters in the right direction, based on an individual update. However, since the different training examples might update the parameters in different directions, the later updates might also \"undo\" some of the earlier updates, and some of the earlier examples would no longer be correctly classified. In other words, there may be cases where the perceptron algorithm needs to go over the training set multiple times before a separable solution is found (I let you try as exercise what would happen if the second point is +1-2 instead of -6+1, still with negative label).","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"So we have to go through the training set here multiple times. In the jargon of machine learning, we call epoch each time an algorithm goes through the whole training set, either in order or selecting at random. On each record, we look at whether the current classify makes a mistake and eventually perform a simple update.","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"function perceptron displaystyle left(big  (x^(n) y^(n)) n=1Nbig   epochs right):\ninitialize theta =0 (vector);\nfor t=1epochs do\nfor n=1N do\nif y^(n)(theta cdot x^(n)) leq 0 then\nupdate theta = theta + y^(n)x^(n)\nreturn theta","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"So the perceptron algorithm takes two parameters: the training set of data (pairs feature vectors => label) and the epochs parameter that tells how many times going over the training set.","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"For a sufficiently large number of epochs, if there exists a linear classifier through the origin that correctly classifies the training samples (i.e. the training set is linearly separable), this simple algorithm actually will find a solution to that problem. Typically, there are many solutions, but the algorithm will find one. And note that the one found is not, in general, the \"optimal\" one, where the points are \"best\" separated, just one where the points are separated.","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html#Support-Vector-Machines:-\"better\"-linear-classifiers","page":"0301 - Machine Learning -  main concepts","title":"Support Vector Machines: \"better\" linear classifiers","text":"","category":"section"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"While the Perceptron algorithm finds one possible classifier, it is clear that this may not be the \"best\" one. See the following figure:","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"(Image: Different linear classifiers over the same dataset)","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"Linear classifiers do generalise relatively well and the epochs parameter could be used as a form of regularisation. Still, we could end up with a perceptron classifier like in figure (a), very dependent on noisy data and that doesn't generalise well. Support Vector Machines (SVM, which we will not develop further in this course except in this small discussion) are linear classifiers that try to maximise the boundary with the classified data. So here we enter the realm of \"optimisation\", usually achieved employing a gradient-based approach that we'll see when discussing neural networks: it is no longer indifferent one classifier that separates the data from another, but SVM (should) retrieve the classifier that is more \"far away\" from the two datasets, on both the directions (figure (b)). The second important characteristic of SVM is that this optimisation can be \"adjusted\" to consider not only the points that lie closest to the decision surface (the \"support vectors\", from which the name..) but rather to give importance to the points that are farther away from the boundary. This adjustment takes the form of a regularisation parameter that can try to optimize with the cross-validation technique above. So, in the figure example, we intuitively see that the third classifier ( figure (c) ) would be the best for our dataset, it will better match with the nature of our data, even if it would make some classification mistakes, and we can steer an SVM algorithm toward the one depicted on figure (c) by increasing its regularisation parameter.","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html#Using-linear-classifiers-for-non-linear-classification","page":"0301 - Machine Learning -  main concepts","title":"Using linear classifiers for non-linear classification","text":"","category":"section"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"Often the relationship between the X and the Y is not linear in nature, and even employing the best linear classifier would result in significant classification errors (same for regressions). The \"good news\" is that we can easily engineer our data performing non-linear transformation over it and still using a linear classifier.","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"For example in the left diagram in the figure below, the three points are not separable in one dimension:  ","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"(Image: Inseparable points becoming separable in higher dimensions)","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"However, we can \"engineer\" our dataset by creating a new dimension that is the square of our original dimension (diagram on the right): the points are now clearly separable by a linear classifier !","category":"page"},{"location":"03_-_ML1_-_Introduction_to_Machine_Learning/0301_-_Machine_learning_main_concepts.html","page":"0301 - Machine Learning -  main concepts","title":"0301 - Machine Learning -  main concepts","text":"The \"bad news\" is that this engineering is not \"learned\" by the algorithm, but it is something we still do on a case-by-case, using our expertise of the specific problem on hand.  This is the main difference between linear algorithms used together with feature transformation and more \"modern\" non-linear algorithms where the non-linearity is learned from the data itself, like in trees-based approaches and neural networks. ","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0002_-_Program.html#Program","page":"0002 - Program","title":"Program","text":"","category":"section"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0002_-_Program.html","page":"0002 - Program","title":"0002 - Program","text":"The following videos (15h:2':30'') are available. All except the introduction follow very close the pages in this site.","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0002_-_Program.html","page":"0002 - Program","title":"0002 - Program","text":"Videos are hosted on YouTube.","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0002_-_Program.html#KOM:-Kick-off-meeting-(2h:44:54)","page":"0002 - Program","title":"00 KOM: Kick-off meeting (2h:44:54)","text":"","category":"section"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0002_-_Program.html","page":"0002 - Program","title":"0002 - Program","text":"Note that this introduction has been recorded before the rest of the course has been implemented, so that the organisation of the course, and the way it is delivered, are not exactly as described in the videos below.","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0002_-_Program.html","page":"0002 - Program","title":"0002 - Program","text":"Take-home tip: in your projects, implement the introduction and the conclusions as the last elements ;-)","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0002_-_Program.html","page":"0002 - Program","title":"0002 - Program","text":"The slides used in the videos below are available here.","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0002_-_Program.html","page":"0002 - Program","title":"0002 - Program","text":"Course introduction (16:11)\nJulia overview (36:25)\nML Terminology (21:19)\nA first ML Example (7:00)\nML application areas (14:24)\nHands on (42:09)\nPart A (20:15)\nPart B (21:54)\nPkgs, modules and environments (20:56)\nFurther ML Examples (6:34)","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0002_-_Program.html#JULIA1:-Basic-Julia-programming-(5h:52:33)","page":"0002 - Program","title":"01 JULIA1: Basic Julia programming (5h:52:33)","text":"","category":"section"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0002_-_Program.html","page":"0002 - Program","title":"0002 - Program","text":"Basic syntax elements (46:45)\nPart A - Introduction and setup of the environment (8:37)\nPart B - Comments, code organsation, Unicode support, broadcasting (12:04)\nPart C - Math operators, quotation marks (6:49)\nPart D - Missing values (10:04)\nPart E - Stochasticity in programming (9:14)\nTypes and objects (26:38)\nPart A - Types, objects, variables and operators (13:05)\nPart B - Object mutability and effects on copying objects (13:34)      \nPredefined types (1h:39:50)\nPart A - Primitive types, char and strings (9:37)\nPart B - One dimensional arrays (30:42)\nPart C - Multidimensional arrays (23:37)\nPart D - Tuples and named tuples (7:50)\nPart E - Dictionaries and sets (8:57)\nPart F - Date and times (19:11)\nControl flow and functions (44:47)\nPart A - Variables scope (9:47)\nPart B - Loops and conditional statements (9:2)   \nPart C - Functions (25:57)\nCustom Types (40:02)\nPart A - Types of types, composite types (17:56)\nPart B - Parametric types (7:37)\nPart C - Inheritance and composition OO paradigms (14:28)  \nFurther Topics (1:34:30)\nPart A - Metaprogramming and macros (23:46)\nPart B - Interoperability with other languages (23:6)\nPart C - Performances and errors: profiling, debugging, introspection and exceptions(27:33)\nPart D - Parallel computation: multithreading, multiprocessing (20:3)","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0002_-_Program.html#JULIA2:-Scientific-Julia-programming-(2h:39:34)","page":"0002 - Program","title":"02 JULIA2: Scientific Julia programming (2h:39:34)","text":"","category":"section"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0002_-_Program.html","page":"0002 - Program","title":"0002 - Program","text":"Data Wrangling (1h:31:15)\nPart A - Introduction and data import (18:28)\nPart B - Getting insights of the data (25:26)\nPart C - Edit data and dataframe structure (23:40)\nPart D - Pivot, Split-Apply-Combine and data export (23:39)\nFurther topics (1h:08:19)\nPart A - Plotting (15:18)\nPart B - Probability distributions and data fitting (11:55)\nPart C - Constrained optimisation, the transport problem (24:59)\nPart D - Nonlinear constrained optimisation, the optimal portfolio allocation (16:04)","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0002_-_Program.html#ML1:-Introduction-to-Machine-Learning-(1h:29:43)","page":"0002 - Program","title":"03 ML1: Introduction to Machine Learning (1h:29:43)","text":"","category":"section"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0002_-_Program.html","page":"0002 - Program","title":"0002 - Program","text":"Main concepts in Machine Learning(44:45)\nPart A - Introduction, perceptron overall idea (11:23)\nPart B - Hyperparameters and cross-validation (15:18)\nPart C - The perceptron algorithm (9:53)\nPart D - SVM and non-linear classification with linear classifiers (8:9)\nThe Perceptron algorithm for linear classification (44:58)\nPart A - A first version (13:22)\nPart B - A better version (10:28)\nPart C - Cross-validation implementation (21:7)","category":"page"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0002_-_Program.html#NN:-Neural-Networks-(2h:15:36)","page":"0002 - Program","title":"03 NN: Neural Networks (2h:15:36)","text":"","category":"section"},{"location":"00_-_INTRO_-_Introduction_julia_ml/0002_-_Program.html","page":"0002 - Program","title":"0002 - Program","text":"Introduction to Neural Networks (1h:25:17)\nPart A - Introduction and motivations (5:32)\nPart B - Feed-forward neural networks (18:57)\nPart C - How to train a neural network (18:4)\nPart D - Convolutional neural networks (13:21)\nPart E - Multiple layers in convolutional neural networks (10:33)\nPart F - Recurrent neural networks (17:49)\nNeural Network workflows in Julia (50:18)\nPart A - Binary classification (15:54)\nPart B - Multinomial classification (15:1)\nPart C - Regression (6:3)\nPart D - Convolutional neural network (13:19)","category":"page"}]
}
